<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      Introduction &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/convex-optimization/public/css/poole.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/syntax.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/lanyon.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/logo.png">
  <link rel="shortcut icon" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://simonseo.github.io/convex-optimization/convex-optimization/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item active" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/convex-optimization/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>01. Introduction</h1>






<!-- Get first post and show it -->

<p>Mathematical Optimization Problem -특히 Convex Optimization Problem-에 대한 소개.</p>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">01-01 Optimization problems?</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_2">01-02 Convex optimization problem</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_3">01-03 Goals and Topics</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_4">01-04 Brief history of convex optimization</a>
    </li>
  
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>01-01 Optimization problems?</h1>
            <p>최적화 문제(Optimization problems)란 여러개의 선택가능한 후보 중에서 최적의 해(Optimal value) 또는 최적의 해에 근접한 값을 찾는 문제를 일컫는다. 일반적으로 기계학습 분야에서는 비용함수(Cost function)를 최소화 또는 최대화 시키는 모델의 파라미터(parameter)를 구하게 되는데, 이것은 최적화 문제로 정의될 수 있다.</p>

<h2 id="mathematical-optimization-problems">Mathematical optimization problems</h2>
<p>Mathematical optimization problem은 다음과 같은 형태로 표현될 수 있다.</p>

<blockquote>
\[\begin{align*} 
&amp;\min_{x\in D}\ &amp;&amp; f(x) \\
&amp;\text{subject to} &amp;&amp; g_i(x) \le 0,\ i = 1, ...m \\
&amp;&amp;&amp; h_j(x) = 0,\ j = 1,\ ...r 
\end{align*}\]
</blockquote>

<p><strong>Mathematical Optimization Problem in standard form [3]</strong></p>

<ul>
  <li>\(x \in R^n\) is the optimization variable</li>
  <li>\(f: R^n \rightarrow R\) is the objective or cost function</li>
  <li>\(g_i: R^n \rightarrow R, i = 1, ..., m\) are the inequality constraint functions</li>
  <li>\(h_i: R^n \rightarrow R, j = 1, ..., r\) are the equality constraint functions</li>
</ul>

<p>위의 제약조건을 모두 만족하는 정의역(feasible domain)에서 objective function f를 최소로 만드는 벡터 \(x\)를 \(x^*\)로 표시하고 이를 optimal solution이라 부른다. [1]</p>

<p>제약조건의 경우 다음과 같이 두 가지로 구분될 수 있다. [2]</p>

<ol>
  <li>Explicit constraints: 말 그대로 optimization problem에 직접적으로 명시된 제약조건을 뜻한다. 위에서 서술한 optimization problem의 standard form에서 함수 \(g_i\)와 \(h_i\)로 표현된 제약조건이 이에 해당한다. 참고로 explicit constraint가 없는 문제를 unconstrained problem이라고 부른다.</li>
  <li>Implicit constraints: Optimization problem에 직접적으로 명시되지 않는 제약조건을 말한다. 이는 Objective function과 모든 constraint function들의 정의역에 대한 교집합이다.</li>
</ol>

<p>\(D = dom(f) \cap \bigcap_{i=1}^m {\rm dom}(g_i) \cap \bigcap_{j=1}^r dom(h_j)\)<br /></p>

<p><strong>Note:</strong> \(dom(f)\)는 함수 \(f\)의 정의역을 의미한다.</p>

<blockquote>
  <p><strong>Example: implicit constraint ↔ explicit constraint</strong></p>

  <p>최적화 문제가 다음과 같이 주어졌다고 하자.</p>

\[\begin{align*} \text{minimize } &amp; &amp; log(x) \end{align*}\]

  <p>여기서 objective function인 log함수의 정의역이 \(x &gt; 0\)이므로 \(x &gt; 0\)이 이 문제에서의 implicit constraint가 된다. 이 문제를 explicit constraint가 포함된 형태의 최적화문제로 표현하면 다음과 같다.</p>

\[\begin{align*} &amp;\text{minimize } &amp;&amp;log(x) \\ &amp;\text{subject to } &amp;&amp;x &gt; 0 \end{align*}\]
</blockquote>

<h2 id="applications">Applications</h2>

<p>최적화 문제는 다양한 영역에 걸쳐 적용될 수 있다. [2]</p>

<h4 id="portfolio-optimization">Portfolio optimization</h4>
<ul>
  <li>variables: 각 자산에 대한 투자금</li>
  <li>constraints: 예산, 자산당 최소/최대 투자가능 금액, 최소 수익</li>
  <li>objective: 전반적인 위험도 또는 주가 수익률 분산 (return variance)</li>
</ul>

<h4 id="device-sizing-in-electonic-circuits">Device sizing in electonic circuits</h4>
<ul>
  <li>variables: 각 부품의 너비와 길이</li>
  <li>constraints: 제조 공정상 제약사항, 최대 면적</li>
  <li>objective: 전력소비량</li>
</ul>

<h4 id="data-fitting">Data fitting</h4>
<ul>
  <li>variables: 모델 파라미터</li>
  <li>constraints: 사전 정보(e.g. 어떤 파라미터는 non-negative), 파라미터에 대한 제약사항</li>
  <li>objective: 예측값에 대한 에러</li>
</ul>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>01-02 Convex optimization problem</h1>
            <p>Convex optimization problem은 optimization problem의 한 종류이다.</p>

<blockquote>
\[\begin{align*} 
&amp;\min_{x\in D}\ &amp;&amp;f(x) \\
&amp;\text{subject to} &amp;&amp; g_i(x) \le 0,\ i = 1, ...m \\
&amp;&amp;&amp; h_j(x) = 0,\ j = 1,\ ...r 
\end{align*}\]
</blockquote>

<p><strong>Convex Optimization Problem in standard form [3]</strong></p>

<p>여기서 objective function \(f\)와 inequality constraint function \(g_i\)가 convex이고, equality constraint function \(h_j\)가 affine이라는 조건이 추가된다. 이때 affine function이란 다음과 같이 linear function에 상수합이 붙은 형태의 함수를 의미한다.</p>
<blockquote>
  <p>\(h_j,\ j = 1, ..., r\) are affine: \(h_j(x) = a_{j}^T x + b_{j},\ j=1, ..., r\)</p>
</blockquote>

<p>그렇다면 convex function은 어떤 함수를 의미하는 것일까? 이를 이해하기 위해서 convex set을 이해할 필요가 있다.</p>

<h2 id="convex-sets">Convex sets</h2>
<p>두 점 \(x_1\)과 \(x_2\)를 잇는 선분(line segment)은 다음과 같이 정의된다.</p>

<blockquote>
  <p>\(x = \theta x_1 + (1 - \theta) x_2\) with \(0 \le \theta \le 1\)</p>
</blockquote>

<p>어떤 집합(set)이 주어져 있다고 하자. 이 집합의 원소인 두 점 \(x_1\)과 \(x_2\)를 잇는 선분이 이 집합에 다시 포함될때 우리는 이 집합을 convex set이라고 부른다. 다시 말하면 집합 C가 convex가 될 조건은 다음과 같다.</p>

<blockquote>
  <p>\(x_1, x_2 \in C\), \(0 \le \theta \le 1\)  \(\Rightarrow\) \(\theta x_1 + (1-\theta)x_2 \in C\)</p>
</blockquote>

<p>예를 들어, 다음 세 가지 그림 중 가장 좌측의 그림만이 convex set에 해당한다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter01/Convex_set.png" alt="Convex Set" width="70%" />
  <figcaption style="text-align: center;">[Fig1] left: a convex set, mid &amp; right: non-convex sets [2]</figcaption>
</p>
</figure>

<h2 id="convex-functions">Convex functions</h2>
<p>Convex function은 다음과 같이 정의된다.</p>

<blockquote>
  <p>\(f: R^n \rightarrow R\) is convex if \(dom(f)\) is a convex set and,</p>

  <p>\(f(\theta x + (1 - \theta)y) \le \theta f(x) + (1-\theta)f(y)\) for all \(x, y \in dom(f),\ 0 \le \theta \le 1\)</p>
</blockquote>

<p>정의에서 부등식으로 표현된 조건은 다음과 같은 기하학적 의미를 가진다. \(f\)의 그래프 상의 임의의 두 점 \((x,\ f(x))\), \((y,\ f(y))\)을 생각해보자. 이 두 점을 잇는 선분은 구간 \([x, y]\)에서 그래프보다 크거나 같게 위치한다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter01/Convex_function.png" alt="Convex Function" width="70%" />
  <figcaption style="text-align: center;">[Fig2] Convex Function [2]</figcaption>
</p>
</figure>

<h2 id="relation-between-a-convex-set-and-a-convex-function">Relation between a convex set and a convex function</h2>
<p>convex function과 convex set 사이에는 다음과 같은 밀접한 관계가 있다.</p>
<blockquote>
  <p>함수 \(f\)의 epigraph가 convex set일때, 함수 \(f\)는 convex function이다.</p>
</blockquote>

<p>여기서 epigraph는 무엇을 의미하는 것일까? Epigraph에서 ‘Epi’는 ‘above’를 뜻하며, 곧 epigraph는 ‘above the graph’를 의미한다. 즉, epi \(f\)란 \(f\)의 그래프의 위쪽 영역에 해당하는 집합이다. 함수 epigraph는 다음과 같이 정의한다.</p>

<blockquote>
\[\eqalign{
&amp; \text{epigraph of } f: R^n \rightarrow R\\
&amp; \text{epi } f = \{(x, t) \in R^{n+1} \mid x \in \text{ dom } f, f(x) \le t\}
}\]
</blockquote>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter01/epigraph.png" alt="Epigraph" width="70%" />
  <figcaption style="text-align: center;">[Fig3] Epigraph [2]</figcaption>
</p>
</figure>

<p>함수 f가 convex function일때 epi f는 항상 convex set이고 이의 역도 성립한다. 이를 주지하고 위의 convex function과 convex set의 정의를 다시 한번 살펴보도록 하자.</p>

<h2 id="nice-property-of-convex-optimization-problems">Nice property of convex optimization problems</h2>
<p>Convex 함수의 local minimum은 항상 global minimum이다. convex optimization problem의 경우 non-convex optimization problem에 비해 일반적으로 solution을 더 쉽게 구할 수 있는데, 그 이유는 convex 함수가 다음과 같은 특성을 가지기 때문이다.</p>
<blockquote>
  <p>\(f\)가 convex이고 \(x\)가 \(f(x)\)의 locally optimal point일때(즉 \(f(x)\)가 local minimum), 사실 x는 globally optimal point이다.</p>
</blockquote>

<p>이를 한번 증명해보자.</p>

<blockquote>
  <p><strong>proof by contradiction:</strong></p>

  <p>Convex function f에 대해 \(x\)가 locally optimal point일때, 어딘가에 \(f(y) &lt; f(x)\)를 만족하는 feasible \(y\)가 있다고 가정하자. (이 가정이 참임이 증명된다면 ‘locally optimal point = global optimal point’가 성립하지 않을 것이다.)</p>

  <p>\(x\)가 locally optimal point라는 것은 다음을 만족하는 양수 \(R\)이 존재한다는 것과 같다: 
\(z\) feasible, \(\vert z - x \vert_2 \le R \Rightarrow f(z) \ge f(x)\)</p>

  <p>이때 \(z = \theta y + (1 - \theta) x\)라고 하면 (\(0 &lt; \theta &lt; 1\)),</p>

  <p>1.\(\phantom{1} z\)는 두 개의 feasible points \(x, y\)에 대한 convex combination*이므로 또한 feasible하다.</p>

  <p>2.\(\phantom{1}\)가정한 것처럼 \(f(y) &lt; f(x)\)가 성립한다면, 이는 곧 \(f(z) \le \theta f(y) + (1 - \theta) f(x) &lt; \theta f(x) + (1 - \theta) f(x) = f(x)\)</p>

  <p>2는 x가 locally optimal point이기 위한 전제조건 \(f(z) \ge f(x)\)에 대한 모순이므로 \(f(y)&lt;f(x)\)를 만족하는 feasible y는 존재하지 않는다. 즉, locally optimal point x가 곧 globally optimal point임을 의미한다.</p>
</blockquote>

<h2 id="convex-combination">convex combination</h2>

<blockquote>
  <p>\(x_1, ..., x_k\)에 대한 convex combination x는 다음과 같이 정의된다.</p>

  <p>\(x = \theta_1 x_1 + \theta_2 x_2 + ... + \theta_k x_k\) with \(\theta_1 + ... + \theta_k = 1, \theta_i \ge 0\)</p>

  <p>\(D\)가 convex set일때 \(x_1, x_2, ..., x_k \in D\)이면, \(x \in D\)이다.</p>
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>01-03 Goals and Topics</h1>
            <h2 id="goals">Goals</h2>
<p>앞으로의 학습과정을 통해 다음과 같은 능력이 배양되는 것을 목표로 한다.</p>

<ul>
  <li>주어진 문제상황이 convex optimization problem에 해당함을 파악(recognize)</li>
  <li>주어진 문제상황을 convex optimization problem으로 표현(formulate)</li>
  <li>정의된 convex optimization problem을 풀기위한 가장 적절한 알고리즘을 선택</li>
</ul>

<h2 id="topics">Topics</h2>
<p>위와 같은 목표를 달성하기 위해 다음과 같은 주제들이 다룰 것이다.</p>

<ul>
  <li>convex sets, functions, optimization problems</li>
  <li>examples and applications</li>
  <li>algorithms</li>
</ul>

<p>특히 위의 주제들 중에서, algorithm에 대한 내용이 주를 이루게 될 것이다.</p>

<h2 id="algorithms">Algorithms</h2>
<p>보통 최적화 문제를 풀기 위해서는 굉장히 다양한 방법이 적용될 수 있다. 서로 다른 방법들은 각각 정의된 문제의 성질에 따라 성능이 달라질 수 있다. 즉, 문제를 해결하기 가장 효율적인 알고리즘을 선택하기 위해서는 주어진 문제와 각 알고리즘에 대한 깊은 이해가 필요하다. Total variation denoising을 예로 들어보자.</p>

<h4 id="example-total-variation-denoising">Example: Total variation denoising</h4>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter01/2d_fused_lasso.png" alt="2D Fused Lasso" width="70%" />
  <figcaption style="text-align: center;">[Fig1] Total Variation Denoising [3]</figcaption>
</p>
</figure>

<p>노이즈가 잔뜩 낀 이미지 Data(중간)를 받았을 때, 그 이미지에서 노이즈를 제거하고 True Image(좌측)에 가까운 Solution(우측)을 얻고 싶은 상황이라고 가정하자. 각 pixel값을 \(y_i, i = 1, ..., n\)라고 한다면 이 문제는 다음과 같은 최적화 문제로 정의될 수 있으며, 이는 보통 2d fused lasso 또는 2d total variation denoising problem으로 불린다.</p>

<blockquote>
\[\min_{\theta} \frac{1}{2} \Sigma_{i=1}^n (y_i - \theta_{i})^2 + \lambda \Sigma_{(i,j) \in E} \vert \theta_i - \theta_j \vert\]
</blockquote>

<ul>
  <li>E: 인접한 모든 \(\theta\) 사이의 Edge들을 모아둔 집합</li>
  <li>\(\frac{1}{2} \Sigma_{i=1}^n (y_i - \theta_{i})^2\): Least squares loss. \(\theta\)가 \(y\)에 가까워지게 한다.</li>
  <li>\(\Sigma_{(i,j) \in E} \vert \theta_i - \theta_j \vert\): Total variation smoothing. 인접한 pixel 간 값의 변화가 이미지 전반에 거쳐 그리 많지 않을때 (piecewise constant) 이용할 수 있는 방법이다. 이와 같이 올바른 smoothing 기법의 선택을 위해서는 대상의 특성이 충분히 고려되어야 한다. (Total variation smoothing에 대한 더 자세한 설명은 참고문헌 1의 챕터 6.1.2와 6.3에서 볼 수 있다.)</li>
</ul>

<p>앞서 정의된 convex optimization problem은 <a href="http://stanford.edu/~boyd/admm.html">Specialized ADMM</a> 알고리즘을 이용하면 20번의 iteration으로 우측과 같은 solution을 얻을 수 있다.</p>

<h4 id="specialized-admm-20-iterations">Specialized ADMM, 20 iterations</h4>
<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter01/result1.png" alt="Result1" width="50%" />
  <figcaption style="text-align: center;">[Fig2] Specialized ADMM Result [3]</figcaption>
</p>
</figure>

<h4 id="proximal-gradient-descent-1000-iterations">Proximal gradient descent, 1000 iterations</h4>
<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter01/result2.png" alt="Result2" width="50%" />
  <figcaption style="text-align: center;">[Fig3] Proximal Gradient Descent Result [3]</figcaption>
</p>
</figure>

<h4 id="coordinate-descent-10k-cycles">Coordinate descent, 10K cycles</h4>
<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter01/result3.png" alt="Result3" width="50%" />
  <figcaption style="text-align: center;">[Fig4] Coordinate Descent Result [3]</figcaption>
</p>
</figure>

<p>위 결과에서 알 수 있듯이 2d fused lasso problem에 대해서는 세 가지 방법 중 Specialized ADMM이 가장 좋은 성능을 발휘한다. 하지만 문제가 달라지면 다른 두 방법이 Specialized ADMM을 압도하는 경우도 발생할 수 있다. 이후의 챕터에서는 다양한 알고리즘과 문제를 분석하여 적절한 알고리즘을 선택하는 방법에 대해 알아볼 것이다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>01-04 Brief history of convex optimization</h1>
            <h2 id="theory-convex-analysis">Theory (convex analysis)</h2>
<p>ca1900 - 1970</p>

<h2 id="algorithms">Algorithms</h2>

<ul>
  <li>1947: simplex algorithm for linear programming (Dantzig)</li>
  <li>1960s: early interior-point methods (Fiacco &amp; McCormick, Dikin, . . . )</li>
  <li>1970s: ellipsoid method and other subgradient methods</li>
  <li>1980s: polynomial-time interior-point methods for linear programming (Karmarkar 1984)</li>
  <li>late 1980s–now: polynomial-time interior-point methods for nonlinear convex optimization (Nesterov &amp; Nemirovski 1994)</li>
</ul>

<h2 id="applications">Applications</h2>
<ul>
  <li>before 1990: mostly in operations research; few in engineering</li>
  <li>since 1990: many new applications in engineering (control, signal processing, communications, circuit design, . . . ); new problem classes (semidefinite and second-order cone programming, robust optimization)</li>
</ul>

        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/convex-optimization/public/js/script.js'></script>
  </body>
</html>
