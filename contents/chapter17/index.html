<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      Primal-Dual Interior-Point Methods &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://convex-optimization-for-all.github.io/public/logo.png">
  <link rel="shortcut icon" href="https://convex-optimization-for-all.github.io/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://convex-optimization-for-all.github.io/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="https://convex-optimization-for-all.github.io/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>17. Primal-Dual Interior-Point Methods</h1>






<!-- Get first post and show it -->

<p>본 장에서는 앞서 배운 Barrier method의 centering step을 한 단계로 줄여서 성능을 개선한 <strong>Primal-Dual Interior-Point Method</strong>를 살펴볼 것이다.</p>

<p><strong>Primal-Dual Interior-Point Method</strong>는 centering step에서 반드시 feasible해야 한다는 제약조건을 완화하고  Newton’s Method의 root finding 버전을 이용하여 비선형 방정식을 선형 방정식으로 근사하여 해를 구하는 방식으로 Barrier method에 비해 빠르고 정확도가 높다.</p>

<h2 id="references-and-further-readings">References and further readings</h2>
<ul>
  <li>S. Boyd and L. Vandenberghe (2004), “Convex optimization,” Chapter 11</li>
  <li>S. Wright (1997), “Primal-dual interior-point methods,” Chapters 5 and 6</li>
  <li>J. Renegar (2001), “A mathematical view of interior-point methods”</li>
  <li>Y. Nesterov and M. Todd (1998), “Primal-dual interior-point methods for self-scaled cones.” SIAM J. Optim.</li>
</ul>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">17-01 Barrier method & duality & optimality revisited</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_2">17-02 Primal-dual interior-point method</a>
    </li>
  
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_3"> 17-02-01 Central path equations and Newton step</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_4"> 17-02-02 Surrogate duality gap, residuals</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_5"> 17-02-03 Primal-Dual Algorithm</a>
    </li>
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_6">17-03 Some history</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_7">17-04 Special case, linear programming</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_8">17-05 Optimality conditions for semideﬁnite programming</a>
    </li>
  
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>17-01 Barrier method & duality & optimality revisited</h1>
            <p>15장에서 barrier method에 대해, 13장과 16장에서는 duality에 대해 살펴보았다.
본 장의 내용을 다루기 전에 barrier method와 duality에 대해 간단하게 다시 정리해 보고자 한다.</p>

<h2 id="barrier-method">Barrier method</h2>
<p>아래와  같은 primal 문제가 convex이고 \(f, h_i , i = 1, . . . m\)가 미분가능 할 때,</p>
<blockquote>
\[\begin{align}
&amp;\min_{x} &amp;&amp; f(x) \\
&amp;\text{subject to } &amp;&amp;h_{i}(x) \leq 0, i = 1, \dotsc, m \\
&amp;&amp;&amp; Ax = b \\
\end{align}\]
</blockquote>

<p>Log barrier function을 사용하여 다음과 같이 primal 문제를 barrier 문제로 바꿀 수 있다.</p>

<blockquote>
\[\begin{align}
&amp; \min_{x} &amp;&amp; f(x) + \frac{1}{t} \phi(x) &amp; \qquad &amp; \min_{x} &amp;&amp; tf(x) + \phi(x) \\
&amp; \text{subject to } &amp;&amp; Ax = b &amp; \iff \qquad &amp; \text{subject to } &amp;&amp; Ax = b \\
&amp; \text{where } &amp;&amp; \phi(x) = - \sum_{i=1}^{m} \log(-h_i(x))
\end{align}\]
</blockquote>

<p>알고리즘은 \(t &gt; 0\)를 만족하는 \(t = t^{(0)}\)에서 시작해서 \(\frac{m}{t}\)가 \(\epsilon\)보다 작거나 같아질 때까지 증가시킨다. 이때, Newton’s method를 이용해 초기값 \(x^{(0)}\)에 대한 \(x^{\star}(t)\)를 구하고 \(k = 1, 2, 3, . . .\)에 대해 각 단계에서  \(x^{(k+1)} = x^{\star}(t)\)를 구하는 과정을 반복 한다.</p>

<p>알고리즘을 간략히 정리하면 다음과 같다.</p>

<ol>
  <li>\(t^{(0)} \gt 0\)이고 \(k := 0\)을 선택한다.</li>
  <li>\(t = t^{(0)}\)에서 barrier problem을 풀어서 \(x^{(0)} = x^{\star}(t)\)을 구한다.</li>
  <li>While \(m/t \gt \epsilon\) <br />
  3-1. \(t^{(k+1)} = µt\)로 업데이트 한다. \((µ &gt; 1)\) <br />
  3-2. Newton’s method를 \(x^{(k)}\)로 초기화한다. (warm start)<br />
     \(t = t^{(k+1)}\)에서 barrier problem을 풀어서 \(x^{(k+1)} = x^{\star}(t)\)을 구한다.<br />
  end while<br /></li>
</ol>

<ul>
  <li>자세한 내용은  <a href="/contents/chapter15/2021/03/28/15_01_02_log_barrier_function_and_barrier_method/">15-01-02 Log barrier function &amp; barrier method</a> 참조</li>
</ul>

<h2 id="duality">Duality</h2>
<p>다음과 같은 primal 문제가 주어졌을 때,</p>
<blockquote>
\[\begin{align}
   \mathop{\text{minimize}}_x &amp;\quad f(x) \\\\
   \text{subject to} &amp;\quad f Ax = b \\\\
   &amp;\quad h(x) \le 0
\end{align}\]
</blockquote>

<p>이를 Lagrangian 형태로 바꾸면 다음과 같이 바꿀 수 있다.</p>
<blockquote>
\[L(x,u,v) = f(x) + u^Th(x) + v^T(Ax - b)\]
</blockquote>

<p>이와 같이 정의된 Lagrangian을 이용해서 primal과 dual problem을 다음과 같은 형태로 다시 정의할 수 있다. 자세한 내용은 16장을 다시 살펴보기 바란다.<br /></p>
<h4 id="primal-problem">Primal Problem</h4>
<blockquote>
\[\min_x \mathop{\max_{u,v}}_{u \geq 0} L(x,u,v)\]
</blockquote>

<h4 id="dual-problem">Dual problem</h4>
<blockquote>
\[\mathop{\max_{u,v}}_{u \geq 0} \min_x L(x,u,v)\]
</blockquote>

<h2 id="optimality-conditions">Optimality conditions</h2>

<p>\(f,h_1,...h_m\)은 convex 이고 미분 가능하고, 또한 주어진 문제가 strong duality를 만족한다고 가정할 때, 이 문제에 대한 KKT 최적 조건(optimality condition)은 아래와 같다.</p>

<blockquote>
\[\begin{array}{rcl}
∇f(x) +∇h(x)u + A^Tv &amp; = &amp; 0 &amp; \text{(Stationarity)}\\\
 Uh(x) &amp; = &amp; 0 &amp; \text{(Complementary Slackness)} \\\
Ax &amp; = &amp; b &amp; \text{(Primal Feasibility)}\\\
u,−h(x)  &amp; ≥ &amp; 0 &amp; \text{(Dual Feasibility)}
\end{array}\]
</blockquote>

<p>여기서 \(U\)는 \(\text{diag}(u)\)를 뜻하며, \(∇h(x)\)는 \([ ∇h_1(x) ··· ∇h_m(x) ]\)를 의미한다.</p>

<ul>
  <li>자세한 내용은 <a href="/contents/chapter12/2021/04/02/12_00_KKT_conditions/">12장 KKT conditions</a> 참조</li>
</ul>

<h2 id="central-path-equations">Central path equations</h2>
<p>함수 \(f(x)\)를 barrier 문제로 아래와 같이 재정의 할 수 있다.<br />
아래 수식에서 \(τ\)는 \(\frac{1}{t}\)이며 \(\tau\)를 점점 0에 가깝게 해서 반복적으로 해를 구함으로써 최종적으로 원래 문제의 해를 구하게 된다.</p>

<blockquote>
\[\begin{align}
&amp;\min_{x} &amp;&amp; {f(x) + τ\phi(x)} \\\\
&amp; &amp;&amp;{Ax = b} \\\
&amp; \text{where } &amp;&amp; \phi(x) = −\sum_{i=1}^m \log(−h_i(x)).
\end{align}\]
</blockquote>

<p>즉, 위 식에서 \(τ\)에 따라 primal 문제와의 차이가 발생하며, \(τ\)에 따라 생기는 궤적 즉, barrier 문제에 대한 해의 집합을 central path라고 한다.</p>

<p>그리고 이 barrier 문제에 대한 optimality conditions은 다음과 같다.</p>
<blockquote>
\[\begin{array}{rcl}
∇f(x) +∇h(x)u + A^Tv  &amp; = &amp; 0 \\\
Uh(x) &amp; = &amp; −τ\mathbb{1} \\\
Ax &amp; = &amp; b \\\
u,−h(x)  &amp; &gt; &amp; 0
\end{array}\]
</blockquote>

<ul>
  <li>자세한 내용은 <a href="/contents/chapter16/2021/03/31/16_02_optimality_conditions/">16-02 Optimality conditions</a> 참조</li>
</ul>

<p>이번 장에서 소개할 <strong>Primal-Dual interior point method</strong>는 위의 처음 세 가지 식을 residual로 정의하고 이를 \(0\)으로 줄이면서 해를 구하는 방식이다.</p>

<h5 id="useful-fact">Useful fact</h5>
<p>솔루션 \((x(τ),u(τ),v(τ))\)는 다음의 \(mτ\) 즉 \(\frac{m}{t}\) 크기 만큼의 duality gap을 갖는다.</p>
<blockquote>
\[f(x(τ))−\min_x L(x,u(τ),v(τ)) = mτ= \frac{m}{t}\]
</blockquote>


        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>17-02 Primal-dual interior-point method</h1>
            <p>Barrier method와 같이 <strong>primal-dual interior-point method</strong>도 central path 위의 점을 (근사적으로) 계산하는 것을 목표로 한다. 그러나 두 가지 방법은 여러 차이점이 있다.</p>

<h2 id="primal-dual-interior-point-method와-barrier-method의-차이점">Primal-dual interior-point method와 barrier method의 차이점</h2>
<ul>
  <li>일반적으로 iteration 별로 <strong>한 번의 뉴턴 스텝</strong>을 실행한다. (즉, 센터링 스텝을 위한 추가 반복문이 없다.)</li>
  <li><strong>반드시 feasible일 필요는 없다</strong>.  (Backtracking line search를 통해 feasible한 곳으로 밀어준다.)</li>
  <li>일반적으로 <strong>더 효과적</strong>이다. 특히 적절한 조건 위에서 linear convergence보다 뛰어난 성능을 보인다.</li>
  <li>Barrier method에 비해 조금은 덜 직관적이다.</li>
</ul>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>17-02-01 Central path equations and Newton step</h1>
            <p><strong>Primal-dual interior-point method</strong>는 barrier method와 마찬가지로 central path를 찾아서 해를 구하는 방식이다. 그러기 위해 perturbed KKT conditions를 residual 함수로 정의하고 이를 0으로 만드는 해를 찾는다. 이 절에서는 이와 같은 접근 방식을 설명하려고 한다.</p>

<h2 id="central-path-equations">Central path equations</h2>
<p>앞의 <a href="/contents/chapter17/2021/05/01/17_01_barrier_method_duality_optimality_revisited/">17-01 Optimality conditions</a>에서 설명했던 central path equations에서 우항을 좌항으로 옮기면 다음과 같이 정리할 수 있다. (Central path equations의 optimality condition을 perturbed KKT conditions라고도 한다.)</p>
<blockquote>
\[\begin{array}{rcl}
∇f(x) +∇h(x)u + A^Tv &amp; = &amp; 0 \\\
 Uh(x) + \tau\mathbb{1}  &amp; = &amp; 0 \\\
Ax−b &amp; = &amp; 0 \\\
u,−h(x)  &amp; &gt; &amp; 0
\end{array}\]
</blockquote>

<p>원래 문제에 대한 KKT conditions에서의 complementary slackness와 inequality constraint가 perturbed KKT conditions에서와 다르다는 점을 유의해서 보자. 원래 문제의 경우 \(Uh(x) = 0\) 그리고 \(u,−h(x)  \ge 0\)이지만, perturbed KKT conditions에서는 \(Uh(x) = - \tau\mathbb{1}\) 그리고 \(u,−h(x)  \gt 0\)이다.</p>

<p>이렇게 정리된 비선형 방정식인 perturbed KKT conditions는 Newton’s method의 root finding 버전을 이용해서 선형 방정식으로 근사해서 해를 구할 수 있다.</p>

<h2 id="newton-step">Newton step</h2>
<p>그러면 perturbed KKT conditions를 선형으로 근사하여 해를 구해는 방법에 대해 알아보자. Perturbed KKT conditions 식을 다음과 같은 residual의 함수 \(r(x, u, v) = 0\)로 정의할 수 있다. (함수 이름이 residual인 이유는 이 값들이 0이 되어야 optimal이 되기 때문이다.)</p>

<blockquote>
\[r(x,u,v) :=
\begin{bmatrix}
∇f(x) +∇h(x)u + A^Tv \\\
Uh(x) + τ\mathbb{1} \\\
Ax−b
\end{bmatrix}, H(x) = \text{Diag}(h(x))\]
</blockquote>

<p>함수의 근을 찾기 위해 \(r(x, u, v)\)을 Taylor 1차식으로 근사하면 다음과 같다. (이 과정은 non-linear equation을 linear equation으로 근사하는 과정으로 자세한 내용은 <a href="/contents/chapter14/2021/03/26/14_01_newton_method/">14-02-01 Root finding</a>을 참조)</p>
<blockquote>
\[\begin{align}
0 &amp; = r(x + \Delta x, u + \Delta u, r + \Delta v)  \\\\
  &amp; \approx r(x, u, v) + \nabla r(x, u, v) 
\begin{pmatrix}
\Delta x \\\\
\Delta u \\\\
\Delta v \\\\
\end{pmatrix} \\\\
\end{align}\]
</blockquote>

<p>이에 따라 함수 \(r(x, u, v)\)은 다음과 같이 정리할 수 있다.</p>

<blockquote>
\[\begin{align}
\nabla r(x, u, v) 
\begin{pmatrix}
\Delta x \\\\
\Delta u \\\\
\Delta v \\\\
\end{pmatrix} = -r(x, u, v) \\\\
\end{align}\]
</blockquote>

<p>\(r(x, u, v)\)을 \(x, u, v\)에 대해 미분하여 Jacobian matrix \(\nabla r(x, u, v)\)을 구한 후 위의 식을 대입해 보면 아래와 같다.</p>
<blockquote>
  <p>\(\begin{bmatrix}
\nabla^2f(x) + \sum_i u_i \nabla^2h_i(x) &amp; \nabla h(x) &amp; A^T \\\
 U \nabla  h(x)^T &amp; H(x) &amp; 0 \\\
A &amp; 0 &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x \\\
\Delta u \\\
\Delta v
\end{bmatrix} = −r(x,u,v)\)
where
\(r(x,u,v) :=
\begin{bmatrix}
\nabla f(x) +\nabla h(x)u + A^Tv \\\
Uh(x) + τ\mathbb{1} \\\
Ax−b
\end{bmatrix}, H(x) = \text{Diag}(h(x))\)</p>
</blockquote>

<p>이 식의 해인 \((\Delta x, \Delta u, \Delta v)\)는 primal, dual 변수의 업데이트 방향이다. 이 장에서 소개할 방법을 <strong>Primal-Dual</strong> interior point method라고 부르는 이유는 residual 함수를 이용해서 primal, dual 변수를 동시에 업데이트하기 때문이다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>17-02-02 Surrogate duality gap, residuals</h1>
            <p>Primal-Dual 알고리즘을 정의하기 위해 먼저 세 가지 residual 종류와 surrogate duality gap을 정의해보자. Residual과 surrogate duality gap은 Primal-Dual 알고리즘에서 최소화해야 할 목표이다.</p>

<h2 id="residuals">Residuals</h2>
<p>\((x,u,v)\)에서의 dual, central, primal residual는 다음과 같이 정의된다.</p>

<blockquote>
  <p>\(r_{dual} = \nabla f(x) +\nabla h(x)u + A^Tv\\\)
\(r_{cent} =  Uh(x) + τ\mathbb{1} \\\) 
\(r_{prim} = Ax−b\)</p>
</blockquote>

<p>이들은 함수 \(r(x,u,v)\)의 각 row에 해당된다. <strong>Primal-dual interior point method</strong>는 이 세 가지 residual이 계속해서 0이 되게하기 보다는 0을 만족하는 방향으로 실행한다. 즉, 실행 과정에서 반드시 feasible일 필요는 없다는 이야기이다.</p>

<p>\(r_{dual}\)를 dual residual이라고 부르는 이유는 아래 식에서와 같이 \(r_{dual} = 0\)이면 \(u, v\)가 \(g\)의 domain에 있다는 것을 보장하게 되며 이는 곧 dual feasible임을 의미하기 때문이다.</p>

<blockquote>
\[\begin{align}
&amp; r_{dual} = \nabla f(x) +\nabla h(x)u + A^Tv = 0 \\\\
&amp; \iff \min_{x} L(x,u.v) = g(u,v) \\\\
\end{align}\]
</blockquote>

<p>비슷하게 \(r_{prim}=0\)을 만족하면 primal feasble하기 때문에 \(r_{prim}\)을 primal residual이라고 부른다.</p>

<h2 id="surrogate-duality-gap">Surrogate duality gap</h2>
<p>Barrier method는 feasible하기 때문에 duality gap이 존재하지만, primal-dual interior-point method는  반드시 feasible할 필요가 없기 때문에 <strong>surrogate duality gap</strong>을 사용한다. <strong>Surrogate duality gap</strong>은 다음 식으로 정의된다.</p>

<blockquote>
\[−h(x)^Tu  \quad \text{for} \quad h(x) \le 0, u \ge 0\]
</blockquote>

<p>만일 \(r_{dual} = 0\)이고  \(r_{prim} = 0\)라면 surrogate duality gap은 true duality gap이 된다. 즉, primal and dual feasible하면 surrogate duality gap은 실제 duality gap \(\frac{m}{t}\)과 같아진다.</p>

<p><strong>[참고] Perturbed KKT 조건과 파라미터 t</strong> <br /></p>

<ul>
  <li>Perturbed KKT 조건에서 파라미터 t는 \(t = −\frac{m}{h(x)^Tu}\)이다.</li>
  <li>자세한 내용은 <a href="/contents/chapter15/2021/03/28/15_03_01_perturbed_kkt_conditions/">15-03-01 Perturbed KKT conditions</a>와 <a href="/contents/chapter15/2021/03/28/15_03_02_suboptimality_gap/">15-03-02 Suboptimality gap</a>을 참조</li>
</ul>

<p>그리고, \(u &gt; 0,h(x) &lt; 0\)이고 아래의 조건을 만족하면 \((x,u,v)\)는 central path 상에 존재하게 된다.</p>

<blockquote>
  <p>\(r(x,u,v) = 0\) for \(\tau = -\frac{h(x)^Tu}{m}\)</p>
</blockquote>

<p>즉, central path 상에 존재하는 점에서 residual은 0이다.</p>


        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_5"></a>17-02-03 Primal-Dual Algorithm</h1>
            <p>Primal-Dual 알고리즘을 정의하기 위해 먼저 \(\tau(x,u)\)를 다음과 같이 정의하자</p>
<blockquote>
\[\tau(x,u) := -\frac{h(x)^Tu}{m} \quad \text{with} \quad h (x) \le 0, u \ge 0\]
</blockquote>

<p>참고로 Barrier method에서의 \(t\)와 \(\mu\)를 Primal-Dual 알고리즘에서는 \(\tau\)와 \(\sigma\)로 재정의하여 표기한다.</p>
<blockquote>
\[\tau = \frac{1}{t}, \quad \sigma = \frac{1}{\mu}\]
</blockquote>

<h2 id="primal-dual-algorithm">Primal-Dual Algorithm</h2>
<p>Primal-Dual 알고리즘은 다음과 같다.</p>
<blockquote>
  <ol>
    <li>\(\sigma\)를 선택 (\(\sigma ∈ (0,1)\))<br /></li>
    <li>\((x^0,u^0,v^0)\)를 선택 \((h(x^0) &lt; 0\). \(u^0 &gt; 0\))<br /></li>
    <li>다음 단계를 반복 (\(k = 0,1,...\))<br />
\(\quad\) * Newton step 계산 :<br />
\(\qquad \quad (x,u,v) = (x^k,u^k,v^k)\) <br />
\(\qquad \quad \tau := \sigma \tau(x^k,u^k)\) 계산<br />
\(\qquad \quad \tau\)에 대해 \((\Delta x,\Delta u,\Delta v)\) 계산<br />
\(\quad\) * Backtracking으로 step length \(θ_k\)를 선택<br />
\(\quad\) * Primal-Dual 업데이트 :<br />
\(\qquad \quad (x^{k+1},u^{k+1},v^{k+1}) := (x^k,u^k,v^k) + \theta_k(\Delta x,\Delta u,\Delta v)\)<br /></li>
    <li>종료 조건 : \(-h(x^{k+1})^Tu \le \epsilon\) and \((\parallel r_{prim} \parallel^2_2 + \parallel r_{dual} \parallel^2_2)^{1/2} \le \epsilon\) 조건을 만족하면 중지 <br /></li>
  </ol>
</blockquote>

<p>알고리즘은 각 단계 별로 Newton step을 실행하여 \((\Delta x,\Delta u,\Delta v)\)를 계산하고  Primal-Dual 업데이트를 하여 \((x^{k+1},u^{k+1},v^{k+1})\)를 구한다. 단, Backtracking line search를 통해 Primal-Dual 변수가 feasible해 지도록 \(θ_k\)를 선택한다. 알고리즘은 surrogate duality gap과 primal and dual residual이 \(\epsilon\) 보다 작아지면 종료한다.</p>

<h2 id="backtracking-line-search">Backtracking line search</h2>
<p>Primer-Dual 알고리즘에서 Newton step을 한번만 실행하기 때문에 정확한 해을 찾기 보다는 해의 방향을 구한 것으로 볼 수 있다. 따라서, 그 방향으로 이동하면서 feasible set으로 들어올 수 있도록 적절한 step length를 구해야 한다.</p>

<p>즉, 알고리즘의 각 스텝에서 \(θ\)를 구해서 primal-daual 변수를 업데이트한다.</p>

<blockquote>
\[x^+ = x + θ\Delta x, \quad  u^+ = u + θ\Delta u, \quad v^+ = v + θ\Delta v\]
</blockquote>

<p>이 과정에는 두 가지 주요 목표가 있다.</p>

<ul>
  <li>\(h(x) &lt; 0, u &gt; 0\)의 조건을 유지하는 것</li>
  <li>\(\parallel r(x,u,v) \parallel\)을 감소시키는 것</li>
</ul>

<p>이를 위해 다단계 백트랙킹 선형 검색(<strong>multi-stage backtracking line search</strong>)을 사용한다.</p>

<h4 id="stage-1-dual-feasiblity-u-gt-0">Stage 1: dual feasiblity \(u \gt 0\)</h4>
<p>처음에는 \(u + \theta \Delta u ≥ 0\)를 만족하는 가장 큰 스텝 \(\theta_{max} ≤ 1\)으로 시작한다.</p>

<blockquote>
\[\theta_{\max} = \min \Biggl\{1,\  \min \Bigl\{ −\frac{u_i}{\Delta u_i} : ∆u_i &lt; 0 \Bigr\} \Biggr\}\]
</blockquote>

<p>위의 식은 다음과 같이 유도된다.</p>

<blockquote>
\[\begin{align}
&amp;u + \theta \Delta u &amp;&amp; \ge 0  \\\\
\Leftrightarrow \quad &amp;u &amp;&amp; \ge -\theta \Delta u \\\\
\Leftrightarrow \quad &amp;- u/\Delta u &amp;&amp; \ge \theta \quad  \text{ such that }-\Delta u \gt 0  \\\\
\end{align}\]
</blockquote>

<p>이는 \(u\)를 feasible하게 만드는 과정이다.</p>

<h4 id="stage-2-primal-feasiblity-hx-lt-0">Stage 2: primal feasiblity \(h(x) \lt 0\)</h4>
<p>그 다음엔 파라미터  \(\alpha, \beta \in (0,1)\)로 하고 \(\theta\)를 \(0.99\theta_{max}\)로 설정한 후 다음 업데이트를 수행 한다.</p>

<ul>
  <li>\(h_i(x^+) &lt; 0, i = 1,...m\)를 만족할 때까지, \(θ = βθ\)를 업데이트 <br /></li>
</ul>

<p>이는 \(x\)를 feasible하게 만드는 과정이다.</p>

<h4 id="stage-3--reduce-parallel-rxuv-parallel">Stage 3 : reduce \(\parallel r(x,u,v) \parallel\)</h4>
<ul>
  <li>\(\| r(x^+,u^+,v^+) \| ≤ (1−\alpha \theta) \| r(x,u,v) \|\)를 만족할 때까지, \(\theta = \beta \theta\)를 업데이트</li>
</ul>

<p>Stage 3의 update 식은 기존의 backtracking line search 알고리즘과 동일하다.</p>

<p>위의 식에서 우항은 다음과 같이 유도될 수 있다. 먼저 Newton’s method에서 다음 결과를 얻는다.</p>
<blockquote>
\[\begin{align}
\Delta w = (\Delta x, \Delta u, \Delta v) &amp;\approx -r^{'}(w)^{-1} r(w) \\\\
\Leftrightarrow r(w)  &amp;\approx  -r^{'}(w) \Delta w \\\\
\end{align}\]
</blockquote>

<p>위의 식에서 \(r^{'}(w) \Delta w \approx -r(w)\)이므로 이를 아래 Taylor 1차 근사식에 대입한다.</p>
<blockquote>
\[\begin{align}
r(w + \theta \Delta w) &amp; \approx r(w) +  r^{'}(w) (\theta \Delta w) \\\\
&amp;\approx (1-\theta) r(w) \\\\
\end{align}\]
</blockquote>

<p>결과적으로 \(r(w + \alpha \theta \Delta w) \approx (1-\alpha  \theta) r(w)\)가 된다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_6"></a>17-03 Some history</h1>
            <p>일반적으로 현대의 state-of-art LP Solver들은  Simplex method와 interior-point method를 모두 사용하고 있다.</p>

<ul>
  <li>
    <p>Dantzig(1940년대): Simplex 방법, LP의 general form을 푼 최초의 방식으로 Iteration 없이 exact solution을 구한다. 오늘날까지도 LP를 위한 가장 잘 알려지고 많이 연구되는 알고리즘 중 하나이다.</p>
  </li>
  <li>
    <p>Klee 및 Minty(1972) : \(n\)개의 변수와 \(2n\)개의 제약 조건을 갖는 pathological LP. Simplex method로 풀려면 \(2^n\)번 반복이 필요하다.</p>
  </li>
  <li>
    <p>Khachiyan (1979) : Nemirovski와 Yudin (1976)의 타원체 방법을 기반으로 한 LP의 polynormial-time 알고리즘으로 이론적으로는 강하나, 실제 문제에서는 그렇지 못하다.</p>
  </li>
  <li>
    <p>Karmarkar (1984) : interior-point polynomial-time LP 방법으로 상당히 효과적이며 breakthrough가 된 연구이다. (미국 특허 4,744,026, 2006년 만료).</p>
  </li>
  <li>
    <p>Renegar (1988) : LP를위한 Newton 기반 interior-point 알고리즘. Lee-Sidford의 최신 연구가 나올 때까지 이론적으로 가장 좋은 계산 복잡도를 갖고 있었다.</p>
  </li>
</ul>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_7"></a>17-04 Special case, linear programming</h1>
            <p>이 절에서는 LP(linear programming) 문제에 대한 Primer-Dual method의 예시를 살펴보자.</p>

<h2 id="linear-programming">Linear programming</h2>
<p>다음과 같은 primal LP 문제가 있다.</p>
<blockquote>
\[\begin{align}
   &amp;\min_{x} &amp;&amp; {c^Tx} \\\\
   &amp;\text{subject to } &amp;&amp; {Ax = b} \\\\
   &amp; &amp;&amp;{x ≥ 0} \\\
\end{align}\]

\[\text{for } c ∈R^n, A ∈R^{m×n}, b ∈R^m\]
</blockquote>

<p>위 primal LP 문제의 dual 문제는 아래와 같다.</p>
<blockquote>
\[\begin{align}
   &amp;\max_{y,s}  &amp;&amp; {b^Ty} \\\\
   &amp;\text{subject to } &amp;&amp; {A^Ty + s = c} \\\\
   &amp; &amp;&amp;{s ≥ 0} \\\
\end{align}\]
</blockquote>

<h2 id="optimality-conditions-and-central-path-equations">Optimality conditions and central path equations</h2>
<p>다음은 이전 LP의 primal-dual problem에 대한 최적 조건(KKT Conditions)을 보여준다.</p>
<blockquote>
\[\begin{array}{rcl}
A^Ty + s &amp; = &amp; c \\\
Ax &amp; = &amp; b \\\
XS\mathbb{1} &amp; = &amp; 0 \\\
x,s  &amp; \succeq &amp; 0
\end{array}\]
</blockquote>

<p>Central path equations</p>
<blockquote>
\[\begin{array}{rcl}
A^Ty + s &amp; = &amp; c \\\
Ax &amp; = &amp; b \\\
XS\mathbb{1} &amp; = &amp; τ\mathbb{1} \\\
x,s  &amp; &gt; &amp; 0
\end{array}\]
</blockquote>

<h2 id="primal-dual-method-vs-barrier-method">Primal-dual method vs. barrier method</h2>
<h4 id="newton-steps-for-primer-dual-method">Newton steps for primer-dual method</h4>
<p>다음은 LP문제에 대한 primal-dual method의 Newton 방정식이다.</p>

<blockquote>
\[\begin{bmatrix}
0 &amp; A^T &amp; I \\\
A &amp; 0 &amp; 0 \\\
S &amp; 0 &amp; X 
\end{bmatrix}
\begin{bmatrix}
∆x \\\
∆y \\\
∆s 
\end{bmatrix}= −
\begin{bmatrix}
A^Ty + s−c \\\
Ax−b \\\
XS\mathbb{1}−τ\mathbb{1} 
\end{bmatrix}\]
</blockquote>

<p>Optimal condition에서 다음 관계를 알 수 있다.</p>

\[XS\mathbb{1} = \tau \mathbb{1} \iff s = \tau X^{−1}\mathbb{1} \iff x = \tau S^{−1}\mathbb{1}\]

<p>이에 따라 \(s\)를 제거하여 primal barrier problem에 대한 최적 조건을 얻거나, \(x\)를 제거하여 dual barrier problem에 대한 최적 조건을 얻을 수 있다.</p>

<h4 id="newton-steps-for-barrier-problems">Newton steps for barrier problems</h4>
<p>다음은 barrier problem에 대한 primal과 dual central path equation이다. (왼쪽이 primal 오른쪽이 dual)</p>
<blockquote>
\[\begin{array}{rcr}
A^Ty + τX^{−1}1 &amp; = &amp; c &amp; \qquad \qquad &amp; A^Ty + s &amp; = &amp; c \\\
Ax &amp; = &amp; b &amp; \qquad \qquad &amp; τAS^{−1}\mathbb{1} &amp; = &amp; b\\\
x &amp; &gt; &amp; 0 &amp; \qquad \qquad &amp; s &amp; &gt; &amp; 0
\end{array}\]

</blockquote>

<p>위의 central path equation으로 primal과 dual에 대한 Newton step을 구해보면 다음과 같다.</p>

<p><strong>Primal Newton step</strong></p>
<blockquote>
\[\begin{bmatrix}
τX^{−2} &amp; A^T \\\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
∆x \\\
∆y
\end{bmatrix}= −
\begin{bmatrix}
A^Ty + τX^{−1}\mathbb{1}−c \\\
Ax−b 
\end{bmatrix}\]
</blockquote>

<p><strong>Dual Newton step</strong></p>
<blockquote>
\[\begin{bmatrix}
A^T &amp; I \\\
0 &amp; τAS^{−2}
\end{bmatrix}
\begin{bmatrix}
∆y \\\
∆s
\end{bmatrix}= −
\begin{bmatrix}
A^Ty + s −c \\\
τAS^{−1}\mathbb{1}−b
\end{bmatrix}\]
</blockquote>

<h2 id="example-barrier-versus-primal-dual">Example: barrier versus primal-dual</h2>
<h4 id="standard-lp--n--50-m--100">Standard LP : \(n = 50\), \(m = 100\)</h4>
<p>Primal-dual method의 성능을 확인하기 위해 변수가 \(n = 50\)이고 equality constraint가 \(m = 100\)인 표준 LP문제에 대한 예시를 살펴보자. (Example from B &amp; V 11.3.2 and 11.7.4)</p>

<p>Barrier method는 다양한  \(\mu\)값(2, 50, 150)을 사용한 반면 primal-dual method에서는 \(\mu\)를 10으로 고정하였다.
그리고 두 방법 모두 \(\alpha = 0.01, \beta = 0.5\)를 사용했다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter17/barrier_vs_primal_dual.png" />
  <figcaption style="text-align: center;">[Fig1] Duality gap (Barrier vs. Primal-dual) [1]</figcaption>
</p>
</figure>

<p>그래프에서 보다시피 primal-dual은 빠르게 수렴하면서도 높은 정확도를 보인다.</p>

<h4 id="sequence-of-problem--n--2m-and-n-growing">Sequence of problem : \(n = 2m\) and \(n\) growing.</h4>
<p>이제 \(n = 2m\)이고 \(n\)이 점점 증가하는 일련의 문제에 대해 성능을 살펴보자.</p>

<ul>
  <li>Barrier method는 \(\mu = 100\)를 사용하였고 outer loop는 2회 정도만 수행되었다. (duality gap은 \(10^4\)로 감소하였다)</li>
  <li>Primal-dual method는 \(\mu = 10\)를 사용하였고 duality gap과 feasibility gap이 거의 \(10^{−8}\)일 때 실행을 중단했다.</li>
</ul>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter17/barrier_vs_primal_dual2.png" />
  <figcaption style="text-align: center;">[Fig2] Newton iteration (Barrier vs. Primal-dual) [1]</figcaption>
</p>
</figure>

<p>위 그림에서 알 수 있듯이 Primal-dual 방법은 더 높은 정확도를 갖는 솔루션 찾지만 약간의 iteration이 추가적으로 필요하다.</p>


        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_8"></a>17-05 Optimality conditions for semideﬁnite programming</h1>
            <p>이 절에서는 SDP(semideﬁnite programming) 문제에 대한 Primer-Dual method의 예시를 살펴보려고 한다.</p>

<h2 id="sdp-semideﬁnite-programming">SDP (semideﬁnite programming)</h2>
<p>SDP의 primal 문제는 다음과 같이 정의한다.</p>
<blockquote>
\[\begin{align}
   &amp;\min_{x} &amp;&amp; {C \cdot X} \\\\
   &amp;\text{subject to } &amp;&amp; {A_i \cdot X = b_i, i = 1,...,m} \\\\
   &amp; &amp;&amp;{X \succeq 0}
\end{align}\]
</blockquote>

<p>SDP의 dual 문제는 다음과 같이 정의한다.</p>
<blockquote>
\[\begin{align}
   &amp;\max_{y} &amp;&amp; {b^Ty} \\\\
   &amp;\text{subject to } &amp;&amp; {\sum^m_{X_i=1} y_iA_i + S = C} \\\\
   &amp; &amp;&amp;{S \succeq 0}
\end{align}\]
</blockquote>

<p>참고로 \(\mathbb{S}^n\)의 trace inner product는 다음과 같이 표기한다.</p>
<blockquote>
\[X \cdot S = \text{trace}(XS)\]
</blockquote>

<h2 id="optimality-conditions-for-sdp">Optimality conditions for SDP</h2>
<p>SDP의 primal과 dual 문제는 다음과 같이 linear map을 이용해서 정의할 수 있다.</p>

<blockquote>
\[\begin{align}
   &amp;\min_{x} &amp;&amp; {C \cdot X} &amp; \qquad \qquad \qquad &amp; \max_{y,S}  &amp;&amp; {b^Ty} \\\\
   &amp;\text{subject to } &amp;&amp; {\mathcal{A}(X) = b} &amp; \qquad \qquad \qquad &amp; \text{subject to } &amp;&amp; {\mathcal{A}^{∗}(y) + S = C} \\\\\
   &amp; &amp;&amp;{X \succeq 0} &amp; \qquad \qquad \qquad &amp; &amp;&amp;{S \succeq 0}
\end{align}\]
</blockquote>

<p>여기서 \(\mathcal{A}: \mathbb{S}^n → \mathbb{R}^m\) 는 linear map을 의미한다.</p>

<p>Strong duality를 만족한다고 가정했을 때,  \(X^{\star}\) 와 \((y^{\star}, S^{\star})\)는 \((X^{\star}, y^{\star}, S^{\star})\)의 솔루션은 primal과 dual의 최적 솔루션이며 그역도 성립한다.</p>

<blockquote>
\[\begin{array}{rcl}
\mathcal{A}^∗(y) + S &amp; = &amp; C \\\
\mathcal{A}(X) &amp; = &amp; b \\\
XS &amp; = &amp; 0 \\\
X,S &amp; \succeq &amp; 0
\end{array}\]
</blockquote>

<h2 id="central-path-for-sdp">Central path for SDP</h2>
<p><strong>Primal barrier problem</strong></p>
<blockquote>
\[\begin{align}
   &amp;\min_{x} &amp;&amp; {C \cdot X−τ \log(det(X))} \\\\
   &amp;\text{subject to } &amp;&amp; {A(X) = b} 
\end{align}\]
</blockquote>

<p><strong>Dual barrier problem</strong></p>
<blockquote>
\[\begin{align}
   &amp;\max_{y, S} &amp;&amp; {b^Ty + τ \log(det(S))} \\\\
   &amp;\text{subject to } &amp;&amp; {\mathcal{A}^∗(y) + S = C} 
\end{align}\]
</blockquote>

<p><strong>Primal &amp; dual을 위한 Optimality conditions</strong></p>
<blockquote>
\[\begin{array}{rcl}
\mathcal{A}^∗(y) + S &amp; = &amp; C \\\
\mathcal{A}(X) &amp; = &amp; b \\\
XS &amp; = &amp; τI \\\
X,S &amp; \succ &amp; 0
\end{array}\]
</blockquote>

<h2 id="newton-step">Newton step</h2>
<p>Primal central path equations</p>
<blockquote>
\[\begin{array}{rcl}
\mathcal{A}^∗(y) + \tau X^{−1} &amp; = &amp; C \\\
\mathcal{A}(X) &amp; = &amp; b \\\
X &amp; \succ &amp; 0
\end{array}\]
</blockquote>

<p>Newton equations</p>
<blockquote>
  <p>\(τX^{−1}\Delta XX^{−1} +\mathcal{A}^∗(\Delta y) = −(\mathcal{A}^∗(y) + \tau X^{−1} −C)\)
\(\mathcal{A}(\Delta X) = −(\mathcal{A}(X)−b)\)</p>
</blockquote>

<p>Dual에 대한 central path equation과 Newton equation도 \((y,S)\)를 포함해서 이와 유사하게 정의된다.</p>

<h2 id="primal-dual-newton-step">Primal-dual Newton step</h2>
<p>Primal central path equations</p>
<blockquote>
\[\begin{bmatrix}
\mathcal{A}^∗(y) + S - C  \\\
\mathcal{A}(X) - b \\\
XS
\end{bmatrix} =
\begin{bmatrix}
0 \\\
0 \\\
τI
\end{bmatrix}
, X, S \succ 0\]
</blockquote>

<p>Newton step:</p>
<blockquote>
\[\begin{bmatrix}
0 &amp; \mathcal{A}^∗ &amp; I \\\
\mathcal{A} &amp; 0 &amp; 0 \\\
S &amp; 0 &amp; X 
\end{bmatrix}
\begin{bmatrix}
\Delta X \\\
\Delta y \\\
\Delta S
\end{bmatrix}= −
\begin{bmatrix}
\mathcal{A}^∗(y) + s−c \\\
\mathcal{A}(x) − b \\\
XS − \tau I 
\end{bmatrix}\]
</blockquote>


        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
