<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      Proximal Gradient Descent and Acceleration &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/convex-optimization/public/css/poole.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/syntax.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/lanyon.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/logo.png">
  <link rel="shortcut icon" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://simonseo.github.io/convex-optimization/convex-optimization/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/convex-optimization/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>09. Proximal Gradient Descent and Acceleration</h1>






<!-- Get first post and show it -->

<h1 id="proximal-gradient-descent-and-acceleration">Proximal Gradient Descent and Acceleration</h1>

<p>Non-differentiable한 함수에 대해 최적화를 하기 위해 subgradient method를 사용하게 되면 성능이 다소 느리다는 단점이 있다. 이 성능 문제를 해결하기 위해 제시된 방법이 <strong>proximal gradient descent</strong>이다.</p>

<p><strong>Proximal gradient descent</strong>는 objective 함수를 미분 가능(differentiable)한 함수와 미분 불가능(non-differentiable)한 함수로 분리한다. 그리고, differentiable한 함수의 다음 위치를 gradient descent로 에측해서 그 위치와 가까우면서 non-differentiable한 함수가 동시에 작아지게 만들 수 있는 가장 좋은 위치로 조정한다.</p>

<p>이 방법은 분석적으로 최적해를 구할 수 있기 때문에 gradient descent와 같은 수렴 속도를 갖게 되며 non-differentiable한 함수가 “simple” 할수록 계산 비용도 gradient descent와 유사해진다.</p>

<p>이 장에서는 <strong>proximal gradient descent</strong>에 전반적인 내용을 살펴보고 추가적인 성능 향상을 위한 여러 <strong>aceleration</strong> 방법들에 대해 살펴보도록 하겠다.</p>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">09-01 Proximal gradient descent</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_2">09-02 Convergence analysis</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_3">09-03 Example: matrix completion</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_4">09-04 Special cases</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_5">09-05 Acceleration</a>
    </li>
  
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_6"> 09-05-01 Accelerated proximal gradient method</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_7"> 09-05-02 Convergence analysis</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_8"> 09-05-03 Example : FISTA</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_9"> 09-05-04 Is acceleration always useful?</a>
    </li>
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>09-01 Proximal gradient descent</h1>
            <h1 id="proximal-gradient-descent">Proximal gradient descent</h1>

<p><strong>Proximal gradient descent</strong>는 objective 함수를 differentiable한 함수와 non-differentiable한 함수로 분리해서 최적해를 찾는 방법이다. 이 절에서는 proximal gradient descent에서 함수를 정의하는 방식과 최적해를 구하는 방식을 살펴보도록 하겠다.</p>

<h2 id="decomposable-functions">Decomposable functions</h2>
<p>Objective 함수 \(f\)를 두 개의 함수 \(g\)와 \(h\)로 분리할 수 있다고 가정하자.</p>

<blockquote>
\[f(x) = g(x) + h(x)\]
</blockquote>

<p>이떄, 두 함수 함수 \(g\)와 \(h\)는 다음과 같은 성질을 갖는다.</p>

<ul>
  <li>\(g\)는 convex이고 differentiable하다. (<strong>dom</strong>\((g) = \mathbb{R}^n\))</li>
  <li>\(h\)는 convex이고 non-differentiable하다.</li>
</ul>

<p>만일 \(f\)가 differentiable하다면 gradient descent로 다음 위치를 찾을 수 있을 것이다.</p>

<blockquote>
\[x^+ = x - t \cdot \nabla f(x)\]
</blockquote>

<p><strong>[참고]</strong> Gradient descent에서는 함수 \(f\)를 \(x\) 근처에서 Tayler 2차식으로 근사하고 2차 항의 hessian \(\nabla^2 f(x)\)를 \(\frac{1}{2t} I\)로 대체해서 정의한다. 그리고, 이 근사식의 최소 위치를 다음 위치로 선정한다. (자세한 내용은 6장 Gradient descent 참조)</p>

<blockquote>
  <p>\begin{align}
x^+ = \underset{z}{\text{argmin}}  \underbrace{ f(x) + \nabla f(x)^T (z - x) + \frac{1}{2t} \parallel z - x \parallel_2 ^2}_{\tilde{f}_t(z)}
\end{align}</p>
</blockquote>

<p>하지만, 함수 \(f\)가 differentiable하지 않다면 gradient descent를 사용할 수 없다. 그런데, 함수 \(f\)가 \(f = g + h\)로 구성된다면 differentiable한 함수 \(g\)는 이차식으로 근사할 수 있지 않을까?</p>

<p>이런 아이디어에서 나온 방법이 <strong>Proximal gradient descent</strong>이다. 이 방법에서는 \(g\)의 gradient descent로 예측된 위치와 가까우면서 non-differentiable한 함수 \(h\)를 동시에 작아지게 만들 수 있는 가장 좋은 위치로 조정하는 방식이다. 이런 과정은 다음 식과 같이 표현될 수 있다.</p>

<blockquote>

\[\begin{align}
x^+ &amp; = \underset{z}{\text{argmin}}   \tilde{g}_t(z) + h(z) \\
&amp; = \underset{z}{\text{argmin}} \ g(x) + \nabla g(x)^T (z - x) + \frac{1}{2t} \parallel z - x \parallel_2 ^2 + h(z) \\
&amp; = \underset{z}{\text{argmin}}  \nabla g(x)^T (z - x) + \frac{1}{2t} \parallel z - x \parallel_2 ^2 + \frac{t}{2} \parallel \nabla g(x) \parallel_2 ^2  + h(z) \\
&amp; = \underset{z}{\text{argmin}}  \frac{1}{2t} \left ( 2t \nabla g(x)^T (z - x) + \parallel z - x \parallel_2 ^2 + t^2 \parallel \nabla g(x) \parallel_2 ^2 \right )  + h(z) \\
&amp; = \underset{z}{\text{argmin}}  \frac{1}{2t} \left ( \parallel z - x \parallel_2 ^2 + 2t \nabla g(x)^T (z - x) + t^2 \parallel \nabla g(x) \parallel_2 ^2 \right ) + h(z) \\
&amp; = \underset{z}{\text{argmin}}   \frac{1}{2t} \parallel z - (x - t \nabla g(x) )\parallel_2 ^2 + h(z) \\
\end{align}\]
</blockquote>

<p>2줄에서 3줄로 갈 때 z에 대한 상수항으로서 \(g(x)\)는 삭제되고 \(\frac{t}{2} \parallel \nabla g(x)^T \parallel_2 ^2\) 항이 추가되었다. 최종 식에서 첫번째 항 \(\frac{1}{2t} \parallel z - (x - t \nabla g(x) )\parallel_2 ^2\)은 \(g\)의 gradient update 위치에 가까워지게 만드는 항이고 두번째 항인 \(h(z)\)는 \(h\)를 작아지게 만드는 항이다.</p>

<h2 id="proximal-gradient-descent-1">Proximal gradient descent</h2>
<p>Proximal gradient descent는 시작점 \(x^{(0)}\)에서 시작해서 다음 과정을 반복한다.</p>

<blockquote>
  <p>\(x^{(k)} = \text{prox}_{t_k}(x^{(k-1)} - t_k \nabla g(x^{(k-1)}) )\), \(k=1,2,3,...\)</p>
</blockquote>

<p>여기서 \(\text{prox}_{t}\)는 proximal mapping으로 다음과 같이 정의된다.</p>

<blockquote>
  <p>\begin{align}
\text{prox}_{t}(x) = \underset{z}{\arg \min}  \frac{1}{2t} \parallel x - z \parallel_2^2 + h(z)
\end{align}</p>
</blockquote>

<p>이 식을 그동안 봐왔던  update 형태로 변경해 보면 다음과 같다. 여기서 \(G_{t}\)는 \(f\)의 generalized gradient이다.</p>

<blockquote>
  <p>\begin{align}
x^{(k)} = x^{(k-1)} - t_k \cdot G_{t_k}(x^{(k-1)}), \space \space \text{where} \space G_{t}(x) = \frac{x-\text{prox}_{t} (x - t \nabla g(x))}{t} <br />
\end{align}</p>
</blockquote>

<h2 id="what-good-did-this-do">What good did this do?</h2>
<p>이렇게 하면 무엇이 좋아지는가? 단지 문제를 다른 형태의 minimization 문제로 바꾼 것이 불과하지 않은가?라고 의문을 가질 수 있다.</p>

<p>핵심 포인트는 대부분의 주요 \(h\) 함수에 대해 \(\text{prox}_{t}(\cdot)\)가 분석적으로 계산될 수 있다는 것이다. 즉, 다음과 같이 계산된다.</p>

<ul>
  <li>맵핑 함수 \(\text{prox}_{t}(\cdot)\)는 \(g\)가 아닌 \(h\)에만 의존한다.</li>
  <li>함수 \(g\)는 매우 복잡한 함수일 수 있는데 여기서는 gradient \(\nabla g\)만 계산하면 된다.</li>
</ul>

<p>수렴 분석은 알고리즘의 반복 횟수에 대해 이뤄지게 될 것이다. 각 반복에서 \(\text{prox}_{t}(\cdot)\)를 계산하며 \(h\)에 따라 계산 비용이 작거나 커질 수 있다는 점을 유의해야 한다.</p>

<h2 id="example-ista">Example: ISTA</h2>
<p>Proximal gradient descent의 예제를 살펴보자. 이전 장에서 \(y \in \mathbb{R}^n\), \(X \in \mathbb{R}^{n \times p}\)일 때, lasso criterion은 다음과 같이 정의되었다.</p>

<blockquote>
  <p>\begin{align}
f(\beta) = \frac{1}{2} \parallel y - X\beta \parallel_2^2 + \lambda \parallel \beta \parallel_1 <br />
\end{align}</p>
</blockquote>

<p>여기서 \(g(\beta) = \frac{1}{2} \parallel y - X\beta \parallel_2^2\)이고 \(h(\beta) = \lambda \parallel \beta \parallel_1\)이다. 이때, proximal mapping은 다음과 같이 정의된다.</p>

<blockquote>

\[\begin{align}
\text{prox}_{t}(\beta) &amp; = \underset{z}{\text{argmin}}  \frac{1}{2t} \parallel \beta - z \parallel_2^2 + \lambda \parallel z \parallel_1 \\
&amp; = S_{\lambda t}(\beta) \\
\end{align}\]
</blockquote>

<p>\(S_{\lambda t}(\beta)\)는 soft-thresholding operator로 다음과 같이 정의된다. (자세한 내용은 7장 Subgradient 참조)</p>

<blockquote>
\[\begin{align}
[S_{\lambda t}(\beta)]_i =  
\begin{cases}
\beta_i - \lambda &amp; \mbox{if } \beta_i \gt \lambda \\
0 &amp; \mbox{if } \lambda \le \beta_i \le \lambda, i = 1, ..., n \\
\beta_i + \lambda &amp; \mbox{if } \beta_i \lt -\lambda \\
\end{cases}
\\
\end{align}\]
</blockquote>

<p>\(g\)의 gradient가 \(\nabla g(\beta) = -X^{T} (y - X \beta)\)이므로 proximal gradient update는 다음과 같이 정의된다.</p>

<blockquote>
\[\beta^+ = S_{\lambda t}(\beta + tX^{T} (y - X \beta) )\]
</blockquote>

<p>이 알고리즘은 <strong>iterative soft-thresholding algorithm (ISTA)</strong> 라고 하는 매우 간단한 알고리즘이다. (Beck and Teboulle (2008), “A fast iterative shrinkage-thresholding algorithm for linear inverse problems”)</p>

<p>다음 그림을 보면 subgradient method와 proximal gradient descent의 확연한 성능 차이를 확인할 수 있다. 반복 횟수 측면에서 proximal gradient descent의 성능은 gradient cescent의 성능과 동일하다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter09/09.01_01_ISTA.PNG" width="80%" height="80%" />
  <figcaption style="text-align: center;">[Fig 1] Example of proximal gradient (ISTA) vs. subgradient method coonvergence rate [3]</figcaption>
</p>
</figure>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>09-02 Convergence analysis</h1>
            <h1 id="convergence-analysis">Convergence analysis</h1>
<p>이 절에서는 proximal gradient descent의 수렴을 분석한다.</p>

<h2 id="convergence-analysis-1">Convergence Analysis</h2>
<p>Objective 함수 \(f(x) = g(x) + h(x)\)에 대해 다음 사항을 가정한다.</p>

<ul>
  <li>\(g\)는 convex, differentiable하며 <strong>dom</strong>\((g) = \mathbb{R}^n\), \(\nabla g\)는 Lipschitz continuous하다 (\(L \gt 0\)).</li>
  <li>\(h\)는 convex이고 \(\text{prox}_{t}(x) = \underset{z} {\text{argmin}} \{ \parallel x - z \parallel_2^2/ (2t) + h(z) \}\)가 계산되어야 한다.</li>
</ul>

<h4 id="convergence-theorem">Convergence Theorem</h4>
<blockquote>
  <p><strong>Proximal gradient descent</strong>는 fixed step size \(t \le 1/L\)에 대해 다음 식을 만족한다. 
\begin{align}
f(x^{(k)}) - f^{*} \le  \frac{ \lVert x^{(0)} - x^{*} \rVert^2_2 }{2tk}
\end{align}</p>
</blockquote>

<p>Proximal gradient descent는 \(O(1/k)\) 또는 \(O(1/\epsilon)\)의 수렴 속도를 갖는데 이는 gradient descent와 동일한 성능이다. 단, 이 성능은 반복 횟수 기준이지 연산 횟수 기준이 아니라는 점을 유념하자. (연산 횟수는 함수 \(h\)에 따라 적을수도 많을 수도 있다.)</p>

<h2 id="backtracking-line-search">Backtracking line search</h2>
<p>Proximal gradient descent의 backtracking line search 방법은 gradient descent와 비슷하지만 함수 \(f\)가 아닌 smooth 파트인 \(g\)에 대해서만 작동한다.</p>

<p>먼저 파라미터를 \(0 \lt \beta \lt 1\)로 설정하고 \(t=1\)로 시작한다. 각 반복에서 다음 조건을 만족하는 동안 \(t\)를 \(t = \beta t\)로 줄이고, 다음 조건을 만족하지 않으면  proximal gradient descent를 update한다.</p>
<blockquote>
  <p>\begin{align}
g(x - tG_t(x)) \gt g(x) - t \nabla g(x)^T G_t(x) + \frac{t}{2} \parallel G_t(x) \parallel_2 ^2
\end{align}</p>
</blockquote>

<p>이 backtracking 조건은 다음 step 위치인 \(x - tG_t(x)\)에서의 함수 \(g\)의 값이 함수 \(g\)의 Tayor 2차 근사 함수의 값보다 클 때를 의미한다.</p>

<p>이 식에서 \(G_t(x) = \nabla g(x)\)이라면 \(g(x - t \nabla g(x)) \gt g(x) - \alpha t \lVert \nabla g(x) \rVert_2^2\)가 되므로 gradient descent의 backtracking 조건과 동일해진다는 것을 알 수 있다.</p>

<p><strong>참고 :</strong> Gradient descent의 backtracking에 대한 자세한 내용은 <a href="/convex-optimization/contents/chapter06/2021/03/20/06_02_02_backtracking_line_search/">06-02-02 backtracking line search</a> 참조</p>

<h4 id="backtracking-line-search-알고리즘">Backtracking line search 알고리즘</h4>
<p>이 내용을 알고리즘으로 정리하면 다음과 같다. (단, \(\nabla x = - t G_t(x)\))</p>

<ol>
  <li>파라미터를 초기화한다. (\(0 \lt \beta \lt 1\), \(0 \lt \alpha \le 1/2\))</li>
  <li>각 반복에서 \(t = t_{\text{init}}\)로 초기화 한다. (\(t_{\text{init}} = 1\))</li>
  <li>조건 \(g(x - tG_t(x)) \gt g(x) - t \nabla g(x)^T G_t(x) + \frac{t}{2} \parallel G_t(x) \parallel_2 ^2\)을 만족하면 \(t = \beta t\)로 줄인다. 이 조건이 만족되는 동안 3을 반복한다.</li>
  <li>Gradient descent update \(x^+ = x - t G_t(x) = \text{prox}_t(x - t \nabla g(x))\)를 실행한다.</li>
  <li>종료 조건을 만족하지 않으면 2로 간다.</li>
</ol>

<p>Proximal gradient descent에서 backtracking을 할 때 \(G_t(x)\)이 반복적으로 계산되기 때문에 실제  \(G_t(x)\) 안에 포함된 proximal mapping이 반복적으로 계산된다. Proxmal mapping은 계산 비용이 매우 높기 때문에 전체적인 backtracking 계산 비용은 높다고 할 수 있다.</p>

<h4 id="convergence-theorem-1">Convergence Theorem</h4>
<p>앞의 가정과 동일한 가정 하에 backtracking line search 방식도 같은 성능을 구할 수 있다.</p>

<blockquote>
  <p><strong>Proximal gradient descent</strong>는 backtracking line search에 대해 다음 식을 만족한다. Step size는 \(t_{\text{min}} = \text{min} \{1,\beta /L \}\)이다.</p>
</blockquote>

<blockquote>
\[f(x^{(k)})−f^{\star} ≤ \frac{\lVert x^{(0)} − x^{\star} \rVert_{2}^{2}}{2 t_{min}k}, \space t_{\text{min}} = \text{min} \{ 1, \beta / L \} \\\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>09-03 Example: matrix completion</h1>
            <h1 id="example-matrix-completion">Example: matrix completion</h1>

<p>여러 응용에서 측정된 데이터를 행렬로 표현하게 되는데, 이때 행렬의 대부분의 항목들에는 값이 없고 소수의 항목에만 관측 데이터가 있는 희소 행렬인 경우가 있다. 이와 같은 행렬에서 값이 없는 항목(missing entry)들의 값을 채우는 문제를 <strong>Matrix completion</strong> 문제라고 한다.</p>

<p>예를 들어, 추천 시스템에서 아직 구매를 하지 않은 상품이나 서비스를 고객에게 추천할 때 이런 문제가 발생할 수 있다.</p>

<h2 id="maxtrix-completion-problem">Maxtrix Completion Problem</h2>
<p><strong>Matrix completion</strong> 문제는 다음과 같이 정의할 수 있다.</p>

<p>행렬 \(Y ∈ \mathbb{R}^{m×n}\)는 관측 데이터를 갖고 있는 행렬이며, 관측 데이터가 있는 항목을 \(Y_{ij}, (i,j) ∈ \Omega\)라고 하자. 행렬 \(B\)는 관측 값이 없는 항목들을 추정하기 위한 추정 행렬이다.</p>

<blockquote>
\[\min_B \frac{1}{2} \sum_{(i,j)∈\Omega} (Y_{ij} −B_{ij})^2 + λ\lVert B \rVert_{tr}\]
</blockquote>

<p>Objective 함수의 첫번쨰 항은 행렬 \(B\)와 관측 데이터와의 오차를 최소화하며, 두번쨰 항은 행렬 \(B\)를 저차원(low rank) 행렬로 만들어 준다. (여기에는 행렬 B는 저차원의 manifold에 있다고 가정한다.) 따라서, 행렬 \(B\)는 관측 데이터는 그대로 유지하면서 관측 데이터가 형성하고 있는 가장 낮은 차원의 값으로 나머지 부분을 채우는 행렬이 된다.</p>

<h4 id="참고-trace-norm">[참고] Trace Norm</h4>
<p>행렬의 trace norm은 행렬의 sigular value들의 합이다.</p>

<blockquote>
\[\lVert B \rVert_{tr} = \text{trace}(\sqrt{B^* B}) = \sum_{i=1}^r \sigma_i(B), \quad r = rank(B)\]
</blockquote>

<p>\(B^* B\)는 positive semi-definite이고 \(\sigma_1(X) ≥ ... ≥ \sigma_r(X) ≥ 0\)는 singular value이다.</p>

<h4 id="참고-l1-norm-regularizer-vs-trace-norm-regularizer">[참고] <strong>L1</strong> Norm Regularizer vs. Trace Norm Regularizer</h4>
<p>이 문제는 matrix soft-thresholding로 원래 soft-thresholding에서의 벡터가 행렬로 대체되었다고 보면 된다. Regularizer 항을 보면 벡터에 대한 <strong>L1</strong> norm regularizer ( \(\lVert \cdot \rVert_{1}\))가 행렬에 대한 trace norm regularizer (\(\lVert \cdot \rVert_{tr}\))로 대체되었는데 실제 두 regularizer의 역할은 같다고 볼 수 있다.</p>

<p><strong>L1</strong> norm regularizer가 벡터를 sparse하게 만들어 주는데,  trace norm regularizer도 행렬의 sigular value vector를 sprase하게 만들어 주기 때문이다. 즉, 행렬이 diagonal일 때 diagonal을 singular value vector로 볼 수 있으며 trace norm regularizer는 singular value의 합을 최소화 하기 때문에 singular value vector를 sparse하게 해준다.</p>

<p>이 문제에서 trace norm \(\lVert B \rVert_{tr}\)는 \(\text{Rank}(B)\)의 approximation으로 사용되었다고 볼 수 있다.</p>

<h2 id="proximal-gradient-descent">Proximal gradient descent</h2>
<p>이 문제를 projection operator를 이용해서 정의하면 proximal gradient descent를 Nice하게 적용할 수 있다.</p>
<h4 id="projection-operator">Projection Operator</h4>
<p>관측값에 대해 projection operator \(P_\Omega\)를 다음과 같이 정의해보자.</p>

<blockquote>
\[[ P_\Omega(B) ] _{ij} =
\begin{cases}
B_{ij}, &amp; (i,j) ∈ \Omega \\\
0, &amp; (i,j) \notin \Omega
\end{cases}\]
</blockquote>

<h4 id="objective-function">Objective Function</h4>
<p>Projection operator를 이용해서 objective 함수를 정의하면 다음과 같다.</p>

<blockquote>
\[f(B) = \underbrace{ \frac{1}{2} \lVert P_\Omega(Y) − P_\Omega(B) \rVert_F^2 }_{g(B)} + \underbrace{ \lambda \lVert B \rVert_{tr} }_{h(B)}\]
</blockquote>

<p>이제 함수 \(f(B)\)는 differentiable 파트인 \(g(B)\)와 non-differentiable 파트로 \(h(B)\)로 구성되었다.</p>

<h4 id="proximal-mapping">Proximal Mapping</h4>
<p>이제 proximal gradient descent를 적용하기 위해 함수 \(g(B)\)의 gradient를 구하고 proximal mapping를  정의해보자.</p>

<ul>
  <li>함수 \(g(B)\)의 gradient : \(\nabla g(B) = −(P_\Omega(Y )−P_\Omega(B))\)</li>
  <li>Proximal mapping :</li>
</ul>

<blockquote>
\[\begin{align}
\text{prox}_t (B) = \underset{Z}{\text{argmin}} \frac{1}{2t} \Vert B−Z \Vert_F^2 + \lambda \Vert Z \Vert_{tr}
\end{align}\]
</blockquote>

<h4 id="matrix-svd--soft-thresholding">Matrix SVD &amp; Soft-thresholding</h4>
<p>Proximal mapping은 \(\lambda\) 레벨에서의 matrix soft-thresholding로 \(\text{prox}_t(B) = S_{\lambda t}(B)\)이다.</p>

<p>일반적으로 행렬 \(B\)는 매우 크기 때문에 Singular Vector Decompoisition(SVD)를 해서 연산량을 최소화 해야만 한다. 따라서, \(B = U \Sigma V^T\)와 같이 SVD를 했다면 \(S_\lambda(B)\)는 다음과 같이 정의할 수 있다.</p>

<blockquote>
\[S_\lambda(B) = U \Sigma_\lambda V^T\]
</blockquote>

<p>이때, \(\Sigma_\lambda\)는 다음과 같은 대각 행렬이다.</p>

<blockquote>
\[(\Sigma_\lambda)_{ii} = \max \{ \Sigma_{ii} −\lambda,0 \}\]
</blockquote>

<h4 id="참고-sigma_lambda_ii--max--sigma_ii-lambda0---inducement">[참고] \((\Sigma_\lambda)_{ii} = \max \{ \Sigma_{ii} −\lambda,0 \}\)  inducement</h4>
<p>이 식이 어떻게 도출되었을까? \(\text{prox}_t(B) = Z\)라고 하면 \(Z\)는 다음과 같다.
(\(\text{prox}_t(B)\)의 우변을 Z에 대해 미분하면 다음 결과를 얻을 수 있다.)</p>

<blockquote>
\[0 ∈ Z − B + \lambda t \cdot \partial \lVert Z \rVert_{tr}\]
</blockquote>

<p>이 식을 정리해 보면 다음과 같다.</p>
<blockquote>
\[\begin{align}
Z &amp; = B - \lambda t \cdot \partial \lVert Z \rVert_{tr} \\
 &amp; = U \Sigma V^T - \lambda t \cdot (UV^T + W) \\
 &amp; = U (\Sigma - \lambda t) V^T - \lambda t  W \\
 &amp; = U (\Sigma - \lambda ) V^T  \\
\end{align}\]
</blockquote>

<p>최종 수식은 Lipschitz constant \(L=1\)일 경우 \(t=1\)이고 \(W\)가 0일 경우에 도출될 수 있다.</p>

<p>따라서, \(\text{prox}_t(B) = S_\lambda(B) = Z\)이므로 다음 식이 도출된다..</p>

<blockquote>
\[(\Sigma_\lambda)_{ii} = \max \{ \Sigma_{ii} −\lambda,0 \}\]
</blockquote>

<h4 id="참고-derivative-of-trace-norm">[참고] Derivative of Trace Norm</h4>
<p>만약 \(Z = U \Sigma V^T\)라면 trace norm이 미분은 다음과 같다.</p>
<blockquote>
\[\partial \lVert Z \rVert_{tr} = \{UV^T + W : \lVert W \rVert_{op} ≤ 1, U^TW = 0, WV = 0 \}\]
</blockquote>

<p>\(\lVert W \rVert_{op}\)는 operator norm으로 biggest singular value가 1보다 작다. 그리고, \(W\)는 \(U^T\)와 \(V\)와 orthogonal하다.</p>

<ul>
  <li>증명은 <a href="https://math.stackexchange.com/questions/701062/derivative-of-the-nuclear-norm-with-respect-to-its-argument">Derivative of the nuclear norm with respect to its argument</a> 참조</li>
</ul>

<h4 id="proximal-gradient-update">Proximal gradient update</h4>
<p>이제 proximal gradient 업데이트 식을 정의해 보자.</p>

<blockquote>
\[B^+ = S_{\lambda t} ( B + t( P_\Omega(Y) − P_\Omega(B) ) )\]
</blockquote>

<p>\(L = 1\)일 때 \(\nabla g(B)\)는 Lipschitz continuous이므로 ﬁxed step size \(t = 1\)로 선택할 수 있다.</p>

<p>따라서, 업데이트 식이 다음과 같이 간단해졌다.</p>
<blockquote>
\[B^+ = S_\lambda (P_\Omega(Y) + P_\Omega^\bot (B) )\]
</blockquote>

<p>\(P_\Omega^\bot\)는 미관측 값에 대한 사영(projection)이며 \(P_\Omega(B) + P_\Omega^\bot (B) = B\)를 만족한다.</p>

<p>이 식에서 \(P_\Omega(Y)\)는 observed 파트이고 \(P_\Omega^\bot (B)\)는 missing 파트이다. \(S_\lambda\) 함수는 입력 행렬을 SVD를 해서 \((\Sigma_\lambda)_{ii} = \max \{ \Sigma_{ii} −\lambda,0 \}\)만 계산하면 되므로 매우 간단하다.</p>

<h2 id="soft-impute-algorithm">Soft-Impute Algorithm</h2>
<p>이 알고리즘을 <strong>Soft-Impute</strong>이라고 하며 matrix completion에 간단하고 효과적으로 할 수 있다. 일반적으로 행렬이 큰 경우 SVD 계산 비용은 매우 높은데, 이 알고리즘에서는 \(P_\Omega(Y)\)가 sparse하고 \(P_\Omega^\bot (B)\)가 low rank이기 때문에 SVD를 효율적으로 할 수 있게 된다.</p>

<ul>
  <li>자세한 내용은 논문 참조 : Mazumder et al. (2011), “Spectral regularization algorithms for learning
large incomplete matrices”</li>
</ul>


        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>09-04 Special cases</h1>
            <h1 id="special-cases">Special cases</h1>

<p>Proximal gradient descent는 composite gradient descent 또는 generalized gradient descent라고도 한다.</p>

<p>그렇다면 왜 <strong>“generalized”</strong>라고 할까? 그 이유는 Proximal gradient descent가 다음과 같은 특별 케이스들을 모두 포함하기 때문이다.</p>

<ul>
  <li>\(h = 0 \to\) gradient descent</li>
  <li>\(h = I_C \to\) projected gradient descent</li>
  <li>\(g = 0 \to\) proximal minimization algorithm</li>
</ul>

<p>따라서 이 알고리즘들은 모두 \(O(1/\epsilon)\)의 수렴 속도를 갖는다.</p>

<h2 id="projected-gradient-descent">Projected gradient descent</h2>
<p>\(I_C(x)\)가 closed convex set \(C ∈ \mathbb{R}^n\)의 Indicator 함수일 때  \(g(x)\)를 집합 \(C\)에 대해 최소화 하는 문제는 다음과 같이 재정의하여 표현할 수 있다. (참고로 \(C\)가 closed set이어야 하는 이유는 closed set이어야 projection이 잘 정의될 수 있기 때문이다.)</p>

<blockquote>
\[\min_{x \in C} g(x) \iff \min_x g(x) + I_C(x)\]
</blockquote>

<blockquote>
\[I_C(x) = 
\begin{cases}
0, &amp; x \in C \\
\infty, &amp; x \notin C
\end{cases}\]
</blockquote>

<p>이때 proximal mapping은 다음과 같이 정의된다.</p>

<blockquote>
\[\begin{align}
\text{prox}_t(x)  
&amp;= \underset{z}{\text{argmin}} \frac{1}{2t} \lVert x−z \rVert_2^2 + I_C(z) \\
&amp; = \underset{z \in C}{\text{argmin}} \lVert x−z \rVert_2^2
\end{align}\]
</blockquote>

<p>결과적으로 proximal mapping \(\text{prox}_t(x) = P_C(x)\)는 \(C\)에 대한 사영 오퍼레이터 (projection operator)라고 할 수 있다.</p>

<p>Proximal gradient 업데이트 단계는: \(x^+ = P_C (x−t \nabla g(x) )\)와 같이 정의된다. 풀어서 설명하면 gradient descent로 업데이트를 수행 한 후 \(C\)에 대해 사영(project)을 수행한다고 보면 된다.</p>

<p>아래 그림을 보면 분홍색 사각형이 feasible set  \(C\)이며 현재 위치는 사각형 아래의 두 점 중 윗쪽 점이다. Gradient descent로 한 스텝을 이동하게 되면 \(C\)를 벗어나게 되므로 다시 \(C\)에 대해 사영을 해서 feasible set 안쪽으로 들어오게 됨을 알 수 있다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter09/projected_gradient_descent.png" width="80%" height="80%" />
  <figcaption style="text-align: center;">[Fig1] Projected Gradient Descent [3]]</figcaption>
</p>
</figure>

<h2 id="proximal-minimization-algorithm">Proximal minimization algorithm</h2>

<p>다음가 같이 Convex \(h\)를 최소화하는 문제를 고려해보자. 이때, \(h\)는 미분가능할 필요는 없으며  \(g(x) = 0\)이다.</p>
<blockquote>

  <p>\begin{align}
\min_x h(x)
\end{align}</p>
</blockquote>

<p>Proximal mapping은 다음과 같이 정의된다.</p>
<blockquote>
\[\begin{align}
x^{(k)} &amp;= \text{prox}_{t_k} \big(x^{(k-1)} - t_k \nabla g ( x^{(k-1)} ) \big) , \qquad k = 1, 2, 3, ... \\
&amp;= \text{prox}_{t_k} \big(x^{(k-1)} \big) ,  \qquad \qquad \qquad \qquad \; k = 1, 2, 3, ... \\
\end{align}\]
</blockquote>

<p>따라서 proximal gradient 업데이트 단계는 다음과 같다:</p>
<blockquote>
\[x^+ = \underset{z}{\text{argmin}} \frac{1}{2t} \lVert x−z \rVert_2^2 + h(z)\]
</blockquote>

<p>이와 같이 \(g\)는 \(0\)이고 \(h\)로만 정의되는 proximal gradient 방법을 <strong>proximal minimization algorithm(PMA)</strong> 라고 한다.
이 방법은 subgradient보다는 빠르지만 proximal mapping이 closed form이 아니면 구현이 불가능하다.</p>

<h2 id="what-happens-if-we-cant-evaluate-prox">What happens if we can’t evaluate prox?</h2>
<p>이론적으로는 \(f = g + h\)에 대해 proximal gradient을 적용할 때는 prox 함수가 정확히 계산될 수 있다고 가정한다. 즉, proximal mapping을 통해 최소값을 정확히 구할 수 있다고 가정한다.</p>
<blockquote>
\[\text{prox}_t(x )= \underset{z}{\text{argmin}} \frac{1}{2t} \lVert x−z \rVert_2^2 + h(z)\]
</blockquote>

<p>일반적으로 최소값을 근사할 경우 어떤 일이 일어나는지는 정확히 알 수 없다.
하지만 prox operator를 근사할 때 에러를 정확히 제어 할 수 있다면 원래의 수렴 속도를 가질 수 있음이 밝혀져 있다. (Schmidt et al. (2011), Convergence rates of inexact proximal-gradient methods for convex optimization”)</p>

<p>실제 prox가 근사적으로 계산될 수 있다면 높은 정확도로 수행될 것이다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_5"></a>09-05 Acceleration</h1>
            <h1 id="acceleration">Acceleration</h1>

<p>이 장에서는 <strong>proximal gradient descent</strong>를 가속화하는 방법을 살펴볼 것이다. 그리고 수렴 분석과 FISTA 예시를 살펴보며 가속화가 항상 유용한지를 분석해 본다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_6"></a>09-05-01 Accelerated proximal gradient method</h1>
            <h1 id="accelerated-proximal-gradient-method">Accelerated proximal gradient method</h1>

<p>Proximal gradient descent를 가속화(accerleration)하게 되면 최적의 수렴 속도인  \(O(1/\sqrt{\epsilon})\)를 달성할 수 있다. Nesterov는 다음과 같은 네 가지 방식을 제안했으며 이 중 세 가지가 가속화 방법이다.</p>

<ul>
  <li>1983 : Smooth function에 대한 원래 가속화 방법</li>
  <li>1988 : Smooth function에 대한 다른 가속화 방법</li>
  <li>2005 : 원래 가속화 방법과 함께 nonsmooth function을 smoothing하는 기법</li>
  <li>2007 : Composite function에 대한 가속화 방법</li>
</ul>

<p>이제 Nesterov(1983)의 composite function에 대한 방법을 확장한 Beck과 Teboulle(2008)의 방법을 살펴보자.</p>

<h2 id="accelerated-proximal-gradient-method-1">Accelerated proximal gradient method</h2>
<p>이전과 동일하게 \(g\)가 convex이고 미분가능하며 \(h\)는 convex일 때 다음과 같이 문제가 정의된다고 하자.</p>
<blockquote>

  <p>\begin{align}
\min_x g(x) + h(x)
\end{align}</p>
</blockquote>

<p><strong>Accelerated proximal gradient method</strong>는 \(x\)에서 다음 위치로 갈 때 방향이 급격히 바뀌지 않도록 \(x\)가 진행하던 방향을 고려하는 방식이다. 즉, 진행 방향에 모멘텀(momentum)을 주어 지금까지 진행하던 방향으로 계속 진행하려는 관성을 갖게 만든다.</p>

<p>알고리즘의 초기값은 \(x^{(0)} = x^{(−1)} \in \mathbb{R}^n\)로 둔다. 그리고, 모멘텀을 고려한 위치 \(v\)를 계산한 후 proximal gradient를 실행한다.</p>

<blockquote>
\[\begin{align}
v &amp; = x^{(k−1)} + \frac{k−2}{k + 1}  (x^{(k−1)} −x^{(k−2)}) \\
x^{(k)} &amp; = \text{prox}_{t_k} (v − t_k \nabla g(v)), k = 1,2,3,...  \\
\end{align}\]
</blockquote>

<ul>
  <li>첫번째 스텝 \(k = 1\)에서는 \(x^{(k−1)} −x^{(k−2)}\)가 0이기 때문에 proximal gradient update와 동일하다.</li>
  <li>그 다음 단계에서는 \(v\)는 이전 진행 방향인 \(x^{(k−1)} −x^{(k−2)}\)로 모멘텀을 갖는다. 이때, \(k\)가 작을수록 모멘텀 가중치는 작으며 \(k\)가 클수록 모멘텀 가중치는 커져서 1로 수렴한다.</li>
  <li>\(h = 0\)인 경우 <strong>accelerated gradient method</strong>와 같다.</li>
</ul>

<p>아래 그림에는 \(k\)의 변화에 따른 모멘텀 가중치의 변화를 보여주고 있다.</p>

<p>이 그림을 보면 k = 0일 때 값이 음수인데 이때 모멘텀 항이 0이 되기 때문에 문제가 되지는 않는다. k값이 증가할수록 가중치가 1에 가까워 지므로 값이 갱신될 때 좀 더 멀리 나아가면서 빠르게 수렴할 수 있도록 도와준다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter09/momentum_weight.png" width="80%" height="80%" />
  <figcaption style="text-align: center;">[Fig1] Momentum weights [3]</figcaption>
</p>
</figure>

<h2 id="lasso-example">Lasso example</h2>
<p>이전에 보았던 Losso 예시에 accelerated proximal gradient를 적용해 보면 다음 그림과 같은 결과를 얻을 수 있다. Accelerated proximal gradient subgradient나 proximal gradient에 비해 매우 빠른 성능을 갖고 있음을 알 수 있다.</p>

<p>그래프의 중간에 튀는 부분이 보이는데 이를 “Nesterov ripples”이라고 부른다. 따라서, accelerated proximal gradient는 monotonic decreasing하지 않기 때문에 descent 방법이 아니다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter09/accelerated_proximal_gradient.png" width="80%" height="80%" />
  <figcaption style="text-align: center;">[Fig2] Accelerated Proximal Gradient [3]</figcaption>
</p>
</figure>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_7"></a>09-05-02 Convergence analysis</h1>
            <h1 id="convergence-analysis">Convergence analysis</h1>

<p>이 절에서는 accelerated proximal gradient method의 수렴을 분석한다.</p>

<h2 id="convergence-analysis-1">Convergence analysis</h2>
<p>Objective 함수 \(f(x) = g(x) + h(x)\)에 대해 다음 사항을 가정한다.</p>

<ul>
  <li>\(g\)는 convex, differentiable하며 <strong>dom</strong>\((g) = \mathbb{R}^n\), \(\nabla g\)는 Lipschitz continuous하다 (\(L \gt 0\)).</li>
  <li>\(h\)는 convex이고 \(\text{prox}_{t}(x) = \underset{z}{\text{argmin}} \left \{ \parallel x - z \parallel_2^2/ (2t) + h(z) \right \}\)가 계산되어야 한다.</li>
</ul>

<h4 id="convergence-theorem">Convergence Theorem</h4>
<blockquote>
  <p><strong>Accelerated proximal gradient method</strong>는 fixed step size \(t \le 1/L\)에 대해 다음 식을 만족한다. 
\begin{align}
f(x^{(k)}) − f^{\star} ≤ \frac{2 \lVert x(0) −x^{\star} \rVert_2^2 }{ t(k + 1)^2}
\end{align}</p>
</blockquote>

<p>Frst-order method의 optimal rate인 \(O (1 / k^2)\)의 성능을 가지며 이는 \(O(1/ \sqrt {\epsilon})\)이다.  Subgradient method의 성능은 \(O(1/\varepsilon^{2})\), proximal gradiant의 성능은 \(O(1/\varepsilon)\), accelerated proximal gradient의 성능은 \(O(1/\sqrt{ \varepsilon})\)이다.</p>

<h2 id="backtracking-line-search">Backtracking line search</h2>
<p>가속(acceleration)을 사용할 때 Backtracking line search를 하는 복잡한 방법들도 있지만, 여기에서 좀 더 간단한 방법을 알아보자.</p>

<p>먼저 \(β &lt;1, t_0 = 1\)로 고정한다. 그리고, \(k\)번째 반복에서 \(t = t_{k-1}\)로 시작을 한다. (Gradient descent에서는 \(t=1\)로 시작을 하는데 accelerated proximal gradient method에서는 이전 단계의 step size부터 시작한다는 점을 유의하라.)</p>

<blockquote>
\[g(x^+) &gt; g(v) +\nabla g(v)^T(x^+ − v) + \frac{1}{2t} \lVert x+ −v \rVert_2^2\]
</blockquote>

<p>그리고 위 수식을 만족하는 동안 \(t = βt\)를 줄이고, \(x^+ = \text{prox}_t (v − t \nabla g(v))\)를 갱신한다. 그 외에는 \(x^+\)를 유지한다.</p>

<h4 id="convergence-theorem-1">Convergence Theorem</h4>
<p>동일한 가정 하에서 같은 성능을 얻을 수 있다.</p>

<blockquote>
  <p><strong>Accelerated proximal gradient method</strong>는 backtracking line search에 대해 다음 식을 만족한다. Step size는 \(t_{\text{min}} = \text{min} \{1,\beta/L \}\)이다.</p>
</blockquote>

<blockquote>
\[\begin{align}
f(x^{(k)})−f^{\star} ≤ \frac{2 \lVert x(0) −x^{\star} \rVert_2^2 }{ t_{min} (k + 1)^2}, \space t_{\text{min}} = \text{min} \{1,β/L \} \\
\end{align}\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_8"></a>09-05-03 Example : FISTA</h1>
            <h1 id="example--fista">Example : FISTA</h1>

<p>이 절에서는 Accelerated proximal gradient method의 예제로서 <strong>FISTA</strong>를 살펴볼 것이다.  <strong>FISTA</strong>는 <strong>ISTA</strong>(Iterative Soft-thresholding Algorithm)의 accerated version이라고 할 수 있다.</p>

<h2 id="fista-fast-iterative-soft-thresholding-algorithm">FISTA (Fast Iterative Soft-thresholding Algorithm)</h2>
<p>이전에 정의했던 Lasso problem을 기억해보자. \(y \in \mathbb{R}^n\), \(X \in \mathbb{R}^{n \times p}\)일 때, lasso criterion은 다음과 같이 정의되었다.</p>

<blockquote>
\[\min_\beta \frac{1}{2} \lVert y−X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_1\]
</blockquote>

<p>그리고, ISTA의 정의도 기억해 보자.  \(S_\lambda (·)\)는 vector soft-thresholding일 떄 Proximal gradient update가 다음과 같이 정의되었었다. (<a href="chapter09/2020/01/08/09_01_proximal_gradient_descent/">09-01 Proximal gradient descent</a> 참조)</p>
<blockquote>
\[\beta^{(k)} = S_{\lambda t_k} (\beta^{(k−1)} + t_kX^T(y − X\beta^{(k−1)})), k = 1,2,3,...\]
</blockquote>

<p>이 식에 acceleration을 적용하면 \(\beta\) 대신에 \(v\)를 계산해서 proximal gradient update를 한다.</p>

<blockquote>
\[v = \beta^{(k−1)} + \frac{k − 2}{k + 1} (\beta^{(k−1)} − \beta^{(k−2)}) \\
β(k) = S_{\lambda t_k} (v + t_kX^T(y−Xv) ), k = 1,2,3,...,\]
</blockquote>

<p>다음 그림은  lasso regression에 FISTA를 적용한 결과로 100개의 instance에 대해 실행하였다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter09/FISTA.png" width="80%" height="80%" />
  <figcaption style="text-align: center;">[Fig1] Lasso Regresssion : 100 instances (with n = 100, p = 500) [3]</figcaption>
</p>
</figure>

<p>다음 그림은  lasso logistic regression에 FISTA를 적용한 결과이다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter09/FISTA2.png" width="80%" height="80%" />
  <figcaption style="text-align: center;">[Fig2] Lasso Logistic Regression : 100 instances (n = 100, p = 500) [3]</figcaption>
</p>
</figure>

<p>100가지의 샘플을 토대로 Lasso regression, lasso logistic regression 에 대해 평균을 낸 결과, \(k\)값이 증가할수록 FISTA 기법이 훨씬 더 빠르게 수렴하는 것을 확인할 수 있다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_9"></a>09-05-04 Is acceleration always useful?</h1>
            <h1 id="is-acceleration-always-useful">Is acceleration always useful?</h1>

<p>Acceleration은 매우 효과적인 속도 향상의 도구일 수 있다. 하지만 항상 사용하는 것이 좋을까?</p>

<h2 id="성능-향상에-도움이-되지-않는-경우">성능 향상에 도움이 되지 않는 경우</h2>
<p>실제로 <strong>Warm start</strong>를 적용할 때 acceleration을 사용하면 속도가 향상되지 않을 수 있다.
예를 들어, 파라미터를 튜닝하면서 lasso problem을 푼다고 가정해 보자.</p>

<blockquote>
\[\lambda_1 &gt; \lambda_2 &gt; ... &gt; \lambda_r\]
</blockquote>

<ul>
  <li>\(\lambda_1\)에 대해 문제를 풀 때, \(x^{(0)} = 0\)로 초기화를 하고 해 \(\hat{x}(\lambda_1)\)를 저장해둔다.</li>
  <li>\(\lambda_j\)에 대해 문제를 풀 때, \(\lambda_{j−1}\)에 대해 저장된 해로 \(x^{(0)} = \hat{x} (\lambda_{j−1})\)와 같이 초기화를 한다.</li>
</ul>

<p>\(\lambda\)값이 충분히 튜닝된 경우 proximal gradient descent는 가속 없이도 좋은 성능으로 실행된다. 이 경우 가속을 하게 되면 성능을 저하시키지는 않지만 성능 향상에 크게 도움이 되지는 않는다.</p>

<h2 id="성능이-저하되는-경우">성능이 저하되는 경우</h2>
<p>어떤 경우에는 acceleration을 사용하면 성능을 저하될 수도 있다. 예를 들어 backtracking에 acceleration을 사용할 경우가 그런 경우이다. Matrix completion problem의 proximal gradient 업데이트에 backtracking을 할 경우를 고려해보자.</p>

<blockquote>
\[B^+ = S_\lambda ( B + t (P_\Omega(Y ) − P^\bot (B) ) )\]
</blockquote>

<p>여기서 \(S_\lambda\)는 matrix soft-thresholding operator로 SVD를 실행한다.</p>

<ul>
  <li>
    <p>\(t\)를 감소시키면서 backtracking을 할 때마다 generalized gradient \(G_t (x)\)를 계산해야 하는데 이는 내부적으로 행렬 SVD 계산을 포함하는 \(\text{prox}_t (x)\)의 계산을 의미한다. 이는 성능을 약화시키는 원인이 된다.</p>
  </li>
  <li>
    <p>또한, acceleration은 prox로 전달하는 argument를 변경한다. Matrix completion을 위해 \(x-t \nabla g (x)\) 대신 \(v-t \nabla g (v)\)를 사용하게 되는데, \(v\)를 계산하면서 행렬이 더 이상 low rank가 아니게 바뀔 수 있어서 SVD 연산이 더 느려질 수 있다.</p>
  </li>
</ul>

<blockquote>
  <p>\(B−\nabla g(B) = \underbrace{P_\Omega(Y)}_{\text{sparse}} + \underbrace{P_\Omega^\bot (B)}_{\text{low rank}}\) ⇒ <strong>fast SVD</strong></p>
</blockquote>

<blockquote>
  <p>\(V−\nabla g(V) = \underbrace{P_\Omega(Y)}_{\text{sparse}} + \underbrace{P_\Omega^\bot (V)}_{\text{not necessarily low rank}}\) ⇒ <strong>slow SVD</strong></p>
</blockquote>

<p>따라서, matrix completion에서는 acceleration과 backtracking line search 방식을 사용하게 되면 오히려 좋지 않다.</p>

        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/convex-optimization/public/js/script.js'></script>
  </body>
</html>
