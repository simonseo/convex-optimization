<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      09-03 Example: matrix completion &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/convex-optimization/public/css/poole.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/syntax.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/lanyon.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/logo.png">
  <link rel="shortcut icon" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://simonseo.github.io/convex-optimization/convex-optimization/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/convex-optimization/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">09-03 Example: matrix completion</h1>
  <h1 id="example-matrix-completion">Example: matrix completion</h1>

<p>여러 응용에서 측정된 데이터를 행렬로 표현하게 되는데, 이때 행렬의 대부분의 항목들에는 값이 없고 소수의 항목에만 관측 데이터가 있는 희소 행렬인 경우가 있다. 이와 같은 행렬에서 값이 없는 항목(missing entry)들의 값을 채우는 문제를 <strong>Matrix completion</strong> 문제라고 한다.</p>

<p>예를 들어, 추천 시스템에서 아직 구매를 하지 않은 상품이나 서비스를 고객에게 추천할 때 이런 문제가 발생할 수 있다.</p>

<h2 id="maxtrix-completion-problem">Maxtrix Completion Problem</h2>
<p><strong>Matrix completion</strong> 문제는 다음과 같이 정의할 수 있다.</p>

<p>행렬 \(Y ∈ \mathbb{R}^{m×n}\)는 관측 데이터를 갖고 있는 행렬이며, 관측 데이터가 있는 항목을 \(Y_{ij}, (i,j) ∈ \Omega\)라고 하자. 행렬 \(B\)는 관측 값이 없는 항목들을 추정하기 위한 추정 행렬이다.</p>

<blockquote>
\[\min_B \frac{1}{2} \sum_{(i,j)∈\Omega} (Y_{ij} −B_{ij})^2 + λ\lVert B \rVert_{tr}\]
</blockquote>

<p>Objective 함수의 첫번쨰 항은 행렬 \(B\)와 관측 데이터와의 오차를 최소화하며, 두번쨰 항은 행렬 \(B\)를 저차원(low rank) 행렬로 만들어 준다. (여기에는 행렬 B는 저차원의 manifold에 있다고 가정한다.) 따라서, 행렬 \(B\)는 관측 데이터는 그대로 유지하면서 관측 데이터가 형성하고 있는 가장 낮은 차원의 값으로 나머지 부분을 채우는 행렬이 된다.</p>

<h4 id="참고-trace-norm">[참고] Trace Norm</h4>
<p>행렬의 trace norm은 행렬의 sigular value들의 합이다.</p>

<blockquote>
\[\lVert B \rVert_{tr} = \text{trace}(\sqrt{B^* B}) = \sum_{i=1}^r \sigma_i(B), \quad r = rank(B)\]
</blockquote>

<p>\(B^* B\)는 positive semi-definite이고 \(\sigma_1(X) ≥ ... ≥ \sigma_r(X) ≥ 0\)는 singular value이다.</p>

<h4 id="참고-l1-norm-regularizer-vs-trace-norm-regularizer">[참고] <strong>L1</strong> Norm Regularizer vs. Trace Norm Regularizer</h4>
<p>이 문제는 matrix soft-thresholding로 원래 soft-thresholding에서의 벡터가 행렬로 대체되었다고 보면 된다. Regularizer 항을 보면 벡터에 대한 <strong>L1</strong> norm regularizer ( \(\lVert \cdot \rVert_{1}\))가 행렬에 대한 trace norm regularizer (\(\lVert \cdot \rVert_{tr}\))로 대체되었는데 실제 두 regularizer의 역할은 같다고 볼 수 있다.</p>

<p><strong>L1</strong> norm regularizer가 벡터를 sparse하게 만들어 주는데,  trace norm regularizer도 행렬의 sigular value vector를 sprase하게 만들어 주기 때문이다. 즉, 행렬이 diagonal일 때 diagonal을 singular value vector로 볼 수 있으며 trace norm regularizer는 singular value의 합을 최소화 하기 때문에 singular value vector를 sparse하게 해준다.</p>

<p>이 문제에서 trace norm \(\lVert B \rVert_{tr}\)는 \(\text{Rank}(B)\)의 approximation으로 사용되었다고 볼 수 있다.</p>

<h2 id="proximal-gradient-descent">Proximal gradient descent</h2>
<p>이 문제를 projection operator를 이용해서 정의하면 proximal gradient descent를 Nice하게 적용할 수 있다.</p>
<h4 id="projection-operator">Projection Operator</h4>
<p>관측값에 대해 projection operator \(P_\Omega\)를 다음과 같이 정의해보자.</p>

<blockquote>
\[[ P_\Omega(B) ] _{ij} =
\begin{cases}
B_{ij}, &amp; (i,j) ∈ \Omega \\\
0, &amp; (i,j) \notin \Omega
\end{cases}\]
</blockquote>

<h4 id="objective-function">Objective Function</h4>
<p>Projection operator를 이용해서 objective 함수를 정의하면 다음과 같다.</p>

<blockquote>
\[f(B) = \underbrace{ \frac{1}{2} \lVert P_\Omega(Y) − P_\Omega(B) \rVert_F^2 }_{g(B)} + \underbrace{ \lambda \lVert B \rVert_{tr} }_{h(B)}\]
</blockquote>

<p>이제 함수 \(f(B)\)는 differentiable 파트인 \(g(B)\)와 non-differentiable 파트로 \(h(B)\)로 구성되었다.</p>

<h4 id="proximal-mapping">Proximal Mapping</h4>
<p>이제 proximal gradient descent를 적용하기 위해 함수 \(g(B)\)의 gradient를 구하고 proximal mapping를  정의해보자.</p>

<ul>
  <li>함수 \(g(B)\)의 gradient : \(\nabla g(B) = −(P_\Omega(Y )−P_\Omega(B))\)</li>
  <li>Proximal mapping :</li>
</ul>

<blockquote>
\[\begin{align}
\text{prox}_t (B) = \underset{Z}{\text{argmin}} \frac{1}{2t} \Vert B−Z \Vert_F^2 + \lambda \Vert Z \Vert_{tr}
\end{align}\]
</blockquote>

<h4 id="matrix-svd--soft-thresholding">Matrix SVD &amp; Soft-thresholding</h4>
<p>Proximal mapping은 \(\lambda\) 레벨에서의 matrix soft-thresholding로 \(\text{prox}_t(B) = S_{\lambda t}(B)\)이다.</p>

<p>일반적으로 행렬 \(B\)는 매우 크기 때문에 Singular Vector Decompoisition(SVD)를 해서 연산량을 최소화 해야만 한다. 따라서, \(B = U \Sigma V^T\)와 같이 SVD를 했다면 \(S_\lambda(B)\)는 다음과 같이 정의할 수 있다.</p>

<blockquote>
\[S_\lambda(B) = U \Sigma_\lambda V^T\]
</blockquote>

<p>이때, \(\Sigma_\lambda\)는 다음과 같은 대각 행렬이다.</p>

<blockquote>
\[(\Sigma_\lambda)_{ii} = \max \{ \Sigma_{ii} −\lambda,0 \}\]
</blockquote>

<h4 id="참고-sigma_lambda_ii--max--sigma_ii-lambda0---inducement">[참고] \((\Sigma_\lambda)_{ii} = \max \{ \Sigma_{ii} −\lambda,0 \}\)  inducement</h4>
<p>이 식이 어떻게 도출되었을까? \(\text{prox}_t(B) = Z\)라고 하면 \(Z\)는 다음과 같다.
(\(\text{prox}_t(B)\)의 우변을 Z에 대해 미분하면 다음 결과를 얻을 수 있다.)</p>

<blockquote>
\[0 ∈ Z − B + \lambda t \cdot \partial \lVert Z \rVert_{tr}\]
</blockquote>

<p>이 식을 정리해 보면 다음과 같다.</p>
<blockquote>
\[\begin{align}
Z &amp; = B - \lambda t \cdot \partial \lVert Z \rVert_{tr} \\
 &amp; = U \Sigma V^T - \lambda t \cdot (UV^T + W) \\
 &amp; = U (\Sigma - \lambda t) V^T - \lambda t  W \\
 &amp; = U (\Sigma - \lambda ) V^T  \\
\end{align}\]
</blockquote>

<p>최종 수식은 Lipschitz constant \(L=1\)일 경우 \(t=1\)이고 \(W\)가 0일 경우에 도출될 수 있다.</p>

<p>따라서, \(\text{prox}_t(B) = S_\lambda(B) = Z\)이므로 다음 식이 도출된다..</p>

<blockquote>
\[(\Sigma_\lambda)_{ii} = \max \{ \Sigma_{ii} −\lambda,0 \}\]
</blockquote>

<h4 id="참고-derivative-of-trace-norm">[참고] Derivative of Trace Norm</h4>
<p>만약 \(Z = U \Sigma V^T\)라면 trace norm이 미분은 다음과 같다.</p>
<blockquote>
\[\partial \lVert Z \rVert_{tr} = \{UV^T + W : \lVert W \rVert_{op} ≤ 1, U^TW = 0, WV = 0 \}\]
</blockquote>

<p>\(\lVert W \rVert_{op}\)는 operator norm으로 biggest singular value가 1보다 작다. 그리고, \(W\)는 \(U^T\)와 \(V\)와 orthogonal하다.</p>

<ul>
  <li>증명은 <a href="https://math.stackexchange.com/questions/701062/derivative-of-the-nuclear-norm-with-respect-to-its-argument">Derivative of the nuclear norm with respect to its argument</a> 참조</li>
</ul>

<h4 id="proximal-gradient-update">Proximal gradient update</h4>
<p>이제 proximal gradient 업데이트 식을 정의해 보자.</p>

<blockquote>
\[B^+ = S_{\lambda t} ( B + t( P_\Omega(Y) − P_\Omega(B) ) )\]
</blockquote>

<p>\(L = 1\)일 때 \(\nabla g(B)\)는 Lipschitz continuous이므로 ﬁxed step size \(t = 1\)로 선택할 수 있다.</p>

<p>따라서, 업데이트 식이 다음과 같이 간단해졌다.</p>
<blockquote>
\[B^+ = S_\lambda (P_\Omega(Y) + P_\Omega^\bot (B) )\]
</blockquote>

<p>\(P_\Omega^\bot\)는 미관측 값에 대한 사영(projection)이며 \(P_\Omega(B) + P_\Omega^\bot (B) = B\)를 만족한다.</p>

<p>이 식에서 \(P_\Omega(Y)\)는 observed 파트이고 \(P_\Omega^\bot (B)\)는 missing 파트이다. \(S_\lambda\) 함수는 입력 행렬을 SVD를 해서 \((\Sigma_\lambda)_{ii} = \max \{ \Sigma_{ii} −\lambda,0 \}\)만 계산하면 되므로 매우 간단하다.</p>

<h2 id="soft-impute-algorithm">Soft-Impute Algorithm</h2>
<p>이 알고리즘을 <strong>Soft-Impute</strong>이라고 하며 matrix completion에 간단하고 효과적으로 할 수 있다. 일반적으로 행렬이 큰 경우 SVD 계산 비용은 매우 높은데, 이 알고리즘에서는 \(P_\Omega(Y)\)가 sparse하고 \(P_\Omega^\bot (B)\)가 low rank이기 때문에 SVD를 효율적으로 할 수 있게 된다.</p>

<ul>
  <li>자세한 내용은 논문 참조 : Mazumder et al. (2011), “Spectral regularization algorithms for learning
large incomplete matrices”</li>
</ul>


</div>










<div class="related">
  <ul class="related-posts">
    
      
    
      
    
      
    
      
        <li>
          <h2>Previous Post</h2>
          <h3>
            <a href="/convex-optimization/contents/chapter09/2020/01/08/09_02_convergence_analysis/">
              09-02 Convergence analysis
            </a>
          </h3>
        </li>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
    
  
    
  
    
  
    
  
    
  
    
      <li>
        <h2>Next Post</h2>
        <h3>
          <a href="/convex-optimization/contents/chapter09/2020/01/08/09_04_special_cases/">
            09-04 Special cases
          </a>
        </h3>
      </li>
    
  
    
  
    
  
    
  
    
  
  </ul>
</div>


<div class="view-counts">
  <a href="https://hits.seeyoufarm.com">
    <img class="view-counts__img" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fconvex-optimization-for-all.github.io/contents/chapter09/2020/01/08/09_03_example_matrix_completion/&count_bg=%237A86DE&title_bg=%23646464&icon=&icon_color=%23FFFFFF&title=views&edge_flat=false"/>
  </a>
</div>

<script src="https://utteranc.es/client.js"
        repo="convex-optimization-for-all/convex-optimization-for-all.github.io"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/convex-optimization/public/js/script.js'></script>
  </body>
</html>
