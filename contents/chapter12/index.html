<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      KKT Conditions &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/convex-optimization/public/css/poole.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/syntax.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/lanyon.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/logo.png">
  <link rel="shortcut icon" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://simonseo.github.io/convex-optimization/convex-optimization/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/convex-optimization/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>12. KKT Conditions</h1>






<!-- Get first post and show it -->

<p>Primal problem이 convex일 때, Karush–Kuhn–Tucker (KKT) conditions는 zero duality gap인 primal &amp; dual optimal points에 대한 충분조건이 된다. 또한 primal problem의 목적함수 및 제약함수들이 미분 가능하며 strong duality를 만족할때는 primal &amp; dual optimal points에서 항상 KKT conditions를 만족하게 된다. KKT conditions는 최적화에서 상당히 중요한 위치를 차지하고 있다. 이 조건은 몇몇 특수한 문제들을 해석적으로(analytically) 풀 수 있게끔 해주기도 하며, 또한 컨벡스 최적화의 많은 알고리즘들이 KKT conditions를 풀기 위한 방법으로 해석되기도 한다 [1]. 이번 장에서는 KKT conditions의 정의와 성질을 알아보고 이에 기반한 몇 가지 예시를 살펴보도록 한다.</p>

<p><em>여담으로</em> KKT conditions는 본래 Harold W. Kuhn과 Albert W. Tucker에 의해 1951년에 세상에 알려졌고, 당시에는 KT (Kuhn-Tucker) conditions로 불렸다. 그리고 이후 학자들에 의해 이 문제의 necessary conditions가 1939년 William Karush의 석사 논문에 의해 다루어졌음이 발견되었는데, 그 때부터 Karush의 이름이 포함되어 KKT (Karush–Kuhn–Tucker) conditions로 불리게 된다 [3].</p>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">12-01 Karush-Kuhn-Tucker conditions</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_2">12-02 Example quadratic with equality constraints</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_3">12-03 Example water-filling</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_4">12-04 Example support vector machines</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_5">12-05 Constrained and Lagrange forms</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_6">12-06 Uniqueness in L1 penalized problems</a>
    </li>
  
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>12-01 Karush-Kuhn-Tucker conditions</h1>
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

<p>다음과 같은 일반적인 최적화 문제가 주어졌다고 하자.</p>

<blockquote>
\[\begin{align}
   &amp;\min_{x} &amp;&amp;{f(x)} \\\\
   &amp;\text{subject to } &amp;&amp;{h_i(x) \le 0, \text{ } i=1,\dots,m} \\\\
   &amp; &amp;&amp;{l_j(x) = 0, \text{ } j=1,\dots,r}.\\\\
\end{align}\]
</blockquote>

<p>이때 <strong>Karush–Kuhn–Tucker (KKT) conditions</strong> 또는 <strong>KKT conditions</strong>는 다음과 같은 조건들로 구성된다 [3].</p>

<ul>
  <li>\(0 \in \partial \big( f(x) + \sum_{i=1}^{m} \lambda_i h_i(x) + \sum_{j=1}^{r} \nu_j l_j(x) \big)\) (Stationarity): \(\lambda, \nu\)를 고정했을 때 \(x\)에 대한 subdifferential이 0을 포함하고 있음을 의미한다.</li>
  <li>\(\lambda_i \cdot h_i(x) = 0 \text{ for all } i\) (Complementary Slackness):  \(\lambda_i\)와 \(h_i\) 중 적어도 하나의 값은 0을 가짐을 의미한다.</li>
  <li>\(h_i(x) \le 0, l_j(x) = 0 \text{ for all } i, j\) (Primal Feasibility): Primal problem의 제약조건들에 대한 만족여부를 나타낸다.</li>
  <li>\(\lambda_i \ge 0 \text{ for all } i\) (Dual Feasibility): Dual problem의 제약조건들에 대한 만족여부를 나타낸다.</li>
</ul>

<h2 id="sufficiency">Sufficiency</h2>
<p>Convex인 primal problem에 대해 KKT conditions를 만족하는 \(x^\star, \lambda^\star, \nu^\star\)가 있을때, 다음의 과정은 \(x^\star, \lambda^\star, \nu^\star\)가 zero duality gap의 primal &amp; dual solutions임을 보인다.</p>

<blockquote>
\[\begin{align}
   g(\lambda^\star, \nu^\star) &amp;= \min_x L(x, \lambda^\star, \nu^\star) \\\\
                               &amp;= L(x^\star, \lambda^\star, \nu^\star) \\\\
                               &amp;= f(x^\star) + \sum_{i=1}^m \lambda_i^\star h_i(x^\star) + \sum_{j=1}^r \nu_j^\star l_j(x^\star) \\\\
                               &amp;= f(x^\star)
\end{align}\]
</blockquote>

<ol>
  <li>\(L(x,\lambda^\star,\nu^\star) = f(x) + \sum_{i=1}^{m} \lambda_i^\star h_i(x) + \sum_{j=1}^{r} \nu_j^\star l_j(x)\)는 convex 함수다. (convex함수들의 합)</li>
  <li>\(0 \in \partial \big( f(x^\star) + \sum_{i=1}^{m} \lambda_i^\star h_i(x^\star) + \sum_{j=1}^{r} \nu_j^\star l_j(x^\star) \big)\)이므로 \(\min_x L(x, \lambda^\star, \nu^\star) = L(x^\star, \lambda^\star, \nu^\star)\)이다.</li>
  <li>Complementary slackness와 primal feasibility에 의해 \(f(x^\star) + \sum_{i=1}^m \lambda_i^\star h_i(x^\star) + \sum_{j=1}^r \nu_j^\star l_j(x^\star) = f(x^\star)\)이다.</li>
</ol>

<h2 id="neccessity">Neccessity</h2>
<p>\(x^\star, \lambda^\star, \nu^\star\)가 zero duality gap(가령, Slater’s condition을 만족)의 primal &amp; dual solutions일때, 다음의 부등호들이 전부 등호가 되므로 이 문제에서 \(x^\star, \lambda^\star, \nu^\star\)는 KKT conditions를 만족하게 된다.</p>
<blockquote>
\[\begin{align}
   f(x^\star) &amp;= g(\lambda^\star, \nu^\star) \\\\
                  &amp;= \min_x  \big( f(x) + \sum_{i=1}^{m} \lambda_i^\star h_i(x) + \sum_{j=1}^{r} \nu_j^\star l_j(x) \big) \\\\
                  &amp;\le f(x^\star) + \sum_{i=1}^m \lambda_i^\star h_i(x^\star) + \sum_{j=1}^r \nu_j^\star l_j(x^\star) \\\\
                  &amp;\le f(x^\star)
\end{align}\]
</blockquote>

<ol>
  <li>\(f(x^\star) = g(\lambda^\star, \nu^\star)\)는 zero duality gap을 의미한다.</li>
  <li>\(f(x^\star) + \sum_{i=1}^m \underbrace{\lambda_i^\star h_i(x^\star)}_{0} + \sum_{j=1}^r \underbrace{\nu_j^\star l_j(x^\star)}_{0} = f(x^\star)\)를 만족하기 위해서는 complementary slackness와 primal feasibility를 만족해야 한다.</li>
  <li>\(f(x^\star) + \sum_{i=1}^m \lambda_i^\star h_i(x^\star) + \sum_{j=1}^r \nu_j^\star l_j(x^\star) = f(x^\star)\)를 만족하면 위 전개의 모든 부등호는 등호가 된다.</li>
</ol>

<h2 id="putting-it-together">Putting it together</h2>
<p>요약하자면 KKT conditions는:</p>

<ul>
  <li>Zero duality gap의 primal &amp; dual solutions에 대한 충분조건이다.</li>
  <li>Strong duality를 만족한다면 primal &amp; dual solutions에 대한 필요조건이 된다.</li>
</ul>

<p>즉, strong duality를 만족하는 문제에 대해 다음과 같은 관계가 성립한다.</p>
<blockquote>
\[\begin{align}
   x^\star, \lambda^\star, \nu^\star \text{ are primal and dual solutions} \\\\
   \Leftrightarrow x^\star, \lambda^\star, \nu^\star \text{ satisfy the KKT conditions} \\\\
\end{align}\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>12-02 Example quadratic with equality constraints</h1>
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

<p>Equality constraint만을 가진 <a href="">quadratic program</a>은 다음과 같다.</p>
<blockquote>
\[\begin{align}
   &amp;\min_{x} &amp;&amp;{(1/2)x^T P x + q^T x + r} \\\\
   &amp;\text{subject to} &amp;&amp;{Ax = b},\\\\
&amp;\text{where } &amp;&amp;P \in \mathbb{S}_{+}^n \text{ and } A \in \mathbb{R}^{\text{p x n}}.
\end{align}\]
</blockquote>

<p>위 문제는 convex이고 inequality constraint가 없으므로 이 문제는 Slater’s condition을 만족한다 (Strong duality). 이때, primal &amp; dual solutions가 \(x^\star, \nu^\star\)라고 하면 KKT conditions에 의해 아래의 조건들을 만족한다 [1].</p>

<ul>
  <li>Stationarity: \(Px^\star + q + A^T\nu^\star = 0\)</li>
  <li>Complementary Slackness: Inequality constraint가 없으므로 고려하지 않아도 된다.</li>
  <li>Primal &amp; dual feasibility: \(Ax^\star = b\)</li>
</ul>

<p>위 조건들은 block matrix를 이용하여 간략하게 표현할 수 있으며, 이를 KKT matrix라고 부른다 [3].</p>
<blockquote>
\[\begin{bmatrix}
    P       &amp; A^T  \\\\
    A       &amp; 0  \\\\
\end{bmatrix}
\begin{bmatrix}
    x^\star  \\\\
    \nu^\star  \\\\
\end{bmatrix}
=
\begin{bmatrix}
    -q  \\\\
    b  \\\\
\end{bmatrix}\]
</blockquote>

<p>이 matrix곱을 풀면 주어진 문제에 대한 primal &amp; dual solutions를 구할 수 있다.</p>

<p>흥미로운 사실은 이 문제는 equality constrained problem에 대한 Newton step을 구하는 것과 같다고도 볼 수 있다는 것이다 [3]. \(min_x f(x) \text{ subject to } Ax = b\) 라는 문제에 대해 P, q, r을 다음과 같이 설정하면 quadratic program의 목적함수는 \(f(x)\)의 second-order Taylor expansion과 동일해진다.<br /></p>
<blockquote>
  <p>\(P = \nabla^2 f(x^{(k-1)})\), \(q = \nabla f(x^{(k-1)})\), \(r = f(x^{(k-1)})\)</p>
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>12-03 Example water-filling</h1>
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

<p>다음과 같은 컨벡스 최적화 문제가 주어졌다고 하자.</p>

<blockquote>
\[\begin{align}
   &amp;\min_{x} &amp;&amp;{- \sum_{i=1}^n \log(\alpha_i + x_i)} \\\\
   &amp;\text{subject to} &amp;&amp;{x \succeq 0, 1^Tx = 1},\\\\
&amp;\text{where } \alpha_i &gt; 0.
\end{align}\]
</blockquote>

<p>이 문제는 n개의 communication channels에 전력을 할당하는 문제이며, 정보이론(information theory)에서 대두되었다. 변수 \(x_i\)는 i번째 채널에 할당되는 송신기의 출력을 나타내며, \(\log(\alpha_i + x_i)\)는 해당 채널의 capacity 또는 communication rate를 나타낸다. 즉, 이 문제는 communication rate의 총합을 최대화하기 위해 각 채널에 얼마만큼의 전력을 할당해야 하는지 결정하기 위한 문제이다 [1].</p>

<p>Inequality constraint \(x^\star \succeq 0\)와 equality constraint \(1^Tx^\star = 1\)에 대한 Lagrange multipliers를 각각 \(\lambda^\star \in \mathbb{R}^n\), \(\nu^\star \in \mathbb{R}\)라고 하자. 이때 주어진 문제에 대한 KKT conditions는 다음과 같다.</p>
<blockquote>
\[\begin{align}
x^\star \succeq 0, \\\\
1^Tx^\star = 1, \\\\
\lambda^\star \succeq 0, \\\\
\lambda_i^\star x_i^\star = 0, \text{    } i = 1, \dots, n, \\\\
-1 / (\alpha_i + x_i^\star) - \lambda_i^\star + \nu^\star = 0,  \text{    } i= 1, \dots, n.
\end{align}\]
</blockquote>

<p>KKT conditions를 통해 얻은 수식들을 이용하면 \(x^\star, \lambda^\star, \nu^\star\)를 해석적으로(analytically) 구할 수 있다. 일단 \(\lambda^\star\)를 slack variable로 사용하여 위 수식에서 \(\lambda^\star\)를 제거한다.</p>
<blockquote>
\[\begin{align}
x^\star \succeq 0, \\\\
1^Tx^\star = 1, \\\\
x_i^\star(\nu^\star - 1 / (\alpha_i + x_i^\star)) = 0, \text{    } i = 1, \dots, n, \\\\
\nu^\star \ge 1/(\alpha_i + x_i^\star),  \text{    } i= 1, \dots, n.
\end{align}\]
</blockquote>

<p>이는 stationarity와 complementary slackness에 의해 다음과 같이 정리된다.</p>
<blockquote>
\[x_i^\star = 
\begin{cases}
1 / \nu^\star - \alpha_i &amp;\nu^\star &lt; 1/\alpha_i \ \\\\
0 &amp;\nu^\star \ge 1/\alpha_i\\\\
\end{cases}
= \max\{0, 1/\nu^\star - \alpha_i \}, \quad i = 1, \dots, n.\]
</blockquote>

<p>또한 조건 \(1^T x^\star = 1\)에 의해 \(x_i^\star, i = 1, \dots, n\)은 합산하여 1이 된다.</p>
<blockquote>
\[\sum_{i=1}^n \max\{0, 1/\nu^\star - \alpha_i \} = 1.\]
</blockquote>

<p>위 등식의 좌항은 \(1/\nu^\star\)에 대한 piecewise-linear increasing function이므로 이 등식은 고정된 \(\alpha_i\)에 대해 unique solution을 갖는다.</p>

<p>이 solution method를 일컬어 water-filling이라고 부른다. 이 문제는 \(\alpha_i\)가 patch \(i\)에 대한 ground level이라고 할 때, 아래 그림과 같이 물의 높이가 \(1/\nu^\star\)가 되도록 각 영역에 물을 붓는 것으로 생각할 수 있다. 우리는 전체 물의 양이 1이 될 때까지 물을 붓도록 한다.</p>

<figure class="image" style="align: center;">
<p align="center">
 <img src="/convex-optimization/img/chapter_img/chapter12/water-fill.png" alt="" width="70%" height="70%" />
 <figcaption style="text-align: center;">[Fig1] Illustration of water-filling algorithm [1]</figcaption>
</p>
</figure>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>12-04 Example support vector machines</h1>
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

<p>Non-separable set에 대한 support vector machine 문제는 다음과 같다.</p>

<blockquote>
\[\begin{align}
   &amp;\min_{\beta, \beta-0, \xi} &amp;&amp;{\frac{1}{2} \rVert\beta\rVert_2^2 + C\sum_{i=1}^n \xi_i} \\\\
   &amp;\text{subject to} &amp;&amp;{\xi_i \ge 0, \quad i = 1, \dots, n}\\\\
   &amp; &amp;&amp; y_i (x_i^T \beta + \beta-0) \ge 1 - \xi_i, \quad i = 1, \dots, n,\\\\
&amp;&amp;&amp;\text{given } y \in \{-1, 1\}^n \text{ and } X \in \mathbb{R}^{n \times p}.
\end{align}\]
</blockquote>

<p>주어진 문제의 두 inequality constraints에 대한 0 이상의 Lagrange multipliers를 각각 \(v^\star, w^\star\)이라 할때, Lagrangian function은 다음과 같다.</p>
<blockquote>
\[L(\beta, \beta-0, \xi, v^\star, w^\star) = \frac{1}{2} \rVert\beta\rVert_2^2 + C\sum_{i=1}^n \xi_i - \sum_{i=1}^n v_i^\star \xi_i + \sum_{i=1}^n w_i^\star (1 - \xi_i - y_i ( x_i^T \beta + \beta_0))\]
</blockquote>

<p>위 Lagrangian function을 이용하여 이 문제가 KKT stationarity condition을 만족하게 하는 다음의 조건들을 구할 수 있다. (Lagrangian function을 \(\beta, \beta_0, \xi\)에 대해 각각 미분하여 0이 되는 조건을 유도)</p>
<blockquote>
\[0 = \sum_{i=1}^n w_i^\star y_i, \quad \beta = \sum_{i=1}^n w_i^\star y_i x_i, \quad w^\star = C \cdot 1 - v^\star\]
</blockquote>

<p>또한 두 개의 inequality constraints에 대한 complementary slackness는 다음과 같다.</p>
<blockquote>
\[v_i^\star \xi_i = 0, \quad w_i^\star (1 - \xi_i - y_i (x_i^T \beta + \beta-0)) =0, \quad 1 = 1, \dots, n.\]
</blockquote>

<p>즉, 최적점(optimality)에서 \(\beta^\star = \sum_{i=1}^n w_i^\star y_i x_i\)를 만족하며, \(y_i (x_i^T \beta^\star + \beta-0^\star) = 1 - \xi_i^\star\)일때 \(w_i^\star\)는 nonzero가 되는데, 이런 지점  i를 <strong>support points</strong>라고 부른다.</p>

<ul>
  <li>어떤 support point i에 대해, \(\xi_i^\star = 0\)이면 \(x_i\)는 hyperplane 위에 위치하게 되며 이때 \(w_i^\star \in (0, C]\)이다.</li>
  <li>어떤 support point i에 대해, \(\xi_i^\star \neq 0\)이면 \(x_i\)는  hyperplane의 반대쪽에 위치하게 되며 이때 \(w_i^\star = C\)다.</li>
</ul>

<figure class="image" style="align: center;">
<p align="center">
 <img src="/convex-optimization/img/chapter_img/chapter12/svm.png" alt="" width="70%" height="70%" />
 <figcaption style="text-align: center;">$$ \text{[Fig1] Illustration of support points with }\ \xi^\star \neq 0 \text{ [3]}$$ </figcaption>
</p>
</figure>

<p>SVM문제를 최적화 하기 전, non-support points를 걸러내는데 위 방법을 이용할 수 있다 (non-support points를 걸러냄으로써 계산 효율을 높일 수 있다). 사실 KKT conditions는 이 문제의 solution을 도출하기 위한 직접적인 역할을 하지는 않지만, 우리는 이를 통해 SVM 문제를 더 잘 이해하기 위한 직관을 얻을 수 있다 [3].</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_5"></a>12-05 Constrained and Lagrange forms</h1>
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

<p>통계학과 기계학습에서는 종종 <strong>constrained form</strong>과 <strong>Lagrange form</strong> 사이를 오가곤 한다. Constrained form과 Lagrangian form을 다음과 같이 정의해보자.</p>

<h4 id="constrained-form-이하-c">Constrained Form (이하 (C))</h4>
<blockquote>
\[\min_x \: f(x) \quad \text{ subject to } h(x) \le t,\\\\
\text{where } t \in \mathbb{R} \text{ is a tuning parameter.}\]
</blockquote>

<h4 id="lagrange-form-이하-l">Lagrange Form (이하 (L))</h4>
<blockquote>
\[\min_x \: f(x) + \lambda \cdot h(x),\\\\
\text{where } \lambda \ge 0 \text{ is a tuning parameter.}\]
</blockquote>

<p>\(f, h\)가 convex라고 가정할때, 두 문제가 동일한 solution을 도출하는 경우에 대해 알아보도록 하자.</p>

<ol>
  <li><strong>(C) to (L):</strong> (C)가 strictly feasible (Slater’s condition을 만족)하여 strong duality를 만족할 때, (C)의 solution인 \(x^\star\)에 대해 다음의 목적함수를 최소화하는 dual solution \(\lambda^\star \ge 0\)가 존재한다면 \(x^\star\)는 또한 (L)의 solution이다.</li>
</ol>

\[f(x) + \lambda \cdot (h(x) - t)\]

<ol>
  <li><strong>(L) to (C):</strong> 만약 \(x^\star\)가 (L)의 solution이고, \(t = h(x^\star)\)인 (C)가 KKT conditions를 만족하면, \(x^\star\)는 또한 (C)의 solution이다. (L)의 KKT conditions를 만족하는 \(\lambda^\star, x^\star\)는 \(t = h(x^\star)\)인 (C)의 KKT conditions를 또한 만족하기 때문이다.</li>
</ol>

<blockquote>
  <p><strong>\(\rightarrow\) (L)의 KKT conditions:</strong></p>

\[\begin{align}
\nabla_x f(x^\star) + \lambda^\star \nabla_x h(x^\star) &amp;= 0\\\\
\lambda^\star &amp;\ge 0\\\\
\end{align}\]

  <p><strong>\(\rightarrow\) \(t = h(x^\star)\)인 (C)의 KKT conditions:</strong></p>

\[\begin{align}
\nabla_x f(x^\star) + \lambda^\star \nabla_x h(x^\star) &amp;= 0\\\\
\lambda^\star &amp;\ge 0\\\\
\lambda^\star (\underbrace{h(x^\star) - h(x^\star)}_{=0}) &amp;= 0
\end{align}\]
</blockquote>

<p>결론적으로 1과 2는 각각 다음과 같은 관계를 보인다.</p>

<figure class="image" style="align: center;">
<p align="center">
 <img src="/convex-optimization/img/chapter_img/chapter12/conclusion.png" alt="" width="70%" height="70%" />
 <figcaption style="text-align: center;">[Fig1] Conclusion [3]</figcaption>
</p>
</figure>

<p>그렇다면 어떤 상황에서 (C)와 (L)이 perfect equivalence를 보이게 될까?<br />
가령, \(h(x) \ge 0\) (예를 들어 norm), \(t = 0\)이고, \(\lambda = \infty\)인 경우에는 perfect equivalence를 보인다. 주어진 조건에 의해 (C)에서의 제약조건은 \(h(x) = 0\)이 되는데, \(\lambda\)를 \(\infty\)으로 설정하게되면 (L)에서 또한 동일한 제약조건(\(h(x) = 0\))을 거는 것과 같은 효과를 보인다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_6"></a>12-06 Uniqueness in L1 penalized problems</h1>
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

<p>다음의 \(L1\) penalized linear regression 문제는 lasso problem이란 이름으로도 잘 알려져 있다.</p>

<blockquote>
\[\begin{align}
&amp;&amp;&amp;\hat{\beta} \in \text{argmin}_{\beta \in \mathbb{R}^p} \frac{1}{2} \| y - X\beta \|^2_2 + \lambda \|\beta\|_1, \qquad \\\\
&amp;&amp; \text{ --- (1) } &amp;\text{given } y \in \mathbb{R}^n, \\\\
&amp;&amp;&amp; \text{ a matrix } X \in \mathbb{R}^{n \text{ x } p} \ \text{ of predictor variables,} \\\\
&amp;&amp;&amp; \text{and a tuning parameter} \lambda \ge 0.
\end{align}\]
</blockquote>

<p>위 Lasso problem은 \(rank(X) = p\)일 때 strictly convex가 되면서 유일한 solution을 갖는다. 반면, \(rank(X) &lt; p\)일때(strictly convex가 아닐때)는 무수히 많은 solution을 갖을 수도 있게된다 (Reference: <a href="https://en.wikipedia.org/wiki/Underdetermined_system">
Underdetermined system</a>). - 참고로 변수(p)의 갯수가 관측(n)의 갯수보다 크다면, \(rank(X)\)는 반드시 \(p\)보다 작아진다.<br />
흥미롭게도 어떤 특수한 경우에 대해서는 \(X\)의 차원에 상관없이 Lasso problem이 유일한 해를 가짐이 보장된다 [13].</p>

<blockquote>
  <p><strong>Theorem:</strong> 함수 \(f\)가 미분가능하며 strictly convex이고, \(\lambda &gt; 0\)이며 \(X \in \mathbb{R}^{n  \text{ x } p}\)가 \(\mathbb{R}^{np}\)에 대한 어떤 continuous probability distribution을 따를 때, 다음 최적화 문제는 항상 유일한(unique) solution을 갖는다. 또한 그 solution은 많아봐야 \(min\{n,p\}\)만큼의 nonzero components로 구성된다. 이때, \(X\)의 차원에 대한 제약은 없다. (즉, p » n일때도 유효)</p>
</blockquote>

<h2 id="basic-facts-and-the-kkt-conditions">Basic facts and the KKT conditions</h2>

<blockquote>
  <p><strong>Lemma 1.</strong> 임의의 \(y, X, \lambda \ge 0\)에 대해 lasso problem (1)은 다음과 같은 성질을 갖는다.</p>

  <ol>
    <li>유일한 solution을 갖거나 혹은 무한히 많은 수의 solution을 갖는다.</li>
    <li>모든 lasso solution \(\hat{\beta}\)는 같은 \(X\hat{\beta}\)값을 갖는다.</li>
    <li>\(\lambda &gt; 0\)일때, 모든 lasso solution \(\hat{\beta}\)는 같은 \(l_1\) norm을 갖는다 (\(\|\hat{\beta}\|_1\)).</li>
  </ol>
</blockquote>

\[\text{ }\]

<blockquote>
  <p><strong>Proof.</strong><br /></p>
  <ol>
    <li>만약 (1)이 두 개의 solution \(\hat{\beta}^{(1)}\), \(\hat{\beta}^{(2)}\)를 가질때, 임의의 \(0 &lt; \alpha &lt; 1\)에 대해 \(\alpha \hat{\beta}^{(1)} + (1 - \alpha) \hat{\beta}^{(2)}\) 또한 solution이 되므로 무수히 많은 solution이 존재하게 된다.<br /></li>
    <li>&amp; 3. 두 개의 solution \(\hat{\beta}^{(1)}\), \(\hat{\beta}^{(2)}\)가 있다고 가정해보자. 이때 optimal value를 \(c^\star\)라고 하면, 어떤 임의의 solution인 \(\alpha \hat{\beta}^{(1)} + (1 - \alpha) \hat{\beta}^{(2)}\) (\(0 &lt; \alpha &lt; 1\))에 대해 아래의 등식을 항상 만족해야만 한다.</li>
  </ol>

\[\begin{align}
&amp;\frac{1}{2} \| y - X(\alpha \hat{\beta}^{(1)} + (1 - \alpha) \hat{\beta}^{(2)}) \|_2^2 + \lambda \| \alpha \hat{\beta}^{(1)} + (1 - \alpha) \hat{\beta}^{(2)} \|_1 \\
&amp; = \alpha c^\star + (1-\alpha) c^\star = c^\star
\end{align}\]

  <p>위 등식을 만족하기 위해서는 임의의 solution \(\hat{\beta}\)에 대해 \(X\hat{\beta}\)은 항상 같은 값을 가져야 하고, \(\lambda &gt; 0\)일때 \(\| \hat{\beta} \|_1\) 값 또한 항상 같아야 한다.</p>
</blockquote>

<p>다시 처음으로 돌아가, lasso problem (1)에 대한 KKT conditions는 아래와 같다.</p>
<blockquote>
\[\begin{align}
&amp;&amp;X^T (y - X\hat{\beta}) = \lambda \gamma, \qquad \text{ --- (2)} \\\\
&amp;&amp;\gamma_i \in 
\begin{cases}
\{ sign(\hat{\beta_i}) \} &amp; if \hat{\beta_i} \neq 0 \\\\
[-1, 1] &amp; if \hat{\beta_i} = 0,
\end{cases} \\\\
&amp;&amp;\text{for } i = 1, \dots, p. \text{ --- (3)} \\\\
&amp;&amp;\text{Here } \gamma \in \mathbb{R}^p \text{ is called a subgradient of the function } \\
&amp;&amp;f(x) = \| x \|_1 \text{ evaluated at } x = \hat{\beta}.
\end{align}\]
</blockquote>

<p>즉, (1)의 solution인 \(\hat{\beta}\)는 어떤 \(\gamma\)에 대해 (2) 와 (3)을 만족한다.</p>

<p>위에서 얻은 KKT conditions를 이용하여 lasso solution에 대한 조건을 좀 더 명시적인 형태로 변환해보도록 하자. 이후의 진행에서는 유도의 간결함을 위해 \(\lambda &gt; 0\)를 가정하도록 한다. 우선 equicorrelation set \(\mathcal{E}\)을 다음과 같이 정의한다. \(\mathcal{E}\)는 \(\hat{\beta}_i \neq 0\)인 모든 인덱스 \(i\)와 \(\hat{\beta}_j = 0\)이면서 \(\vert\gamma_j\vert = 1\)인 모든 인덱스 \(j\)를 원소로 가진 집합이다.</p>

\[\mathcal{E} = \{ i \in \{1, \dots, p \}  : \vert X_i^T (y - X\hat{\beta}) \vert = \lambda \}. \qquad \text{ --- (4)}\]

<p>또한 equicorrelation sign \(s\)를 다음과 같이 정의한다. 여기서 \(X_\mathcal{E}\)는 행렬 X에서 \(i \in \mathcal{E}\)인 column \(i\) 외의 모든 column을 0 벡터로 교체한 행렬을 의미한다.</p>

\[s = sign(X^T_\mathcal{E} (y -X\hat{\beta}). \qquad \text{ --- (5)}\]

<p>여기서 \(\mathcal{E}, s\)는 \(\gamma\)에 대해 다음과 같이 표현할 수 있다: \(\mathcal{E} = \{i \in \{1, \dots, p \} : \vert \gamma_i \vert = 1 \}\) and \(s = \gamma_{\mathcal{E}}\). 또한 Lemma1-2에 의해 \(X\hat{\beta}\)는 유일한 값을 가지므로 이는 \(\mathcal{E}\), \(s\)이 유일함을 암시한다.</p>

<p>(3)의 subgradient \(\gamma\)에 대한 정의에 의해 모든 lasso solution \(\hat{\beta}\)에 대해 \(\hat{\beta}_{-\mathcal{E}} = 0\)임을 알 수 있다. 그러므로 (2)를 \(\mathcal{E}\) 블록에 대해 표현하면 다음과 같다.</p>

\[X^T_\mathcal{E} ( y - X_\mathcal{E} \hat{\beta_\mathcal{E}} ) = \lambda \gamma_\mathcal{E}=  \lambda s. \qquad \text{ --- (6)}\]

<p>(6)의 양변에 \(X^T_\mathcal{E} (X^T_\mathcal{E})^+\)를 곱하면 다음과 같이 정리된다 (\((X^T_\mathcal{E})^+\)는 \(X^T_\mathcal{E}\)의 pseudoinverse matrix).</p>

\[\begin{align}
&amp; X^T_\mathcal{E} X_\mathcal{E} \hat{\beta_\mathcal{E}} = X^T_\mathcal{E} ( y - (X^T_\mathcal{E})^+  \lambda s) \\\\
\Leftrightarrow
&amp; X_\mathcal{E} \hat{\beta_\mathcal{E}} = X^T_\mathcal{E} (X^T_\mathcal{E})^+ ( y - (X^T_\mathcal{E})^+  \lambda s).
\end{align}\]

<p>\(X\hat{\beta} = X_\mathcal{E} \hat{\beta_\mathcal{E}}\)이므로 위 등식은 곧 아래와 같다.</p>

\[X \hat{\beta} = X^T_\mathcal{E} (X^T_\mathcal{E})^+ ( y - (X^T_\mathcal{E})^+  \lambda s), \qquad \text{ --- (7)}\]

<p>그리고 임의의 lasso solution \(\hat{\beta}\)는 다음과 같다.</p>

\[\begin{align}
&amp; \hat{\beta_{-\mathcal{E}}} = 0 \text{ and } \hat{\beta_{\mathcal{E}}} = (X^T_\mathcal{E})^+ ( y - (X^T_\mathcal{E}) + b, \qquad \text{ --- (8)} \\\\
&amp; \text{where } b \in null(X_\mathcal{E}).
\end{align}\]

<h2 id="sufficient-conditions-for-uniqueness">Sufficient conditions for uniqueness</h2>

<p>(8)의 \(\hat{\beta_{\mathcal{E}}}\)의 유일함이 보장되기 위해서는 \(b=0\)이 되어야 한다 ( \((X^T_\mathcal{E})^+ ( y - (X^T_\mathcal{E})\)은 유일하기 때문에). \(b=0\)이어야 함을 주지하고 (8)의 등식을 변형하면 다음의 결론을 얻게 된다.</p>

<blockquote>
  <p><strong>Lemma 2.</strong> 임의의 \(y, X, \lambda &gt; 0\)에 대해, 만약 \(null(X_\mathcal{E}) = {0}\), 또는 \(rank(X_\mathcal{E}) = \vert\mathcal{E}\vert\) (<a href="https://www.quora.com/When-the-null-space-of-a-matrix-is-the-zero-vector-the-matrix-is-in\vertible-Why/answer/Alexander-Farrugia">참고</a>),이면 lasso solution은 유일(unique)해지며, 이는 곧 다음과도 같다.</p>
</blockquote>

<blockquote>
\[\begin{align}
&amp;&amp; \hat{\beta_{-\mathcal{E}}} = 0 \text{ and } \hat{\beta_{\mathcal{E}}} = (X^T_\mathcal{E}X^T_\mathcal{E})^{-1} ( X^T_\mathcal{E} y - \lambda s), \qquad \text{ --- (9)} \\\\
&amp;&amp; \text{where } \mathcal{E} \text{ and } s \text{ are the equicorrelation set and signs as defined in (4) and (5)}.
\end{align}\]
</blockquote>

<p>참고로 이 solution은 많아 봐야 \(min\{n, p\}\)의 nonzero components로 구성된다.</p>

<p>그렇다면 \(null(X_\mathcal{E}) = {0}\)을 암시하는 (\(X\)에 대한) 좀 더 자연스러운 조건에 대해 알아보도록 하자. 이를 알아보기에 앞서 우선 \(null(X_\mathcal{E}) \neq {0}\)이라 가정해보겠다. 이 경우, 어떤 \(i \in \mathcal{E}\)에 대해 다음과 같은 등식을 만족한다.</p>

\[X_i = \sum_{j \in \mathcal{E} \backslash \{i\} } c_j X_j,\\\\
\text{where } c_j \in \mathbb{R}, j \in \mathcal{E}.\]

<p>위 등식의 양변에 \(s_i\)를 곱해주고, 우항에 \(s_j s_j = 1\)을 곱해준다.</p>

\[s_i X_i = \sum_{j \in \mathcal{E} \backslash \{i\} } (s_i s_j c_j) \cdot (s_j X_j). \qquad \text{ --- (10)}\]

<p>\(r = y - X \hat{\beta}\)로 r(lasso residual)을 정의하면 임의의 \(j \in \mathcal{E}\)에 대해 \(X_j^T r = s_j \lambda\)를 만족한다. r을 위 (10)의 양변에 곱해주면 \(\lambda\)에 대한 부등식을 얻을 수 있다. (\(\lambda &gt; 0\)이라 가정)</p>

\[\lambda = \sum_{j \in \mathcal{E} \backslash \{i\} } (s_i s_j c_j) \lambda \quad \text{ and } \quad \sum_{j \in \mathcal{E} \backslash \{i\} } (s_i s_j c_j) = 1.\]

<p>즉, \(null(X_\mathcal{E}) \neq {0}\)이면, 어떤 \(i \in \mathcal{E}\)에 대해 다음 등식이 성립한다.</p>

\[s_iX_i = \sum_{j \in \mathcal{E} \backslash \{i\} } a_j \cdot s_j X_j, \text{ with } \sum_{j \in \mathcal{E} \backslash \{i\} } a_j = 1.\]

<p>위 등식은 \(s_iX_i\)이 \(s_j X_j, j \in \mathcal{E} \backslash \{i\}\)의 affine span 위에 존재한다는 의미와도 같다. 또한 이는 어떤 k+2개의 원소를 포함한 subset으로는 최대 k dimensional affine space만을 표현할 수 있다는 것과도 같다.</p>

<figure class="image" style="align: center;">
<p align="center">
 <img src="/convex-optimization/img/chapter_img/chapter12/l1_uniqueness.png" alt="" width="70%" height="70%" />
 <figcaption style="text-align: center;">[Fig 1] 4 elements on 2-dimensional affine space [3]</figcaption>
</p>
</figure>

<p>우리가 원하는 것은 행렬 \(X \in \mathbb{R}^{n \text{ x } p}\)가 \(null(X_\mathcal{E}) = {0}\)을 만족하는 것이며, 이는 곧 행렬 \(X\)의 column들이 <a href="https://en.wikipedia.org/wiki/General_position">general position</a>에 있는 것과도 같다. 바꿔말하면, 그 어떤 k-dimensional affine subspace도 set 안의 k+1개보다 더 많은 element를 포함하지 않는다는 것이다.</p>

<blockquote>
  <p><strong>Lemma 3.</strong> 만약 행렬 \(X\)의 column들이 general position에 있으면, 임의의 \(y\)와 \(\lambda &gt; 0\)에 대한 lasso solution은 유일(unique)하며 또한 이 solution은 (9)를 만족한다.</p>
</blockquote>

<p>그렇다면 어떤 행렬 \(X\)가 항상 위 조건을 만족할 수 있을까? 결론부터 말하자면 다음과 같다.</p>

<blockquote>
  <p><strong>Lemma 4.</strong> 행렬 \(X \in \mathbb{R}^{n \text{ x } p}\)의 모든 원소가 \(\mathbb{R}^{np}\) 상의 continuous probability distribution을 따른다면, 임의의 \(y\)와 \(\lambda &gt; 0\)에 대해 lasso solution은 unuque하고 항상 (9)를 만족한다.</p>
</blockquote>

<p>왜냐하면 continuous probability distribution을 따를때, 모든 column vector들은 linearly independent하기 때문이다. (<a href="https://math.stackexchange.com/questions/432447/probability-that-n-vectors-drawn-randomly-from-mathbbrn-are-linearly-ind?rq=1">참고</a>)</p>

<h2 id="general-convex-loss-functions">General convex loss functions</h2>

<p>좀 더 일반적인 lasso problem에 대해서도 같은 내용을 적용할 수 있다 [13].</p>

\[\hat{\beta} \in \text{argmin}_{\beta \in \mathbb{R}^p} f(X\beta) + \lambda \|\beta\|_1, \qquad \text{ --- (11) }\]

<blockquote>
  <p><strong>Lemma 5.</strong> 만약 행렬 \(X \in \mathbb{R}^{n \text{ x } p}\)의 모든 원소가 \(\mathbb{R}^{np}\) 상의 continuous probability distribution을 따를때, 미분 가능하고 strictly convex인 임의의 함수 \(f\)는 임의의 \(\lambda &gt; 0\)에 대해 (11)의 문제에서 항상 유일(unique)한 solution을 보장한다. 이 solution은 많아봐야 \(min\{n,p\}\)개의 nonzero components로 구성된다.</p>
</blockquote>

        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/convex-optimization/public/js/script.js'></script>
  </body>
</html>
