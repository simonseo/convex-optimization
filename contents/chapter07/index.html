<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      Subgradient &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/convex-optimization/public/css/poole.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/syntax.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/lanyon.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/logo.png">
  <link rel="shortcut icon" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://simonseo.github.io/convex-optimization/convex-optimization/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/convex-optimization/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>07. Subgradient</h1>






<!-- Get first post and show it -->

<p>본 장에서는 앞장에서의 gradient 및 first-order 최적 조건을 일반화한 subgradient 및 subgradient 최적 조건의 개념을 설명하며, 몇 가지 적용예들에 대해서 살펴본다.</p>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">07-01 Subgradient</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_2">07-02 Sub-differentials</a>
    </li>
  
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_3"> 07-02-01 Connection to a Convexity Geometry</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_4"> 07-02-02 Subgradient Calculus</a>
    </li>
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_5">07-03 Subgradient Optimality Condition</a>
    </li>
  
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_6"> 07-03-01 Subgradient Optimality Condition</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_7"> 07-03-02 Derivation of First-Order Optimality Condition</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_8"> 07-03-03 Example: Lasso Optimality Condition</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_9"> 07-03-04 Example: Soft-Thresholding</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_10"> 07-03-05 Example: Distance to a Convex Set</a>
    </li>
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>07-01 Subgradient</h1>
            <h1 id="subgradient">Subgradient</h1>

<p>어떤 볼록함수(convex function) \(f:\mathbb{R}^n \to \mathbb{R}\)의 subgradient는 다음의 조건을 만족하는 \(g \in \mathbb{R}^n\)로 정의된다.</p>

<blockquote>
\[\begin{equation}\label{subgradient}
f(y) \geq f(x) + g^T(y-x), \text{ for all y}
\end{equation}\]
</blockquote>

<p>위에서 정의된 subgradient는</p>

<ul>
  <li>미분가능한 볼록함수의 gradient를 미분가능하지 않은 볼록함수에도 적용할 수 있도록 일반화한 것이며,</li>
  <li>볼록함수에 대해서는 항상 존재하는 값으로서, 만약 \(f\)가 \(x\)에서 미분가능하면 \(\nabla f(x)\)를 유일하게 갖게된다.</li>
  <li>비볼록함수(non-convex function) 에 대해서도 동일하게 subgradient가 구해질 수 있으나 이 때는 함수에 따라서 값이 존재하지 않을 수 있다.</li>
</ul>

<p>다음은 몇 가지 함수들에 대한 subgradient의 예를 보여준다.</p>

<h3 id="example1">Example1</h3>

<center>
$$f:\mathbb{R} \to \mathbb{R}, f(x) =  \vert x \vert $$
</center>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter07/07_01_subgrad-1.png" alt="Subgradient1" width="80%" height="80%" />
</p>
  <figcaption style="text-align: center;">$$\text{[Fig 1] Subgradient of } f(x)= \vert x \vert \text{ [3]}$$</figcaption>
</figure>

<ul>
  <li>\(x \neq 0\)에 대해, \(\vert y \vert \geq \vert x \vert + g^T(y-x)\)를 만족해야 한다. 즉,</li>
</ul>

<p>\(\vert y \vert -g^Ty \geq  \vert x \vert -g^Tx\). \(\vert x \vert -g^Tx = 0\)이면 즉, \(g=\text{sign}(x)\)이면, 모든 \(y\)에 대해 항상 만족됨. 따라서, \(g=\text{sign}(x)\) 
[<a href="https://en.wikipedia.org/wiki/Sign_function">(Wikipedia) Sign function</a>]</p>
<ul>
  <li>\(x=0\)에 대해, \(\vert y \vert  \geq g^Ty\)를 만족해야 함. 따라서, \(g \in [-1,1]\)</li>
</ul>

<h3 id="example2">Example2</h3>

<center>
$$f:\mathbb{R}^n \to \mathbb{R}, f(x) =  \vert  \vert x \vert  \vert _1$$ 
</center>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter07/07_01_subgrad-3.png" alt="Subgradient2" width="80%" height="80%" />
</p>
  <figcaption style="text-align: center;">$$\text{[Fig 2] Subgradient of }f(x)= \vert x \vert _1\text{ [3]}$$</figcaption>
</figure>

<p>한 점 \(x=(x_1,x_2,\dots,x_n)\)에서,</p>

<ul>
  <li>
    <p>\(x_i \neq 0, i \in \{1,2,\dots,n\}\)에 대해, \(x_i\)에서 미분가능하므로 \(g_i=\text{sign}(x_i)\)</p>
  </li>
  <li>
    <p>\(x_i=0, i \in \{1,2,\dots,n\}\)에 대해, \(g_i \in [-1,1]\)</p>
  </li>
</ul>

<h3 id="example3">Example3</h3>

<center>
$$f:\mathbb{R}^n \to \mathbb{R}, f(x) =  \vert  \vert x \vert  \vert _2$$
</center>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter07/07_01_subgrad-2.png" alt="Subgradient3" width="80%" height="80%" />
</p>
  <figcaption style="text-align: center;">$$\text{[Fig 3] Subgradient of }f(x)= \vert x \vert _2\text{ [3]}$$</figcaption>
</figure>

<ul>
  <li>
    <p>\(x \neq 0\)에 대해, 미분가능하므로 \(g=\nabla \sqrt{x^Tx} = \frac{1}{2}(x^Tx)^{-\frac{1}{2}} (2x) = \frac{x}{ \vert  \vert x \vert  \vert _2}\)</p>
  </li>
  <li>
    <p>\(x=0\)에 대해, \(\vert  \vert y \vert  \vert _2 \geq g^Ty \Longrightarrow  \vert  \vert y \vert  \vert _2 \geq  \vert  \vert g \vert  \vert _2 \vert  \vert y \vert  \vert _2 \cos \theta\). 따라서 \(g \in \{z: \vert  \vert z \vert  \vert _2 \leq 1 \}\)</p>
  </li>
</ul>

<h3 id="example4">Example4</h3>

<p>\(f(x) = \max f_1(x),f_2(x)\), 이때, \(f_1,f_2:\mathbb{R}^n \to \mathbb{R}\)이며, 모두 볼록함수이고 미분가능.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter07/07_01_subgrad-4.png" alt="Subgradient4" width="80%" height="80%" />
</p>
  <figcaption style="text-align: center;">$$\text{[Fig 4] Subgradient of }f(x)=\max f_1(x),f_2(x) \text{ [3]}$$</figcaption>
</figure>

<ul>
  <li>
    <p>\(f_1(x) &gt; f_2(x)\)에 대해, \(g = \nabla f_1(x)\)</p>
  </li>
  <li>
    <p>\(f_1(x) &lt; f_2(x)\)에 대해, \(g = \nabla f_2(x)\)</p>
  </li>
  <li>
    <p>\(f_1(x) = f_2(x)\)에 대해, \(g \in \{\theta_1 \nabla f_1(x) + \theta_2 \nabla f_2(x): \theta_1 + \theta_2 = 1, \theta_1 \geq 0, \theta_2 \geq 0 \}\)</p>
  </li>
</ul>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>07-02 Sub-differentials</h1>
            <p>한 볼록함수 \(f\)의 \(x\)에서의 subdifferential \(\partial f(x)\)는 \(x\)에서의 모든 subgradient들의 집합을 의미한다.</p>

<blockquote>

  <p>\begin{equation}
\partial f(x) = {g \in \mathbb{R}^n | \text{g is a subgradient of f at x} }
\end{equation}</p>
</blockquote>

<p>Sub-differential은 다음과 같은 특성이 있다.</p>

<ul>
  <li>
    <p>\(\partial f(x)\) 는 \(f\)가 볼록함수이든지 아니든지 항상 닫혀있는 볼록 집합이 된다.</p>
  </li>
  <li>
    <p>\(\partial f(x)\) 는 \(f\)가 볼록함수이면 항상 하나이상의 원소를 가지며, 볼록함수가 아닐때는 공집합이 될 수 도 있다.</p>
  </li>
  <li>
    <p>만약 \(f\)가 \(x\) 에서 미분가능하고 볼록함수이면, \(\partial f\)는 \(\{\nabla f(x)\}\) 만을 원소로 갖는다.</p>
  </li>
  <li>
    <p>만약 \(\partial f(x) = \{g\}\) 이면, \(f\)는 \(x\) 에서 미분가능하며, \(\nabla f(x)\)가 \(g\)가 된다.</p>
  </li>
</ul>


        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>07-02-01 Connection to a Convexity Geometry</h1>
            <p>한 볼록집합 (convex set) \(C \subseteq \mathbb{R}^n\)에 대해서, 아래와 같은 indicator 함수 \(I_C: \mathbb{R}^n \to \mathbb{R}\)를 정의했을 때,</p>

<blockquote>

\[I_C(x) = I\{x \in C \} =
\begin{cases}
0               &amp;\text{if } x \in C \\\\
\infty         &amp;\text{if } x \notin C 
\end{cases}\]
</blockquote>

<p>해당 함수의 subdifferential은 다음의 기하학적 의미가 있다.</p>

<h4 id="lemma">Lemma</h4>
<p>\(x \in C\)에 대해서, \(I_C(x)\) 함수의 \(\partial I_C(x)\)는 \(x\)에서의 집합 \(C\)에 대한 normal cone \(\mathcal{N}_C(x)\)과 일치한다.</p>

<blockquote>

  <p>\begin{equation}
\mathcal{N}_C(x) = {g \in \mathbb{R}^n | g^Tx \geq g^Ty \text{  for all  } y \in C }
\end{equation}</p>
</blockquote>

<h4 id="proof">Proof</h4>

<p>Subgradient는 정의에 의해, 다음의 식이 만족된다.</p>
<blockquote>

  <p>\begin{equation}
I_C(y) \geq I_C(x) + g^T(y-x) \text{ for all \(y\)}
\end{equation}</p>
</blockquote>

<p>여기서, \(x \in C\)이고 \(I_C(x)=0\) 이므로, 아래와 같이 된다.</p>

<blockquote>

  <p>\begin{equation}
I_C(y) \geq g^T(y-x) \text{ for all \(y\)}
\end{equation}</p>
</blockquote>

<p>첫째, 모든 \(y \in C\)에 대해서 아래의 식이 성립되어야 하므로,</p>
<blockquote>

  <p>\begin{equation}
I_C(y) = 0 \geq g^T(y-x)
\end{equation}</p>
</blockquote>

<p>subgradient \(g\)는 \(g^Tx \geq g^Ty\)를 만족해야 한다.</p>

<p>둘째, 모든 \(y \notin C\)에 대해서, \(I_C(y) = \infty\) 이므로, \(g\)가 어떤 값이든 관계없이</p>
<blockquote>

\[I_C(y)=\infty \geq g^T(y-x)\]
</blockquote>

<p>가 항상 성립된다.</p>

<p>위 두 조건에 대해, subgradient는 모두 만족시켜야 하므로, 위 함수에 대한 subgradient는</p>
<blockquote>

\[\{g \in \mathbb{R}^n | g^Tx \geq g^Ty\}\]
</blockquote>

<p>가 된다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter07/07_02_subgrad-5.png" alt="connection_to_convexity_geometry" width="80%" height="80%" />
</p>
  <figcaption style="text-align: center;">[Fig 1] Normal cone [1]</figcaption>
</figure>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>07-02-02 Subgradient Calculus</h1>
            <p>볼록함수의 subdifferential에 대한 다음의 몇가지 기본적인 규칙이 성립된다.</p>

<h3 id="scaling">Scaling</h3>

<blockquote>

\[\eqalign{
\text{if } &amp; a&gt;0, \\
\text{then } &amp;\partial (af) = a\cdot \partial f
}\]
</blockquote>

<h3 id="addition">Addition</h3>

<blockquote>

\[\partial(f_1 + f_2) = \partial f_1 + \partial f_2\]
</blockquote>

<p>위에서 두집합의 연산 \(A+B= \{a+b:a\in A, b \in B\}\)를 의미함.</p>

<h3 id="affine-composition">Affine composition</h3>

<blockquote>

\[\eqalign{
\text{if } &amp; g(x)=f(Ax+b), \\
\text{then } &amp; \partial g(x) = A^T \partial f(Ax+b)
}\]
</blockquote>

<h3 id="finite-pointwise-maximum">Finite pointwise maximum</h3>

<blockquote>

\[\eqalign{
\text{if } &amp; f(x)=\max_{i=1,\dots,m} f_i(x), \\
\text{then } &amp; \partial f(x) = \text{conv}\left(\bigcup_{i:f_i(x)=f(x)} \partial f_i(x)\right)
}\]
</blockquote>

<p>즉, \(\partial f(x)\)는 \(x\)에서 \(f(x)\)값을 갖는 함수들의 subdifferential의 합집합에 대한 convex hull로 정의된다.</p>

<h3 id="general-pointwise-maximum">General pointwise maximum</h3>

<blockquote>
\[\eqalign{
\text{if } &amp; f(x) = \max_{s \in S} f_s(x),\\ 
\text{then } &amp; \partial f(x) \supseteq cl \left \{ \text{conv} \left(\bigcup_{s:f_s(x)=f(x)} \partial f_s(x)\right) \right\}
}\]
</blockquote>

<p>여기서 \(S\)는 무한집합으로서 무한한 갯수의 집합들의 합집합은 열린집합이 될 수 있으므로, subdifferential이 닫힌집합이 될 수 있도록, closure를 취해주어야 한다.</p>

<p>한편 집합 \(S\)가 컴팩트하고 (closed and bounded), \(f_s\) 함수들이 \(s\)에 대해서 연속적이면, 등호 관계가 성립된다.</p>

<p>예를들어 다음의 p-norm 함수 \(f(x)\)에 대해서,</p>
<blockquote>

  <p>\begin{equation}
f(x) =  \vert  \vert x \vert  \vert _p = \max_{ \vert  \vert z \vert  \vert _q \leq 1} z^Tx, \qquad 1/p + 1/q =1
\end{equation}</p>
</blockquote>

<p>\(f_z(x)=z^Tx\)라고 하면, \(f(x)=f_{z^*}(x)\)가 되는 \(z^*\)가 \(\arg\max_{ \vert  \vert z \vert  \vert _q \leq 1} z^Tx\)에 속하게 된다.</p>

<p>한편 \(\partial f_{z^*}(x)=z^*\) 이므로, \(\bigcup \partial f_{z^*}(x)\)는 모든 \(z^*\)의 합집합으로서, \(\partial f(x) = \arg\max_{ \vert  \vert z \vert  \vert _q \leq 1} z^Tx\) 가 된다.</p>

<p>여기서 \(S={z: \vert  \vert z \vert  \vert _q \leq 1}\)는 컴팩트 집합이고, \(f_z(x)=z^Tx\)는 선형이므로,</p>

<p>general pointwise maximum 규칙에 의해,  \(\bigcup \partial f_{z^*}(x)\)에 대해서 convex hull을 취한 뒤 closure를 취해도 추가되는 집합의 원소가 존재하지 않는다.</p>

<p>따라서 \(f(x)\) 함수의 subgradient는 아래와 같다.</p>

<blockquote>

  <p>\begin{equation}
\partial f(x) = \arg\max_{ \vert  \vert z \vert  \vert _q \leq 1} z^T x
\end{equation}</p>
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_5"></a>07-03 Subgradient Optimality Condition</h1>
            <p>본 절에서는 subgradient를 이용한 최적 조건을 살펴보고, 몇 가지 예를 들어 적용방법 및 유용성을 설명한다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_6"></a>07-03-01 Subgradient Optimality Condition</h1>
            <h3 id="lemma">Lemma</h3>

<p>모든 함수 \(f\)에 대해서, 어떤 \(x^*\)에서 함수의 최소값을 갖는 것과 \(x^*\)에서 subgradient가 \(0\)인 것은 서로 필요충분조건이다.</p>

<blockquote>
\[\begin{equation}
f(x^*) = \min_x f(x) \Longleftrightarrow 0 \in \partial f(x^*)
\end{equation}\]
</blockquote>

<h3 id="proof">Proof</h3>
<blockquote>

\[\begin{align}
&amp;f(x^*) = \min_x f(x)\\
\Longleftrightarrow &amp;f(y) \geq f(x^*) \text{ for all } y\\
\Longleftrightarrow &amp;f(y) \geq f(x^*) + 0^T(y-x^*)\\
\Longleftrightarrow &amp;0 \in \partial f(x^*)
\end{align}\]
</blockquote>

<p>위 증명에서 함수 \(f\)에 대한 볼록성은 전혀 이용되지 않았으며, 따라서 비볼록함수에서도 예외없이 적용되는 최적 조건이라고 할 수 있다.</p>


        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_7"></a>07-03-02 Derivation of First-Order Optimality Condition</h1>
            <p>만약 \(f\)가 볼록함수이고 미분가능하면, subgradient 최적 조건은 first-order 최적 조건과 일치함을 아래와 같이 증명할 수 있다.</p>

<h4 id="proof">Proof</h4>
<blockquote>

\[\begin{alignat}{2}
f(x^{*}) = \min_x f(x)  \quad &amp; \Longleftrightarrow &amp; &amp; \quad f(x^{*}) = \min_x f(x) + I_C(x) \\
                      \quad &amp; \Longleftrightarrow &amp; &amp;\quad 0 \in \partial(f(x^{*}) + I_C(x^{*})) \\
                      \quad &amp; \Longleftrightarrow &amp; &amp;\quad 0 \in \{\nabla f(x^{*}) \} + \mathcal{N}_C(x^{*}) \\
                      \quad &amp; \Longleftrightarrow &amp; &amp;\quad - \nabla f(x^{*}) \in \mathcal{N}_C(x^{*}) \\
                      \quad &amp; \Longleftrightarrow &amp; &amp;\quad - \nabla f(x^{*})^Tx^{*} \geq -\nabla f(x^{*})^Ty, \text{ for all }  y \in C \\                      
					  \quad &amp; \Longleftrightarrow &amp; &amp;\quad \nabla f(x^{*})^T(y-x^{*}) \geq 0, \text{ for all } y \in C 
\end{alignat}\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_8"></a>07-03-03 Example: Lasso Optimality Condition</h1>
            <p>아래와 같이 주어진 lasso 문제에 대해,</p>
<blockquote>

  <p>\begin{equation}
\min_{\beta} \frac{1}{2}  \vert  \vert y-X\beta \vert  \vert _2^2 + \lambda \vert  \vert \beta \vert  \vert _1
\end{equation}</p>
</blockquote>

<p>여기서 \(y \in \mathbb{R}^n\), \(X \in \mathbb{R}^{n \times p}\), \(\lambda \geq 0\).</p>

<p>위 문제의 subgradient 최적 조건은 아래와 같이 표현될 수 있다.</p>
<blockquote>

\[\eqalign{
0 \in \partial\left(\frac{1}{2} \vert  \vert y-X\beta \vert  \vert _2^2 + \lambda \vert  \vert \beta \vert  \vert _1\right)
&amp;\quad \Longleftrightarrow \quad 0 \in - X^T (y-X\beta) + \lambda \partial  \vert  \vert \beta \vert  \vert _1 \\
&amp;\quad \Longleftrightarrow \quad X^T (y-X\beta)  = \lambda v \\
&amp; \quad \text{for some } v \in \partial  \vert  \vert \beta \vert  \vert _1
}\\\]
</blockquote>

<p>여기서, 한 점 \(\beta=(\beta_1,\beta_2,\dots,\beta_p )\)에 대한 subgradient를  \(v=(v_1,v_2,\dots,v_p)\) 라고 할 때,</p>

\[v_i, i \in \{1,2,\dots,p \} = 
\begin{cases}
\{ 1 \}  &amp;\text{if } \beta_i &gt; 0 \\
\{-1 \}  &amp;\text{if } \beta_i &lt; 0 \\
[-1,1]   &amp;\text{if } \beta_i = 0
\end{cases}\]

<p>다음을 만족하는 \(\beta\)를 찾을 수 있으며, 해당 \(\beta\)는 최적해가 된다.</p>
<blockquote>

  <p>\begin{equation}
X^T(y-X\beta) = \lambda v 
\end{equation}</p>
</blockquote>

<p>즉, 위 문제에 대한 최적 \(\beta\)에 대해서 다음의 조건이 성립됨을 알 수 있다.</p>
<blockquote>

\[\begin{cases}
X_i^T(y-X\beta) = \lambda \cdot \text{sign}(\beta_i) &amp;\text{if } \beta_i \neq 0 \\
 \vert X_i^T(y-X\beta) \vert  \leq \lambda &amp;\text{if } \beta_i = 0 
\end{cases}\]
</blockquote>

<p>위 식에서 \(X_i, i \in \{1,2,\dots, p \}\)는 주어진 행렬 \(X\)의 \(i\)번째 열(column) 데이터를 의미한다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_9"></a>07-03-04 Example: Soft-Thresholding</h1>
            <p>\(X=I\)인 좀 더 간단한 lasso 문제는 아래와 같다.</p>
<blockquote>

  <p>\begin{equation}
\min_{\beta} \frac{1}{2} \vert \vert y-\beta \vert \vert _2^2 + \lambda \vert \vert \beta \vert \vert _1
\end{equation}</p>
</blockquote>

<p>앞선 예제로부터 subgradient 최적 조건은 아래와 같게 된다.</p>
<blockquote>

\[\begin{cases}
y_i-\beta_i = \lambda \cdot \text{sign}(\beta_i) &amp;\text{if } \beta_i \neq 0 \\
 \vert y_i-\beta_i \vert \leq \lambda &amp;\text{if } \beta_i = 0
\end{cases}\]
</blockquote>

<p>위 조건으로부터 \(\beta = S_{\lambda}(y)\)의 해를 구할 수 있다. 이때,</p>

<blockquote>

\[[S_{\lambda}(y)]_{i} = 
\begin{cases}
y_i - \lambda &amp;\text{if }y_i &gt; \lambda \\
0             &amp;\text{if }-\lambda \leq y_i \leq \lambda, \quad \quad i \in \{1,2,\dots,n \} \\
y_i + \lambda &amp;\text{if } y_i &lt; -\lambda
\end{cases}\]
</blockquote>

<p>여기서 \(S_{\lambda}\)를 soft-thresholding operator라 부른다.</p>

<figure class="image" style="align: center;">
<p align="center">
 <img src="/convex-optimization/img/chapter_img/chapter07/07_03_subgrad-6.png" alt="connection_to_convexity_geometry" width="80%" height="80%" />
</p>
 <figcaption style="text-align: center;">$$\text{[Fig 1] Soft-thresholding, y (x-axis), } \beta \text{ (y-axis), } \lambda=1/2 \text{ [3]}$$ </figcaption>
</figure>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_10"></a>07-03-05 Example: Distance to a Convex Set</h1>
            <p>닫힌 볼록집합 \(C\)까지의 거리함수를 아래와 같이 정의한다.</p>
<blockquote>

  <p>\begin{alignat}{1}
dist(x,C) &amp; = \min_{y \in C} \vert \vert y-x \vert \vert _2 <br />
      &amp; = \vert \vert x-P_C(x) \vert \vert _2 <br />
      &amp; \geq 0 
\end{alignat}</p>
</blockquote>

<p>여기서 \(P_C(x)\)는 한 점 \(x\)에서 집합 \(C\)의 가장 가까운 곳으로의 사영(projection) 이다. 위 거리 함수의 subgradient는 아래와 같다.</p>
<blockquote>

  <p>\begin{equation}
\partial dist(x,C) = {\frac{x-P_C(x)}{ \vert \vert x-P_C(x) \vert \vert _2}}
\end{equation}</p>
</blockquote>

<h4 id="proof">Proof</h4>

<p>만약 \(u=P_C(x)\)라면, first-order 최적 조건에 의해,</p>
<blockquote>

  <p>\begin{equation}
(x-u)^T(y-u) \leq 0 \ \text{ for all \(y \in C\)}
\end{equation}</p>
</blockquote>

<p>여기서,</p>
<blockquote>

  <p>\begin{equation}
C \subseteq H = \{y:(x-u)^T(y-u) \leq 0 \}
\end{equation}</p>
</blockquote>

<p>(i) \(y \in H\)에 대해,</p>
<blockquote>

  <p>\begin{equation}
(x-u)^T(y-u) \leq 0
\end{equation}</p>
</blockquote>

<p>한편, \(dist(y,C)\geq 0\) 이므로</p>
<blockquote>

  <p>\begin{equation}
dist(y,C) \geq \frac{(x-u)^T(y-u)}{ \vert \vert x-u \vert \vert _2} \text{ for all \(y \in H\)}
\end{equation}</p>
</blockquote>

<p>(ii) \(y \notin H\)에 대해,</p>
<blockquote>

  <p>\begin{equation}
(x-u)^T(y-u) = \vert \vert x-u \vert \vert _2 \vert \vert y-u \vert \vert _2 \cos\theta,
\end{equation}</p>
</blockquote>

<p>여기서 \(\theta\)는 \(x-u\) 와 \(y-u\) 가 이루는 각을 의미한다. 그러면,</p>

<blockquote>

\[\eqalign{
dist(y,C) &amp;\geq dist(y,H) \\
&amp;= \vert \vert y-u \vert \vert _2 \cos \theta \\
&amp;= \frac{(x-u)^T(y-u)}{ \vert \vert x-u \vert \vert _2} \text{ for all }y \notin H
}\]
</blockquote>

<p>따라서 (i)과 (ii)로부터, 모든 \(y\)에 대해,</p>
<blockquote>

\[\eqalign{
dist(y,C) &amp;\geq \frac{(x-u)^T(y-u)}{ \vert \vert x-u \vert \vert _2} \\
&amp;= \frac{(x-u)^T(y-x+x-u)}{ \vert \vert x-u \vert \vert _2}\\
&amp; = \vert \vert x-u \vert \vert _2 + \left(\frac{x-u}{ \vert \vert x-u \vert \vert _2}\right)^T(y-x)
}\]
</blockquote>

<p>결론적으로, \(dist(x,C)\)는 \(x\)에서 다음의 subgradient를 갖는다.</p>
<blockquote>

\[g=\frac{x-u}{ \vert \vert x-u \vert \vert _2}=\frac{x-P_C(x)}{ \vert \vert x-P_C(x) \vert \vert _2}\]
</blockquote>

<p>한편, 거리함수의 subdifferential 함수 \(\partial dist(x,C)\)는 하나의 원소만을 갖으므로 \(dist(x,C)\)는 미분가능하고 그 미분값이 곧 subgradient와 일치한다.</p>

        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/convex-optimization/public/js/script.js'></script>
  </body>
</html>
