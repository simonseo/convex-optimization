<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      Alternating Direction Method of Mulipliers &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://convex-optimization-for-all.github.io/public/logo.png">
  <link rel="shortcut icon" href="https://convex-optimization-for-all.github.io/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://convex-optimization-for-all.github.io/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="https://convex-optimization-for-all.github.io/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>21. Alternating Direction Method of Mulipliers</h1>






<!-- Get first post and show it -->

<p>이 장에서는 <a href="/contents/chapter20/2021/03/27/20_00_Dual_Methos/">20장</a>에서 다루었던 ADMM을 조금 더 자세히 다루어보고자 한다. 기본적인 개념은 20장에서 다룬 내용과 깊이에서 큰 차이가 없고, 응용 사례들을 위주로 살펴본다.</p>

<h4 id="참고-논문">참고 논문</h4>

<ul>
  <li>Boyd, Stephen, et al. [BPCPE11] “Distributed optimization and statistical learning via the alternating direction method of multipliers.” Foundations and Trends® in Machine learning 3.1 (2011): 1-122.</li>
  <li>Hong, Mingyi, and Zhi-Quan Luo. [HL12] “On the linear convergence of the alternating direction method of multipliers.” Mathematical Programming 162.1-2 (2017): 165-199.</li>
  <li>Deng, Wei, and Wotao Yin. [DY16] “On the global and linear convergence of the generalized alternating direction method of multipliers.” Journal of Scientific Computing 66.3 (2016): 889-916.</li>
  <li>Iutzeler, Franck, et al. [IBCH14] “Linear convergence rate for distributed optimization with the alternating direction method of multipliers.” 53rd IEEE Conference on Decision and Control. IEEE, 2014.</li>
  <li>Nishihara, Robert, et al. [NLRPJ15] “A general analysis of the convergence of ADMM.” arXiv preprint arXiv:1502.02009 (2015).</li>
  <li>Parikh, Neal, and Stephen Boyd. [NB13] “Proximal algorithms.” Foundations and Trends® in Optimization 1.3 (2014): 127-239.</li>
  <li>Vu, Vincent Q., et al. [VCLR13] “Fantope projection and selection: A near-optimal convex relaxation of sparse PCA.” Advances in neural information processing systems. 2013.</li>
  <li>Candès, Emmanuel J., et al. [CLMW09] “Robust principal component analysis?.” Journal of the ACM (JACM) 58.3 (2011): 11.</li>
  <li>Ramdas, Aaditya, and Ryan J. Tibshirani. [RT16] “Fast and flexible ADMM algorithms for trend filtering.” Journal of Computational and Graphical Statistics 25.3 (2016): 839-858.</li>
  <li>Wytock, Matt, Suvrit Sra, and Jeremy Z. Kolter. [WSK14] “Fast Newton methods for the group fused lasso.” UAI. 2014.</li>
  <li>Barbero, Alvaro, and Suvrit Sra. [BS14] “Modular proximal optimization for multidimensional total-variation regularization.” arXiv preprint arXiv:1411.0589 (2014).</li>
</ul>

<p>ADMM convergence 관련 : [BPCPE11], [HL12], [DY16], [IBCH14], [NLRPJ15]<br />
Sparse subspace estimation : [VCLR13]<br />
Sparse plus low rank decomposition : [CLMW09]<br />
Consensus ADMM : [BPCPE11], [NB13]<br />
Subprogram parameterization : [RT16], [WSK14], [BS14]</p>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">21-01 Last time - Dual method, Augmented Lagrangian method, ADMM, ADMM in scaled form</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_2">21-02 Connection to proximal operators</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_3">21-03 Example - Lasso regression and group lasso Regression</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_4">21-04 Example - Sparse subspace estimation and sparse plus low rank decomposition</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_5">21-05 Consensus ADMM</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_6">21-06 Faster convergence with subprogram parametrization - example of the 2d fused lasso problem</a>
    </li>
  
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>21-01 Last time - Dual method, Augmented Lagrangian method, ADMM, ADMM in scaled form</h1>
            <p>이전 20장에서 우리는 Dual methods, ADMM에 대해 살펴보았다. 여기선 ADMM의 응용을 살펴보기에 앞서, Dual methods와 Augmented Lagrangian method, ADMM, ADMM in scalaed form에 대해 정리하고자 한다.</p>

<h2 id="dual-method">Dual method</h2>
<p>아래의 문제를 살펴보자.</p>

<blockquote>
\[\begin{align}
&amp;\min_{x} &amp;&amp;f(x) \\\\
&amp;\text{ subject to } &amp;&amp;Ax = b
\end{align}\]
</blockquote>

<p>여기서 $f$는 strictly convex하고 닫혀있다고 하자. 이 문제의 Lagrangian은 아래와 같다.</p>
<blockquote>
\[\begin{align}
L(x,u) = f(x)+u^{T}(Ax-b)
\end{align}\]
</blockquote>

<p>위 문제의 dual 문제는 아래와 같다.</p>
<blockquote>
\[\begin{equation}
\max_u -f^{\ast}(-A^T u) - b^T u
\end{equation}\]
</blockquote>

<p>여기에서의 u는 dual variable이다.</p>

<p>이 식에 대한 dual gradient ascent는 아래의 식을 반복적으로 계산한다.($k=1,2,3,…$)</p>
<blockquote>
\[\begin{align}
x^{(k)}&amp;=\underset{x}{\operatorname{argmin}} L(x,u^{(k-1)}) \\\\
u^{(k)}&amp;= u^{(k-1)} +t_{k}(Ax^{(k)}-b)
\end{align}\]
</blockquote>

<p>\(t_{k}\)는 k번째 iteration의 step size이다.</p>

<p>이 dual method에서는, primal 변수 \(x\)는 첫번째 식처럼 이전 스텝에서 주어진 \(u^{(k-1)}\)에서의 Lagrangian을 최소화하는 \(x\)값으로 업데이트되고, dual 변수 \(u\)는 \(Ax-b\)이 gradient 방향인 gradient ascent의 형태로 업데이트가 된다.</p>

<p>이 방법의 장점은 \(f\)가 B개의 문제로 분할이 가능할 때(decomposable), \(x\) 또한 B개의 블록으로 분할하고\(( x =(x_{1}, ...,x_{B})\in \mathbb{R}^{n}, \text{ where }x_{i}\in \mathbb{R}^{n_{i}})\), matrix A 또한 B개의 sub-matrix 블록으로 decompose가 가능해서\((A = [A_{1}, ..., A_{B}] \text{ where }A_{i} \in \mathbb{R}^{m \times n_{i}})\), 쉽게 병렬화 또는 확장이 가능하여 계산이 용이하다. 하지만 단점은 수렴성를 보장하기 위하여 까다로운 조건이 필요하다 ; primal의 feasible을 보장하기 위하여, \(f\)가 strongly convex하다는 조건이 필요하다.<a href="/contents/chapter20/2021/03/27/20_01_01_Convergence_Analysis/">[20-01-01]</a></p>

<h2 id="augmented-lagrangian-method">Augmented Lagrangian method</h2>
<p>Method of multipliers라고도 불리는 Augmented Lagrangian method는 primal 문제에 추가 항을 더하여 계산한다. 이렇게 하면 iteration을 반복되면서 점차 KKT의 conditions을 만족하게 된다. Dual method와 비교하여 수렴성에 대한 조건(f가 strongly convex)을 완화시킨다. 대신 문제의 분해(decompose)가 불가능해지는 단점이 있다. Primal 문제의 정의는 다음과 같다.</p>

<blockquote>
\[\begin{align}
&amp;\min_{x} &amp;&amp;f(x)+\frac{\rho}{2}||Ax-b||_{2}^{2}&amp;\\\\
&amp;\text{subject to} &amp;&amp;Ax=b
\end{align}\]
</blockquote>

<p>여기서 \(\rho&gt;0\)이다. 이 문제의 Lagrangian은 아래와 같다.</p>

<blockquote>
\[\begin{align}
L_{\rho}(x,u)=f(x)+u^{T}(Ax-b)+\frac{\rho}{2}||Ax-b||_{2}^{2}.
\end{align}\]
</blockquote>

<p>Dual gradient ascent는 다음을 반복한다. (\(k=1,2,3,...\))</p>
<blockquote>
\[\begin{align}
x^{(k)}&amp;=\underset{x}{\operatorname{argmin}} L_{\rho}(x,u^{(k-1)}) \\\\
u^{(k)}&amp;= u^{(k-1)} +\rho(Ax^{(k)}-b)
\end{align}\]
</blockquote>

<p>이 방법의 장점은 위에서 언급하였듯, dual method에 비하여 더 나은 수렴 조건을 가진다. 단점은 제곱 항이 추가되는 탓에 분해가능한 성질(decomposability)을 잃게 된다.</p>

<h2 id="alternating-direction-method-of-multipliersadmm">Alternating direction method of multipliers(ADMM)</h2>
<p>ADMM은 dual method와 augmented Lagrangian method의 장점을 섞은 방법이다. 문제가 아래의 형태로 정의 되어있다고 하자.</p>

<blockquote>
\[\begin{align}
\min_{x} f(x)+g(z) \qquad \text{subject to  }Ax+Bz=c
\end{align}\]
</blockquote>

<p>이 식에 \(\rho&gt;0\)인 augmented Lagrangian을 정의할 수 있다.</p>
<blockquote>
\[\begin{align}
&amp;L_{\rho} (x,z,u) = f(x)+g(z)+u^{T}(Ax+Bz-c)+\frac{\rho}{2}||Ax+Bz-c||_{2}^{2}\\\\
\end{align}\]
</blockquote>

<p>이어서 아래를 반복하여 변수를 업데이트한다.</p>
<blockquote>
\[\begin{align}
&amp;\text{for k = 1,2,3,...}\\\\
&amp;x^{(k)}=\underset{x}{\operatorname{argmin}} L_{\rho}(x,z^{(k-1)},u^{(k-1)})\\\\
&amp;z^{(k)}=\underset{z}{\operatorname{argmin}} L_{\rho}(x^{(k)},z,u^{(k-1)})\\\\
&amp;u^{(k)}=u^{(k-1)}+\rho(Ax^{(k)}+Bz^{(k)}-c)
\end{align}\]
</blockquote>

<p>ADMM에서는 primal 변수인 \(x, z\)를 함께 업데이트하지 않고, 순차적으로 각각 업데이트 한다. 그리고 순차적으로 업데이트할 때는 다른 변수는 가장 최근의 값을 이용한다. 즉, k번째 iteration에서 \(z\)를 업데이트 할때에는 이전 iteration의 값 \(x^{(k-1)}\)이 아닌 \(x^{(k)}\)를 이용하고, u를 업데이트 할때 또한 현재 iteration에서 구한 primal 변수 \(x^{(k)}, z^{(k)}\)를 바로 이용한다.</p>

<h2 id="alternating-direction-method-of-multipliersadmm-1">Alternating direction method of multipliers(ADMM)</h2>
<p>ADMM은 제약식 내의 A와 B가 full rank라는 가정 없이, \(f\)와 \(g\)에 대한 큰 제약 없이(under modeset assumption) 모든 \(\rho &gt; 0\)에 대하여 다음을 만족한다.</p>

<ul>
  <li>Residual convergence: \(k\)가 \(\infty\)로 갈 때, \(r^{(k)} = A x^{(k)} - B z^{(k)} - c \to 0\), 즉 primal iteration이 feasibility로 접근한다.</li>
  <li>Objective convergence: \(f(x^{(k)}) + g(x^{(k)}) \to f^{\star} + g^{\star}\), 여기서 \(f^{\star} + g^{\star}\)는 최적의 primal objective 값이다.</li>
  <li>Dual convergence: \(u^{(k)} \to u^{\star}\), 여기서 \(u^{\star}\)는 dual solution 이다.</li>
</ul>

<p>Convergence rate에 대해서는 아직 일반적으로 알려지진 않았고, 연구가 이루어지고있다. Convergence에 대한 참고문헌은 <a href="/contents/chapter21/2021/03/29/21_00_Alternating_Direction_Method_of_Multipliers/">21장 소개파트</a>에 서술되어있다.</p>

<h2 id="admm-in-scaled-form">ADMM in scaled form</h2>
<p>ADMM의 dual 변수 \(u\)를 scale된 변수 \(w=u/\rho\)로 바꾸어서 scaled form으로 표현할 수 있다. 이를 정리하면, ADMM step은 다음과 같이 나타낼 수 있다.</p>
<blockquote>
\[\begin{align}
&amp;x^{(k)} = \underset{x}{\operatorname{argmin}} f(x) + \frac{\rho}{2} ||Ax + Bz^{(k-1)} - c + w^{(k-1)} ||_2^2 \\\\
&amp;z^{(k)} = \arg\min_z g(x) + \frac{\rho}{2} || Ax^{(k)} + Bz - c + w^{(k-1)} ||_2^2  \\\\
&amp;w^{(k)} = w^{(k-1)} + Ax^{(k)} + Bz^{(k)} - c 
\end{align}\]
</blockquote>

<p>여기서, \(w^{(k)}\)은 매순간 residual의 \(k\)번째 까지의 합으로 아래처럼도 표현 가능하다.</p>
<blockquote>
\[\begin{align}
w^{(k)} = w^{(0)} + \sum_{i=1}^k (Ax^{(i)} + Bz^{(i)} - c) 
\end{align}\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>21-02 Connection to proximal operators</h1>
            <p>아래와 같이 한 변수에 대하여 두 개의 함수로 분리된 형태의 최적화 문제를 생각해보자.</p>
<blockquote>
\[\begin{align}
\min_{x} f(x)+g(x)
\end{align}\]
</blockquote>

<p>이는 제약조건을 추가한 형태로도 표현 가능하다.</p>
<blockquote>
\[\begin{align}
\min_{x, z} f(x)+g(z) \qquad \text{subject to  }x=z
\end{align}\]
</blockquote>

<p>여기에서의 ADMM step은 아래와 같다.</p>
<blockquote>
\[\begin{align}
&amp;x^{(k)} = {\operatorname{prox}}_{f,\frac{1}{\rho}}(z^{(k-1)}-w^{(k-1)})\\\\
&amp;z^{(k)} = {\operatorname{prox}}_{g,\frac{1}{\rho}}(x^{(k)}-w^{(k-1)})\\\\
&amp;w^{(k)}=w^{(k-1)}+x^{(k)}-z^{(k)}
\end{align}\]
</blockquote>

<p>\({\operatorname{prox}}_{f,\frac{1}{\rho}}, {\operatorname{prox}}_{g,\frac{1}{\rho}}\)는 각각 파라미터가 \(\frac{1}{\rho}\)일때 f와 g의 proximal operator이다.</p>

<p>참고로 convex 함수 \(f\)에 대한 <a href="/contents/chapter19/2021/03/24/19_01_01_Reminder-_proximal_gradient_descent/">proximal operator의 정의</a>는 다음과 같다.</p>
<blockquote>
\[\begin{align}
{\operatorname{prox}}_{f, \lambda}(v) = \underset{x}{\operatorname{argmin}}(f(x)+\frac{1}{2\lambda}||x-v||_{2}^{2}). 
\end{align}\]
</blockquote>

<p>Proximal operator로 ADMM의 update가 유도되는 과정은 아래와 같다.</p>

<p>\(x^{+}, z^{+}, w^{+}\)를 각각 \(x, z, w\)에서 한 step update된 값들이라 하자.</p>
<blockquote>
  <p>\(\begin{align}
x^{+}&amp; = \underset{x}{\operatorname{argmin}}f(x)+\frac{\rho}{2}||x-z+w||^{2}_{2}\\\\
&amp; =\underset{x}{\operatorname{argmin}}\frac{1}{2\cdot\frac{1}{\rho}}||(z-w)-x||^{2}_{2}+f(x)\\\\
&amp; ={\operatorname{prox}}_{f,\frac{1}{\rho}}(z-w)
\end{align}\)
\(\begin{align}
z^{+}&amp; = \underset{z}{\operatorname{argmin}}g(z)+\frac{\rho}{2}||x^{+}-z+w||^{2}_{2}\\\\
&amp; =\underset{z}{\operatorname{argmin}}\frac{1}{2\cdot\frac{1}{\rho}}||(x^{+}+w)-z||^{2}_{2}+g(z)\\\\
&amp; ={\operatorname{prox}}_{g,\frac{1}{\rho}}(x^{+}+w)
\end{align}\)</p>
</blockquote>

<p>원래의 ADMM에서의 제약식이 \(Ax+Bz = c\)이고, 여기서의 제약식은 \(x=z\)이다. 즉 x와 z의 선형변환 관계가 identity이면, 원 식의 ADMM update를 prox update로 변형할 수 있다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>21-03 Example - Lasso regression and group lasso Regression</h1>
            <h2 id="lasso-regression">Lasso regression</h2>
<p>Lasso regression 문제를 ADMM으로 해결해본다.</p>

<p>\(y\in \mathbb{R}^{n}, X\in \mathbb{R}^{n\times p}\) 일때 lasso 문제는 아래와 같다.</p>
<blockquote>
\[\begin{align}
\min_{\beta}\frac{1}{2}||y-X\beta||^{2}_{2}+\lambda||\beta||_{1}
\end{align}\]
</blockquote>

<p>이전의 여러 장에서, 우리는 lasso 문제를 여러가지 방법으로 해결해보았다. 대표적으로는 <a href="/contents/chapter09/2020/01/08/09_01_proximal_gradient_descent/">proximal gradient descent(ISTA)</a>, <a href="/contents/chapter09/2020/01/08/09_05_03_example_FISTA/">accelerated proximal gradient descent(FISTA)</a>, <a href="/contents/chapter15/2021/03/28/15_barrier_method/">barrier method</a>, <a href="/contents/chapter17/2021/05/01/17_primal_dual_interior_point_method/">primal-dual interior-point method</a> 등이 있다.</p>

<p>ADMM에서는 dual 식을 유도하는 것과 동일하게, 어떤 식으로 보조 변수(auxiliary variable)을 설정하는가에 따라 알고리즘의 성능이 달라진다. 많은 auxiliary variable의 설정 방법 중 아래의 형태가 가장 효과적인 형태 중 하나로 알려져 있다.</p>
<blockquote>
\[\begin{align}
&amp;\min_{\beta, \alpha} &amp;&amp;||y-X\beta||^{2}_{2}+\lambda||\alpha||_{1}\\\\
&amp;\text{subject to} &amp;&amp;\beta-\alpha= 0.
\end{align}\]
</blockquote>

<p>이 식에 대하여 ADMM update는 아래와 같이 유도된다. \(\beta\)에 대한 식은 \(\beta\)가 2차식이므로 미분을 통하여 최솟값을 구할 수 있고, \(\alpha\)에 대한 식은 앞서 <a href="/contents/chapter07/2021/03/25/07_03_04_example_soft-thresholding/">07장(07-03-04)</a>에서 다루었던 문제와 같이 \(\beta^{+}+w\)의 soft-thresholding의 형태로 해가 됨이 알려져 있다.</p>
<blockquote>
\[\begin{align}
\beta^{+} &amp;= \underset{\beta}{\operatorname{argmin}}\frac{1}{2}||y-X\beta||^{2}_{2}+\frac{\rho}{2}||\beta-\alpha+w||^{2}_{2}\\\\
&amp;= (X^{T}X+\rho I)^{-1}(X^{T}y+\rho (\alpha-w))\\\\
\alpha^{+} &amp;= \underset{\alpha}{\operatorname{argmin}}\lambda||\alpha||_{1}+\frac{\rho}{2}||\beta^{+}-\alpha+w||^{2}_{2}\\\\
&amp;= S_{\frac{\lambda}{\rho}}(\beta^{+}+w)\\\\
w^{+} &amp;=w+\beta^{+}-\alpha^{+}
\end{align}\]
</blockquote>

<p>이 결과는 아래와 같은 특징들을 갖는다.</p>

<ul>
  <li>행렬 \(X^{T}X+\rho I\)는 \(\rho&gt;0\)이므로 \(X\)에 관계없이 항상 invertible하다.</li>
  <li>만약 factorization(대표적으로 Cholesky factorization)을 \(O(\rho^{3})\) flops 안에 계산하면, \(\beta\)에 대한 update는 \(O(\rho^{2})\) flops가 걸린다.</li>
  <li>\(\alpha\) update는 soft-thersholding operator \(S_{t}\)를 적용하는 것이 되며, \(S_{t}\)는 <a href="/contents/chapter07/2021/03/25/07_03_04_example_soft-thresholding/">07-03-04</a>의 내용과 동일하다.</li>
  <li>ADMM 스텝은 ridge regression 계수들을 매번 soft-thresholding하는 것과 “거의” 동일하다.</li>
  <li>\(\rho\)를 다르게 주면 다른 결과가 나온다.</li>
</ul>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter21/lasso.png" alt="[Fig 1] Comparison of various algorithms for lasso regression (50 instances with n = 100, p = 20) [3]" width="70%" />
  <figcaption style="text-align: center;">[Fig 1] Comparison of various algorithms for lasso regression (50 instances with n = 100, p = 20) [3]</figcaption>
</p>
</figure>

<p>[Fig 1]은 lasso regression 문제에 대한 다양한 알고리즘들의 수렴을 비교한 것이다. 모든 알고리즘들은 iteration마다 동일한 계산복잡도를 가지고 있다. 그래프의 수렴 속도에서 볼 수 있다시피, ADMM은 proximal gradient descent(검정)와 비슷한 수렴 속도를 가진다. Accelerated proximal gradient descent(빨강)는 “Nestrov ripples”를 가지지만 조금 더 빠른 수렴 속도를 보인다. 또한  ADMM은 \(\rho\) 값에 따라 다른 수렴 속도를 보인다는 특성도 확인할 수 있다. 후에 <a href="/contents/chapter23/2021/03/28/23_Coordinate_Descent/">23장</a>에서 논하게 될 Coordinate descent(초록)의 경우는 문제에서 더 많은 정보들을 사용하고, 따라서 다른 방법들에 비해 빠른 수렴 속도를 가진다. Coordinate descent의 단점은 문제하기 위한 조건들이 존재한다는 것이다.
\(\rho\)값을 너무 크게 설정하면, 목적함수에서 \(f+g\)를 최소화 하는 비중이 작고, \(\rho\)값을 너무 작게 설정하면, feasiblity가 떨어진다. 따라서 적절한 \(\rho\)값의 설정이 중요하다. 자세한 내용은 <a href="/contents/chapter21/2021/03/29/21_00_Alternating_Direction_Method_of_Multipliers/">21장 reference 논문</a> 중 [BPCPE]에서 논하고 있다.</p>

<h2 id="group-lasso-regression">Group lasso regression</h2>
<p>위와 동일하게  Group lasso regression 문제 또한 ADMM으로 해결하는 것에 대하여 살펴보고자 한다. Group lasso regression의 문제정의는 아래와 같다. \(y\in \mathbb{R}^{n}, X\in \mathbb{R}^{n \times p}\)일때,</p>

<blockquote>
\[\begin{align}
\min_{\beta}\frac{1}{2}||y-X\beta||^{2}_{2}+\lambda\sum^{G}_{g=1} c_{g}||\beta_{(g)}||_{2}.
\end{align}\]
</blockquote>

<p>Lasso regression과 동일하게 문제를 다시 정리할 수 있다.</p>
<blockquote>
\[\begin{align}
&amp;\min_{\beta,\alpha} &amp;&amp;\frac{1}{2}||y-X\beta||^{2}_{2}+\lambda\sum^{G}_{g=1} c_{g}||\beta_{(g)}||_{2}\\\\
&amp;\text{subject to} &amp;&amp;\beta-\alpha=0.
\end{align}\]
</blockquote>

<p>ADMM step은 다음과 같다.</p>
<blockquote>
\[\begin{align}
\beta^{+} &amp;= (X^{T}X+\rho I)^{-1}(X^{T}y+\rho (\alpha-w))\\\\
\alpha^{+} &amp;= R_{c_{g}\frac{\lambda}{\rho}}(\beta^{+}_{(g)}+w_{(g)})\qquad \text{g = 1,...G}\\\\
w^{+} &amp;=w+\beta^{+}-\alpha^{+}
\end{align}\]
</blockquote>

<p>이 결과는 아래와 같은 특징들을 갖는다.</p>

<ul>
  <li>행렬 \(X^{T}X+\rho I\)는 \(\rho&gt;0\)이므로 \(X\)에 관계없이 항상 invertible하다.</li>
  <li>만약 factorization(대표적으로 Cholesky factorization)을 \(O(\rho^{3})\) flops 안에 계산하면, \(\beta\)에 대한 update는 \(O(\rho^{2})\) flops가 걸린다.</li>
  <li>\(\alpha\) update는 group soft-thersholding operator \(R_{t}\)를 적용하는 것이 되며, \(R_{t}\)는 아래와 같이 정의된다.</li>
</ul>

<blockquote>
  <p>\begin{align}
R_{t}(x) = (1-\frac{x}{\lVert x \rVert_{2}})_{+}x
\end{align}</p>
</blockquote>


        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>21-04 Example - Sparse subspace estimation and sparse plus low rank decomposition</h1>
            <h2 id="sparse-subspace-estimation">Sparse subspace estimation</h2>
<p>\(S=X^{T}X, X\in \mathbb{R}^{n\times p}\)일때, 원래의 X와 projection된 X와의 Frobenius norm, 즉, 두 matrix의 거리를 최소화하는 projection를 찾는 문제를 생각해보자.</p>

<blockquote>
\[\begin{align}
&amp;\min_{P} &amp;&amp;||X-XP||^{2}_{F}\\\\
&amp;\text{subject to} &amp;&amp;\text{rank(P)=k where P is a projection matrix}
\end{align}\]
</blockquote>

<p>이 문제는 projection 행렬의 set이 convex set이 아니기 때문에 non-convex 문제이다. 하지만, 아래의 convex 문제와 동일함이 알려져 있다.[<a href="/contents/chapter21/2021/03/29/21_00_Alternating_Direction_Method_of_Multipliers/">VCLR13</a>] 이는 subspace estimation problem이라고도 불린다.</p>

<blockquote>
\[\begin{align}
&amp;\max_{Y} &amp;&amp;tr(SY)\\\\
&amp;\text{subject to } &amp;&amp;Y\in F_{k} = \left\{Y\in \mathbb{S}^{p} : 0 \preceq Y \preceq I, tr(Y) = k \right\}
\end{align}\]
</blockquote>

<p>[VCLR13]에서는  sparse version(L1 norm이 추가된 형태)의 subspace estimation problem의 해결을 논한다. 
자세한 유도과정은 해당 논문을 참고한다.</p>
<blockquote>
\[\begin{align}
&amp;\max_{Y} &amp;&amp;tr(SY)-\lambda ||Y||_{1}\\\\
&amp;\text{subject to } &amp;&amp;Y\in F_{k} 
\end{align}\]
</blockquote>

<p>여기서 \(F_{k}\)는 위 식과 동일하게 Fantope of order k이다.</p>

<p>이다. \(\lambda = 0\), 인 경우 위의 문제는 일반적인 PCA와 동일한 문제이다.</p>

<p>위의 문제는 SDP 형태를 가지고 있고, interior point method로 해결이 가능하다. 하지만, 이는 구현이 복잡하고, 문제 크기가 커지면 무척 느려지는 단점이 있다.</p>

<p>이 문제를 ADMM으로 해결하기 위하여, 문제를 아래와 같이 변형한다.</p>
<blockquote>
\[\begin{align}
&amp;\min_{Y,Z} &amp;&amp;-tr(SY)+I_{F_{k}}(Y) + \lambda||Z||_{1}\\\\
&amp;\text{subject to } &amp;&amp;Y = Z.
\end{align}\]
</blockquote>

<p>문제를 정리하면 ADMM step은 다음과 같다.</p>
<blockquote>
\[\begin{align}
Y^{+} &amp;=  \underset{Y}{\operatorname{argmin}} -tr(SY) + I_{F_{k}}(Y)+\frac{\rho}{2}||Y-Z+W||^{2}_{F}\\\\
&amp;=\underset{Y\in F_{k}}{\operatorname{argmin}} \frac{1}{2}||Y-Z+W-\frac{S}{\rho}||^{2}_{F}\\\\
&amp;=P_{F_{k}}(Z-W+\frac{S}{\rho})\\\\
Z^{+} &amp; = \underset{Z}{\operatorname{argmin}}\lambda||Z||_{1}+\frac{\rho}{2}||Y^{+}-Z+W||^{2}_{F}\\\\
&amp;=S_{\frac{\lambda}{\rho}}(Y^{+}+W)\\\\
W^{+} &amp;=W+Y^{+}-Z^{+}.
\end{align}\]
</blockquote>

<p>여기서 \(P_{F_{k}}\)는 fantope projection operator이다. 이는 eigendecomposition \(A= U\sum U^{T}, \sum = diag(\sigma_{1},...\sigma_{p})\)의  clipping으로 정의된다.[<a href="/contents/chapter21/2021/03/29/21_00_Alternating_Direction_Method_of_Multipliers/">VCLR13</a>]:</p>
<blockquote>
\[\begin{align}
P_{F_{k}}(A) = U\Sigma_{\theta}U^{T}, \: \Sigma_{\theta} = diag(\sigma_{1}(\theta),...\sigma_{p}(\theta))
\end{align}\]
</blockquote>

<p>각각 \(\sigma_{i}(\theta) = \min\left\{\max\left\{\sigma_{i}-\theta,0\right\},1\right\}\)이고, \(\sum^{p}_{i=1}\sigma_{i}(\theta)=k\) 이다.</p>

<h2 id="sparse-plus-low-rank-decomposition">Sparse plus low rank decomposition</h2>
<p>\(M\in \mathbb{R}^{n\times m}\)일때, sparse plue low rank decomposition problem은 다음과 같다.[<a href="/contents/chapter21/2021/03/29/21_00_Alternating_Direction_Method_of_Multipliers/">CLMW09</a>]</p>
<blockquote>
\[\begin{align}
&amp;\min_{L,S} &amp;&amp;||L||_{tr}+\lambda||S||_{1}\\\\
&amp;\text{subject to } &amp;&amp;L+S=M
\end{align}\]
</blockquote>

<p>이 문제의 목표는 관측된 행렬 M을 low rank 행렬 L과 sparse matrix S로 분해(decompose)하는 것이다. 목적함수의 첫번째 항은 L의 trace penalty로, L의 singular value의 합을 최소화한다. 두번째 항은 행렬 S에 대한 \(l_{1}\) norm으로  S에 대한 sparsity를 유도한다. \(\lambda\)는 이 둘을 조절하는 tuning parameter이다. trace norm과 \(l_{1}\) norm 모두 smooth하지 않고, 일반적으로 trace norm은 해를 찾기 어렵다고 알려져 있다. Sparse subspace estimation 문제와 동일하게 이 문제는 SDP의 형태를 가지고, interior point method로 해결 가능하지만, 이 또한 복잡하고 속도가 느리다. 이 문제에 대하여  ADMM은 조금 더 쉬운 update step을 보여준다.</p>

<blockquote>
\[\begin{align}
L^{+} &amp;= S^{tr}_{\frac{1}{\rho}}(M-S+W)\\\\
S^{+} &amp;= S^{l_{1}}_{\frac{\lambda}{\rho}}(M-L^{+}+W)\\\\
W^{+} &amp;= W+M-L^{+}-S^{+}
\end{align}\]
</blockquote>

<p>각각 \(S^{tr}_{\frac{1}{\rho}}\)는 matrix soft-thresholding, \(S^{l_{1}}_{\frac{\lambda}{\rho}}\)는 elementwise soft-thresholding이다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter21/candes.png" alt="[Fig 1] Example of sparse plue low rank decomoposition on surveliance camera[3]" width="70%" />
  <figcaption style="text-align: center;">[Fig 1] Example of sparse plue low rank decomoposition on surveliance camera[3]</figcaption>
</p>
</figure>

<p>[Fig 1]은 sparse plue low rank decomoposition을 감시카메라 비디오 영상에 분석에 활용한 예시이다. 고정된 지역을 오랜 시간 촬영하는 감시카메라로부터, 대부분의 프레임을 공유하는 low rank 부분을 쉽게 분리해낼 수 있고, sparse한 부분은 특정한 프레임들에 대한 특징적인 부분을 뽑아낸다. 예를 들어서 [Fig 1]의 가운데 column은 low rank, 우측 column은 sparse 부분을 나타낸다. 확인할 수 있듯이, low rank 부분은 거의 모든 프레임에서 나타나는 배경 정보를 가지고 있고, sparse한 부분은 특정한 프레임들에서만 나타나는 특징적인 부분만을 담고 있음을 확인할 수 있다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_5"></a>21-05 Consensus ADMM</h1>
            <h2 id="consensus-admm">Consensus ADMM</h2>
<p>아래와 같은 문제를 생각해보자.</p>
<blockquote>
\[\begin{align}
\min_{x}\sum^{B}_{i=1} f_{i}(x)
\end{align}\]
</blockquote>

<p>위 문제에 대하여 ADMM으로 해결하기 위해서는, constraint를 도입하여야 했다. 여기서는 update를 병렬적으로 연산하기 용이한 형태로 식을 변형하고자 한다. consensus ADMM이라 불리는 이 접근은 식을 아래와 같이 reparametrize한다.</p>
<blockquote>
\[\begin{align}
&amp;\min_{x_{1},...,x_{B},x} &amp;&amp;\sum^{B}_{i=1} f_{i}(x_{i})\\\\
&amp;\text{subject to } &amp;&amp;x_{i}=x, i = 1,...B
\end{align}\]
</blockquote>

<p>이를 정리하면 deomposable한 ADMM step을 계산할 수 있다.</p>

<blockquote>
\[\begin{align}
x^{(k)}_{i} &amp;= \underset{x_{i}}{\operatorname{argmin}} f_{i}(x_{i})+\frac{\rho}{2}||x_{i}-x^{(k-1)}+w_{i}^{(k-1)}||_{2}^{2}, i=1,...B\\\\
x^{(k)} &amp;=\frac{1}{B}\sum_{i=1}^{B}(x_{i}^{(k)}+w_{i}^{(k-1)})\\\\
w_{i}^{(k)} &amp;=w_{i}^{(k-1)}+x_{i}^{(k)}-x^{(k)}, i=1,...,B
\end{align}\]
</blockquote>

<p>추가적으로 \(\overline{x}=\frac{1}{B}\sum_{i=1}^{B}x_{i}, \overline{w}=\frac{1}{B}\sum_{i=1}^{B}w_{i}\)로 둘 수 있다. 이렇게 되면, \(k&gt;1\)인 iteration에서 \(\overline{w}^{(k)}=0\)임을 쉽게 확인할 수 있고, ADMM update의 두번째 식은 \(x^{(k)}=\overline{x}^{(k)}\)으로 정리된다. 따라서 ADMM update식을 아래와 같이 정리할 수 있다.</p>

<blockquote>
\[\begin{align}
x^{(k)}_{i} &amp;= \underset{x_{i}}{\operatorname{argmin}} f_{i}(x_{i})+\frac{\rho}{2}||x_{i}-\overline{x}^{(k-1)}+w_{i}^{(k-1)}||_{2}^{2},  i=1,...B\\\\
w_{i}^{(k)} &amp;=w_{i}^{(k-1)}+x_{i}^{(k)}-\overline{x}^{(k)},  i=1,...,B.
\end{align}\]
</blockquote>

<p>\(i = 1,...B\)에 대한 \(x_{i}\) update는 병렬적으로 계산될 수 있다.
정리된 식을 통하여 consensus ADMM에 대한 직관을 얻을 수 있다. 각  \(x_{i}\) update에서는 \(f_{i}(x_{i})\)를 최소화 하려 하고, 동시에 \(l_{2} regularization\)으로 각 \(x_{i}\)를 평균인 \(\overline{x}\)에 맞추어 간다. 만약 \(x_{i}\)가 평균보다 커지면, \(w_{i}\)는 증가한다. 따라서 다음 step에서의 regularization이 커진 \(x_{i}\)를 낮추게 된다.</p>

<h2 id="general-consensus-admm">General consensus ADMM</h2>
<p>Consensus ADMM은 더 일반화된 형태로 만들어질 수 있다. x에 대하여 affine transformation과 임의의 함수 \(g\)가 적용된 문제의 형태를 살펴보자.</p>

<blockquote>
\[\begin{align}
\min_{x}\sum_{i=1}^{B} f_{i}(a^{T}_{i}x+b_{i})+g(x)
\end{align}\]
</blockquote>

<p>이 식에 대해서도, constraint를 추가하기 위하여 reparameterize한다.</p>
<blockquote>
\[\begin{align}
&amp;\min_{x_{1},..x_{B},x} &amp;&amp;\sum^{B}_{i=1}f_{i}(a_{i}^{T}x+b)+g(x)\\\\
&amp;\text{subject to } &amp;&amp;x_{i} = x, i=1,...B
\end{align}\]
</blockquote>

<p>이어서 분해가능한 ADMM update를 유도할 수 있다.</p>
<blockquote>
\[\begin{align}
x_{i}^{(k)} &amp;= \underset{x_{i}}{\operatorname{argmin}}f_{i}(a_{i}^{T}x+b_{i})+\frac{\rho}{2}||x_{i}-x^{(k-1)}+w_{i}^{(k-1)}||^{2}_{2}+g(x)\\\\
x^{(k)}&amp;=\underset{x}{\operatorname{argmin}} \frac{B\rho}{2}||x-\overline{x}^{(k)}-\overline{w}^{(k-1)}||^{2}_{2}+g(x)\\\\
w_{i}^{(k)}&amp;=w_{i}^{(k-1)}+x_{i}^{(k)}-x^{(k)}, i=1,...B
\end{align}\]
</blockquote>

<p>Generalized consensus ADMM과 위에서 유도했던 consensus ADMM과의 차이를 정리하면 다음과 같다.</p>

<ul>
  <li>ADMM step 식이 정리가 되지 않기 때문에, \(\overline{w}^{(k)}=0\)은 더이상 만족하지 않는다.</li>
  <li>\(x_{i}, i=1,...,B\)는 병렬하게 업데이트 가능하다.</li>
  <li>각각의 \(x_{i}\) 업데이트는 \(l2\) 정규화와 함께 해당 부분의 loss를 최소화하는 것으로 생각할 수 있다.</li>
  <li>\(x\) 업데이트는 임의의 함수 \(g\)(일반적으로 regularizer)에 대한 proximal operation이다.</li>
  <li>reparmeterization을 어떻게 하는가에 따라 ADMM 알고리즘이 다르게 도출된다.</li>
</ul>

<p>더 자세한 내용은 <a href="/contents/chapter21/2021/03/29/21_00_Alternating_Direction_Method_of_Multipliers/">참고문헌</a>을 참조한다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_6"></a>21-06 Faster convergence with subprogram parametrization - example of the 2d fused lasso problem</h1>
            <p>ADMM의 성질 중 무척 흥미로운 점은 문제 해결에 있어서 작은 문제(subproblems)들을 특별한 형태로 parametrize하면, 일반적인 방법보다 훨씬 빠른 수렴성능을 보여준다는 것이다. 앞선 consensus ADMM의 예시에서 update는 변수들의 block 모음에 대하여 최적화를 진행하는 형태를 보이는데, 이는 block coordinate descent와 유사하다. 따라서, ADMM 또한 각 변수들의 block 모음에 대하여 거의 orthogonal한 방향들로 업데이트하면서 빠른 수렴 속도를 보이게 할 수 있다.</p>

<p>이 절에서는 예시들을 통하여, 보조적인 constraint를 가장 primal update가 de-correlate하는 방향으로 설계함으로써 위의 내용들을 확인해보고자 한다.</p>

<p>자세한 내용은 [RT16], [WSK14], [BS14]를 참고한다.</p>

<p><a href="/contents/chapter01/2021/01/07/01_01_optimization_problems/">1장</a>에서 살펴보았던 예시중 하나인 2d fussed lasso 또는 2d total variation denoising 문제를 살펴본다. 이미지 \(Y\in \mathbb{R}^{d\times d}\)가 주어졌을때, 문제는 아래와 같이 정의된다.</p>

<blockquote>
\[\begin{align}
\min_{\Theta}\frac{1}{2}||Y-\Theta||^{2}_{F}+\lambda \sum_{i,j}(|\Theta_{i,j}-\Theta_{i+1,j}|+|\Theta_{i,j}-\Theta_{\Theta_{i,j+1}}|).
\end{align}\]
</blockquote>

<p>이 문제에서 이미지의 각 pixel에 대한 parameter가 있으며, 이 parameter 행렬은 \(\Theta\in \mathbb{d\times d}\)이다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter21/2dfussed.png" alt="[Fig 1] Interpretation of the penalty term in 2d fussed lasso[3]" width="70%" />
  <figcaption style="text-align: center;">[Fig 1] Interpretation of the penalty term in 2d fussed lasso[3]</figcaption>
</p>
</figure>

<p>[Fig 1]은 목적함수의 두번째 항인 penalty 항을 시각적으로 보여준다. 정의된 문제에서도 알 수 있다시피, 한 픽셀에 대하여 인접한 수평한 픽셀, 수직한 픽셀 간의 차이를 줄이고자 한다. 즉, 이 penalty 항은 주변의 인접한 픽셀들간의 값을 유사한 값으로 만든다.</p>

<p>Penalty 항의 합을 operator로 정리하면 문제는 아래와 같아진다.</p>
<blockquote>
\[\begin{align}
\min_{\theta}\frac{1}{2}||y-\theta||^{2}_{F} + \lambda||D\theta||_{1}.
\end{align}\]
</blockquote>

<p>\(D\in \mathbb{m\times n}\)은 원 식에 대응되는 2d difference operator이다.</p>

<h2 id="forms-of-admm-updates-for-the-2d-fused-lasso-problem">Forms of ADMM updates for the 2d fused lasso problem</h2>
<p>이제 보조적인 contraint를 적용하여 ADMM step을 두가지 방법으로 만들어보고자 한다.
첫번째로는 2d difference operator를 통하여 만들었던 vector form으로 부터 ADMM을 유도하는 것이다.</p>

<blockquote>
\[\begin{align}
\min_{\theta, z}\frac{1}{2}||y-\theta||^{2}_{2}+\lambda||z||_{1} \qquad \text{subject to   }z = D\theta,
\end{align}\]
</blockquote>

<p>이어서 ADMM step을 유도하면 다음과 같다.</p>
<blockquote>
\[\begin{align}
\theta^{(k)} &amp;= (I+\rho D^{T}D)^{-1}(y+\rho D^{T}(z^{(k-1)}+w^{(k-1)}))\\\\
z^{(k)} &amp;= S_{\frac{\lambda}{\rho}}(D\theta^{(k)}-w^{(k-1)})\\\\
w^{(k)} &amp;= w^{(k-1)}+z^{(k-1)}-D\theta ^{(k)}.
\end{align}\]
</blockquote>

<p>\(\theta\)는 \((I+\rho D^{T}D)^{-1}\)의 linear system을 푸는 것과 같다. 여기서 \(D^{T}D\)는 \(L=D^{T}D\)로 2d grid의 Laplacian행렬이 되어 \(O(n)\)의 연산으로 해결할 수 있다. \(z\) 또한 soft thresholding operator \(S_{t}\)로 연산이 이루어지므로, 동일하게 \(O(n)\)의 연산이 필요하다. 따라서 vector 형태로 ADMM을 푸는 것은 \(O(n)\)의 시간이 걸린다.</p>

<p>두번째 방법으로는 맨 처음의 문제 정의와 동일하게 행렬 형태로 ADMM을 유도하는 것이다.</p>
<blockquote>
\[\begin{align}
&amp;\min_{\Theta, Z} &amp;&amp;\frac{1}{2}||Y-\Theta||^{2}_{F}+\lambda\sum_{i,j}(|\Theta_{i,j}-\Theta_{i+1,j}+|Z_{i+1,j}-Z_{i,j+1}|)\\\\
&amp;\text{subject to } &amp;&amp;\Theta = Z
\end{align}\]
</blockquote>

<p>ADMM steps는 아래와 같다.</p>
<blockquote>
  <p>\(\begin{align}
\Theta_{\cdot \\ , j}^{(k)} &amp;= FL^{1d}_{ \frac{\lambda}{(1+\rho)} } \bigg( \frac{ Y+\rho( Z^{(k-1)}_{\cdot \\ , j}-W_{\cdot \\ ,j}^{(k-1)} ) } {1+\rho} \bigg),\qquad j=1,...,d\\\\
Z_{i, \cdot}^{(k)} &amp;= FL^{1d}_{\frac{\lambda}{\rho}} \bigg(\Theta_{i, \cdot}^{(k)} + W_{i, \cdot}^{(k-1)} \bigg), \qquad j=1,...,d\\\\
W^{(k)} &amp;= W^{(k-1)} + \Theta^{(k)} - Z^{(k)} \\\\
\end{align}\)
여기서 \(FL_{\tau}^{1d}(a)\)는 1d fused lasso이고,  \(FL_{\tau}^{1d}(a) = \underset{x}{\operatorname{argmin}}\frac{1}{2}||a-x||^{2}_{2}+\tau\sum_{i=1}^{d-1}|x_{i}-x_{i+1}|\) 이다.
 행렬 형태의 ADMM 또한 \(O(n)\)의 시간복잡도로 연산을 수행할 수 있다. \(\Theta, Z\) 둘 다 1d fused lasso의 형태이고, 이는 \(O(n)\)의 시간복잡도를 가진다. 
 [Fig 2]는 기존의 penalty 항을 1d fused lasso 문제로 어떻게 분리되는가를 보여준다.</p>
</blockquote>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter21/2dfussedlasso.png" alt="[Fig 2]  Interpretation of the matrix form ADMM updates for 2d fused lasso[3]" width="70%" />
  <figcaption style="text-align: center;">[Fig 2]  Interpretation of the matrix form ADMM updates for 2d fused lasso[3]</figcaption>
</p>
</figure>

<h2 id="image-denoising-experiments">Image denoising experiments</h2>
<p>이제 1장에서 살펴보았던 image denoising 문제를 다시 살펴본다.
[Fig 3]는 data와 denoised된 image를 보여준다. [Fig 4]는 두 ADMM 방법에 대한 비교를 보여준다. vertical/horizontal 방향으로 decompose하여 constraint를 정의하는 matrix form인 “specialized” ADMM은 vector form에서 유도한 “standard ADMM”보다 훨씬 빠른 수렴 성능을 보여준다.
[Fig 5]는 ADMM의 iteration에 따른 image quality를 보여준다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter21/ll1.png" alt="[Fig 3]  Data, exact solution image(300x200 image : n = 60,000).  
left : original image before denoising, right : the exact solution of denoised image[3]" width="70%" />
  <figcaption style="text-align: center;">[Fig 3]  Data, exact solution image(300x200 image : n = 60,000).
left : original image before denoising, right : the exact solution of denoised image[3]</figcaption>
</p>
</figure>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter21/ll2.png" alt="[Fig 4]  Convergence curves of two ADMM algorithms. black : standard(vector form), red : specialized(matrix form) [3]" width="70%" />
  <figcaption style="text-align: center;">[Fig 4]  Convergence curves of two ADMM algorithms. black : standard(vector form), red : specialized(matrix form) [3]</figcaption>
</p>
</figure>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter21/admm_iteration_visualized.png" alt="[Fig 5]  ADMM iterates visualized after k = 10, 30, 50, 100 iterations [3]" width="70%" />
  <figcaption style="text-align: center;">[Fig 5]  ADMM iterates visualized after k = 10, 30, 50, 100 iterations [3]</figcaption>
</p>
</figure>

        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
