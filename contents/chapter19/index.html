<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      Proximal Netwon Method &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/convex-optimization/public/css/poole.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/syntax.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/lanyon.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/logo.png">
  <link rel="shortcut icon" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://simonseo.github.io/convex-optimization/convex-optimization/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/convex-optimization/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>19. Proximal Netwon Method</h1>






<!-- Get first post and show it -->

<p>이번 장에서는 <strong>Proximal Newton Method</strong>과 <strong>Proximal quasi-Newton method</strong>, <strong>Projected Newton method</strong>에 대해서 살펴보도록 하겠다.</p>

<h4 id="참고-논문">참고 논문</h4>

<p>Proximal Newton method:</p>

<ul>
  <li>J. Friedman and T. Hastie and R. Tibshirani (2009), “Regularization paths for generalized linear models via coordinate descent”</li>
  <li>C.J. Hsiesh and M.A. Sustik and I. Dhillon and P. Ravikumar (2011), “Sparse inverse covariance matrix estimation using quadratic approximation”</li>
  <li>M. Patriksson (1998), “Cost approximation: a unified framework of descent algorithms for nonlinear programs”</li>
  <li>J. Lee and Y. Sun and M. Saunders (2014), “Proximal Newton-type methods for minimizing composite functions”</li>
  <li>P. Tseng and S. Yun (2009), “A coordinate gradient descent method for nonsmooth separable minimization”</li>
</ul>

<p>Projected Newton method:</p>

<ul>
  <li>A. Barbero and S. Sra (2011), “Fast Newton-type methods for total variation regularization”</li>
  <li>D. Bertsekas (1982), “Projected Newton methods for optimization problems with simple constraints”</li>
  <li>D. Kim and S. Sra. and I. Dhillon (2010), “Tackling box-constrained optimization via a new projected
quasi-Newton approach”</li>
  <li>M. Schmidt and D. Kim and S. Sra (2011), “Projected Newton-type methods in machine learning”</li>
</ul>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">19-01 Proximal Newton method</a>
    </li>
  
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_2"> 19-01-01 Reminder - proximal gradient descent</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_3"> 19-01-02 Proximal Newton method</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_4"> 19-01-03 Scaled proximal map</a>
    </li>
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_5">19-02 Backtracking line search</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_6">19-03 When would we use proximal Newton?</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_7">19-04 Convergence analysis</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_8">19-05 Notable examples</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_9">19-06 Proximal quasi-Newton methods</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_10">19-07 Projected Newton method</a>
    </li>
  
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>19-01 Proximal Newton method</h1>
            <p>이 절에서는 <strong>proximal gradient method</strong>를 복습해보고 <strong>proximal newton method</strong>가 이로부터 어떻게 나오게 되었는지를 살펴볼 것이다.</p>

<p>또한, <strong>proximal newton method</strong>의 정의와 일반적인 proxmal map에 비해 uniqueness와 non-expansiveness과 같은 좋은 성질을 갖는 scaled proximal map에 대해서도 살펴볼 것이다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>19-01-01 Reminder - proximal gradient descent</h1>
            <p>이번 장에서 배울 <strong>Proximal newton method</strong>의 살펴보기 전에 먼저 <strong>Proximal gradient descent</strong>를 복습해 보자.</p>

<p>자세한 내용은 <a href="/convex-optimization/contents/chapter09/2020/01/08/09_proximal_gradient_descent_and_acceleration/">09 Proximal Gradient Descent and Acceleration</a> 참조.</p>

<h2 id="proximal-gradient-descent">Proximal gradient descent</h2>
<p><strong>Proximal gradient descent</strong> 다음의 문제에 대해 작동한다.</p>

<blockquote>
\[f(x) = g(x) + h(x)\]
</blockquote>

<ul>
  <li>\(g\)는 convex이고 differentiable하다. (<strong>dom</strong>\((g) = \mathbb{R}^n\))</li>
  <li>\(h\)는 convex이고 non-differentiable하며 “simple”하다.</li>
</ul>

<h4 id="algorithm">Algorithm</h4>
<p>Proximal gradient descent는 시작점 \(x^{(0)}\)에서 시작해서 다음 과정을 반복한다.</p>

<blockquote>
\[x^{(k)} = \text{prox}_{t_k}(x^{(k-1)} - t_k \nabla g(x^{(k-1)}) ),k=1,2,3,...\]
</blockquote>

<p>여기서 \(\text{prox}_{t}(\cdot)\)는 \(h\)와 연관된 proximal operator 이다.</p>

<blockquote>
  <p>\begin{align}
\text{prox}_{t}(x) = \underset{z}{\text{argmin}}  \frac{1}{2t} \parallel x - z \parallel_2^2 + h(z)
\end{align}</p>
</blockquote>

<p>Update 식은 generalized gradient \(G_{t}\)를 사용해서 표준화된 형태로 표현할 수도 있다.</p>

<blockquote>
  <p>\begin{align}
x^{(k)} = x^{(k-1)} - t_k \cdot G_{t_k}(x^{(k-1)}), \space \space \text{where} \space G_{t}(x) = \frac{x-\text{prox}_{t} (x - t \nabla g(x))}{t} \<br />
\end{align}</p>
</blockquote>

<h4 id="performance">Performance</h4>
<ul>
  <li>
    <p><strong>Proximal gradient descent</strong>의 성능은 \(h\)에 따라 달라질 수 있다. 만일, \(h\)가 복잡한 함수이고 특히 closed form이 아니라면 minimize할 때 계산을 많이 해야 하므로 성능이 매우 떨어질 수 있다.</p>
  </li>
  <li>
    <p>또한, \(g\)함수의 convergence rate와 같은 수렴 속도를 갖는다. 단, 반복할 때마다 prox operator를 실행하기 때문에 prox 계산이 효율적인 경우에만 유용하다.</p>
  </li>
</ul>

<h2 id="motivation">Motivation</h2>
<p><strong>Proximal gradient descent</strong>에서는 미분 가능한 함수 \(g\)를 Tayor 2차식으로 근사하고 여기에 미분이 되지 않는 함수인 \(h\)를 더하여 목적 함수로 정의한 후 이를 반복적으로 최소화한다. 따라서, 다음과 같이 2차 식으로 정리해 볼 수 있다.</p>

<p>식에 전개되는 자세한 과정은 <a href="/convex-optimization/contents/chapter09/2020/01/08/09_01_proximal_gradient_descent/">09-01 Proximal gradient descent</a> 참고.</p>

<blockquote>
\[\begin{align}
x^+ &amp; = \underset{z}{\text{argmin}}  \, \frac{1}{2t} \parallel x - t \nabla g(x) - z \parallel_2 ^2 + h(z) \\\\
&amp; = \underset{z}{\text{argmin}} \ \nabla g(x)^T (z - x) + \frac{1}{2t} \parallel z - x \parallel_2 ^2 + h(z) \\\\
\end{align}\]
</blockquote>

<p>두번째 식의 1항과 2항은 \(g\)의 Tayor 2차 근사식으로 부터 유도할 수 있는데, 먼저 상수항 \(g(x)\)은 제거하고 (gradient descent에서와 마찬가지로) Hessian \(\nabla^2 g(x)\)을 \(\frac{1}{t} I\)(spherical curvature)로 대체해서 구할 수 있다.</p>

<p>다음 그림에서는 proximal gradient descent의 update 단계에서 \(g\)를 2차 근사식으로 최소화 하는 과정을 보여주고 있다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter19/09.01_01_proximal_gradient_descent.PNG" alt="[Fig 1] Proximal gradient descent updates [3]" width="70%" />
  <figcaption style="text-align: center;">[Fig 1] Proximal gradient descent updates [3]</figcaption>
</p>
</figure>

<p>Gradient descent와 newton’s method의 차이점는 2차 근사를 할 때 함수의 local hessian인 \(\nabla^2 g(x)\)를 사용하는지 여부이다. 그렇다면, 위의 식에서 \(\frac{1}{t} I\) 대신에 \(\nabla^2 g(x)\)를 사용하면 어떻게 될까?</p>

<p>이것이 바로 다음 절에서 설명하게 될 <strong>proximal newton method</strong>가 나오게 된 배경이다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>19-01-02 Proximal Newton method</h1>
            <p>이전 절에서 <strong>proximal newton method</strong>는 <strong>proximal gradient descent</strong> 식에서 spherical curvature인 \(\frac{1}{t} I\) 대신에 local hessian인 \(\nabla^2 g(x)\)를 사용하고자 하는 방법임을 설명했다. Proximal newton method는 오래 전에 나온 아이디어로 통계학에서는 local score란 용어로 연구되고 있다.</p>

<p>이제 <strong>proximal newton method</strong>가 어떻게 formulation될 수 있는지 살펴보자.</p>

<h2 id="algorithm">Algorithm</h2>
<p>Proximal gradient descent 알고리즘은 다음 step의 direction인 \(v\)를 구한 후 step size인 \(t_k\)를 optimization하는 과정으로 이루어져 있다.</p>

<ul>
  <li>
    <p>1단계 : 시작점 \(x^{(0)}\)에서 시작해서 다음 과정을 반복한다. (\(k=1,2,3,...\))</p>
  </li>
  <li>
    <p>2단계 : 다음 step의 direction인 \(v\)를 구한다.</p>
  </li>
</ul>

<blockquote>
  <p>\begin{align}
v^{(k)} &amp; = \underset{v}{\text{argmin}} \ \nabla g(x^{(k-1)})^T v + \frac{1}{2} v^T H^{(k-1)} v + h(x^{(k-1)} + v)
\end{align}
여기서 \(H^{(k-1)} = \nabla^2 g(x^{(k-1)})\)은 \(x^{(k-1)}\)에서의 Hessian이다.</p>
</blockquote>

<ul>
  <li>3단계 : \(v^{(k)}\) 방향으로 step을 이동하기 위해 step size를 optimization한다.</li>
</ul>

<blockquote>
  <p>\begin{align}
x^{(k)} &amp; =x^{(k-1)} + t_k v^{(k)}
\end{align}</p>
</blockquote>

<p>\(t_k\)는 step size로 \(t_k=1\)이면 pure proximal Newton method이다.</p>

<p>Backtracking line search를 통해 step size를 optimization하는 과정이 있다는 점은 proximal gradient descent method와 다른 점이다.</p>

<h4 id="next-position-view">Next position view</h4>
<p>위의 식을 direction \(v\)이 아닌 다음 위치인 \(z\)의 관점에서 표현하면 다음과 같다.</p>

<blockquote>
\[\begin{align}
z^{(k)} &amp; = \underset{z}{\text{argmin}} \ \nabla g(x^{(k-1)})^T (z - x^{(k-1)})^T + \frac{1}{2} (z - x^{(k-1)})^T H^{(k-1)} (z - x^{(k-1)}) + h(z) \\\\
x^{(k)} &amp; =x^{(k-1)} + t_k (z^{(k)} - x^{(k-1)} )
\end{align}\]
</blockquote>

<p>직관적으로 첫번째 단계에서 목적 함수를 최소화 하는 surrogate point인 \(z\)를 구한다. 그런 다음, \(x^{(k-1)}\)에서 \(z\)의 방향으로 이동하지만 항상 \(z\)로 이동하게 되는 것은 아니다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>19-01-03 Scaled proximal map</h1>
            <p><strong>Proximal newton method</strong>를 <strong>proximal gradient descent</strong>와 같은 형식으로 다시 작성해 볼 수 있다.</p>

<h2 id="scaled-proximal-map">Scaled proximal map</h2>
<p>만일 \(H \succ 0\)라고 하면 <strong>scaled proximal map</strong>은 다음과 같이 정의된다.</p>

<blockquote>
  <p>\begin{align}
\text{prox}_{t}(x) = \underset{z}{\text{argmin}}  \frac{1}{2} \parallel x - z \parallel_H^2 + h(z)
\end{align}</p>
</blockquote>

<p>여기서 \(\parallel x\parallel_H^2 = x^THx\)으로 \(H\text{-norm}\)이다.  \(H = \frac{1}{t} I\)일 때 일반적인 <strong>unscaled proximal map</strong>이 된다.</p>

<p>일반적으로 <strong>scaled proximal map</strong>는 보통의 prox보다 좋은 성질을 갖고 있다.</p>

<ul>
  <li><strong>uniqueness</strong> : 해가 하나만 존재하는 성질 (\(H \succ 0\)이므로 strictly convex optimization problem이기 때문에 만족된다.)</li>
  <li><strong>non-expansiveness</strong> :  팽창하지 않는 성질 (scaled proximal map이 non-expansive 성질을 갖는 projection operator의 일반화이기 때문에 만족된다.)</li>
</ul>

<h4 id="참고-projection-operator의-non-expansiveness">[참고] Projection operator의 non-expansiveness</h4>
<p>두 점 \(x\), \(y\)와 convex set \(C\)에 대한 projection operator \(P_c\)에 대해 non-expansiveness란 \(\parallel P_c(x) - P_c(y) \parallel_2 \le \parallel x - y \parallel_2\)를 만족한다는 것을 의미한다. 즉,  \(P_c\)는 Lipschitz-1을 만족하며 \(C\)가 convex일 경우에만 만족한다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter19/09.01_03_projection_operator.PNG" alt="[Fig 1] Projection onto a convex set C [3]" width="70%" />
  <figcaption style="text-align: center;">[Fig 1] Projection onto a convex set C [3]</figcaption>
</p>
</figure>

<h2 id="proximal-newton-update">Proximal newton update</h2>
<p><strong>Scaled proximal map</strong>을 이용해서 Proximal newton update를 다시 표현해보면 다음과 같다.</p>

<blockquote>
\[\begin{align}
z^{+} &amp; = \underset{z}{\text{argmin}} \nabla g(x)^T (z - x)^T v + \frac{1}{2} (z - x)^T H (z - x) + h(z) \\\\
&amp; =\underset{z}{\text{argmin}} \ \frac{1}{2} \parallel x - H^{-1} \nabla g(x) - z \parallel_H^2 + h(z)
\end{align}\]
</blockquote>

<p>다르게 표현하면 다음과 같다.</p>

<blockquote>
\[\begin{align}
z^{(k)} &amp; = \text{prox}_{H^{(k-1)}} ( x^{(k-1)} - (H^{(k-1)})^{-1} \nabla g (x^{(k-1)}) ) \\\\
x^{(k)} &amp; =x^{(k-1)} + t_k (z^{(k)} - x^{(k-1)} )
\end{align}\]
</blockquote>

<p>직관적으로 \(g\)에 대해서 newton step을 수행하고, \(H^{(k-1)}\)에 대해 scaled prox operator를 적용해서 그 방향으로 이동한다는 것을 의미한다.</p>

<p>이로부터 다음과 같은 사항을 알 수 있다.</p>

<ul>
  <li>\(h(z) = 0\)일때 proximal operator는 identity 함수가 되여 일반적인 Newton update가 된다.</li>
  <li>\(H^{(k+1)}\)를 \(\frac{1}{r_k} I\)로 대체하고 \(t_k = 1\)로 두면 step size \(r_k\)에 대해 proximal gradient update를 구할 수 있다.</li>
  <li>Prox의 어려움은 \(h\)뿐만 아니라 \(g\)의 hessian의 구조에 따라 달라진다. 예를 들어 \(H\)가 diagonal이거나 banded이면 dense한 \(H\)일 경우에 비해 문제가 매우 쉬워진다.</li>
</ul>

<p>따라서,  proximal Newton method는 proximal gradient descent와 Newton’s method를 둘 다 일반화한 것임을 알 수 있다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_5"></a>19-02 Backtracking line search</h1>
            <p><strong>Proximal newton method</strong>는 newton’s method와 같이 pure step size \(t_k=1, k=1,2,3, \cdots\)인 경우에 수렴하지 않을 수 있다. 따라서, backtracking line search를 통해 step size를 optimize해야 한다.</p>

<h2 id="backtracking-line-search-알고리즘">Backtracking line search 알고리즘</h2>

<ol>
  <li>파라미터를 초기화한다. (\(0 \lt \alpha \le 1/2, 0 \lt \beta \lt 1\))</li>
  <li>각 반복에서 \(v = \text{prox}_{H} ( x - H^{-1} \nabla g (x) ) - x\)로  Proximal newton direction을 계산한다.</li>
  <li>\(t=1\)로 초기화 한다.</li>
  <li>\(f(x + tv) \gt f(x) + \alpha t \nabla g(x)^T v + \alpha (h(x + tv) - h(x))\) 조건을 만족하면 \(t=\beta t\)로 줄인다. 이 조건이 만족되는 동안 단계4를 반복한다. (\(f = g + h\))</li>
  <li>Proximal newton update \(x^+ = x + tv\)를 실행한다.</li>
  <li>종료 조건을 만족하지 않으면 단계2로 간다.</li>
</ol>

<p>직관적으로 \(x\)에서 함수 \(f\)의 선형 근사를 \(\alpha\)배 내에 있는 위치로 direction \(v\)를 따라 이동하도록 step size \(t\)를 찾는다. 그리고, \(f\)에서 \(h\) 파트는 미분이 되지 않기 때문에 discrete derivative \(h(x + tv) - h(x)\)를 구했다.</p>

<h2 id="efficientcy-of-algorithm">Efficientcy of algorithm</h2>
<p>Backtracking line search를 수행하기 위한 방법들이 많이 있으며 여기서는 그 중 한 방법을 소개했다.</p>

<p>이 방법의 경우 \(v\)를 계산할 때 prox operator를 한번만 계산한다. Proximal gradient descent의 경우 inner loop에서 prox operator의 계산을 반복해야 했는데 이 점과 확연히 구분되는 특징이다. 따라서, 이 방법은 prox operator의 계산이 복잡할 경우 매우 효율적으로 backtracking line search를 할 수 있다.</p>

<h4 id="참고-method-별--backtracking-line-search">[참고] Method 별  backtracking line search</h4>
<ul>
  <li>Gradient descent <a href="/convex-optimization/contents/chapter06/2021/03/20/06_02_02_backtracking_line_search/">06-02-02 Backtracking line search</a></li>
  <li>Proximal gradient descent <a href="/convex-optimization/contents/chapter09/2020/01/08/09_02_convergence_analysis/">09-02 Convergence analysis</a></li>
  <li>Newton’s method <a href="/convex-optimization/contents/chapter14/2021/03/26/14_04_backtracking_line_search/">14-04 Backtracking line search</a></li>
</ul>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_6"></a>19-03 When would we use proximal Newton?</h1>
            <p>Proximal newton method는 언제 사용해야 좋은가?</p>

<p>Proximal newton method의 유용성을 이해하기 위해  다음 문제에 대해 proximal newton method와 proximal gradient descent를 비교해 보자.</p>

<p><strong>Problem</strong> : \(\min_x g(x) + h(x)\)</p>

<h2 id="proximal-gradient-descent-vs-proximal-newton">Proximal gradient descent vs. proximal newton</h2>

<table>
  <thead>
    <tr>
      <th><strong>Proximal gradient descent</strong></th>
      <th><strong>Proximal Newton</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\(\frac{1}{2} \parallel b - x \parallel_2^2 + h(x)\) 최소화</td>
      <td>\(b^T x + x^T A x + h(x)\) 최소화</td>
    </tr>
    <tr>
      <td>Prox operator가 대부분 closed form으로 정의됨</td>
      <td>Prox operator가 대부분 closed form으로 정의되지 않음</td>
    </tr>
    <tr>
      <td>반복이 저렴</td>
      <td>반복이 아주 비쌈 (newton method보다 비쌈)</td>
    </tr>
    <tr>
      <td>Gradient descent 수렴 속도 <br /> \(O(1/\epsilon)\)</td>
      <td>Newton’s method 수렴 속도 <br /> \(O(\log \log 1/\epsilon)\)</td>
    </tr>
  </tbody>
</table>

<p>두 방법은 비슷해 보이지만 실제 매우 다른 일을 한다.</p>

<p>따라서, proximal newton method는 아주 적은 반복을 기대할 수 있는 scaled prox operator(quadratic + \(h\))에 대한 빠른 inner optimizer를 가질 때 사용할 수 있다. \(h\)가 separable function일 때 inner optimizer로 가장 많이 사용되는 방법이 coordinate descent이다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_7"></a>19-04 Convergence analysis</h1>
            <p>Proximal newton method의 수렴을 분석하기 위해 Lee (2014) [1] 논문의 증명을 따를 것이다.</p>

<p>[1] J. Lee and Y. Sun and M. Saunders (2014), Proximal Newton-type methods for minimizing</p>

<p>수렴을 증명하기 위해 다음과 같이 가정한다.</p>

<ul>
  <li>\(f = g + h\), \(g\)와 \(h\)는 convex이고 \(g\)는 2차 differentiable (smooth)</li>
  <li>\(mI \preceq \nabla^2 g(x) \preceq LI\).</li>
  <li>\(\nabla^2 g(x)\) Lipshitz with constant \(M\)</li>
  <li>\(\text{prox}_H(\cdot)\)는 정확히 계산 가능</li>
</ul>

<p>위에 세가지 가정은 strictly convex라는 것을 의미하며 \(\text{prox}_H(\cdot)\)가 정확히 계산이 가능하다고 가정한 것은 실제 이렇게 되기가 쉽지 않기 때문이다.</p>

<h2 id="convergence-theorem">Convergence Theorem</h2>

<blockquote>
  <p><strong>Proximal newton method</strong>는 backtracking line search를 이용해서 global하게 수렴한다.
\begin{align}
\parallel x^{(k)} - x^{\star} \parallel_2 \le \frac{M}{2m} \parallel x^{(k-1)} - x^{\star} \parallel_2^2
\end{align}</p>
</blockquote>

<p>이것을 <strong>local quadratic convergence</strong>라고 한다. \(k \ge k_0\)이후에 \(f(x^{(k)}) - f^{\star} \le \epsilon\)을 만족하기 위해서는 \(O(\log \log (1/\epsilon))\)의 반복이 필요하다. 단, 각 반복에서 scaled prox를 사용한다.</p>

<h2 id="proof-sketch">Proof sketch</h2>
<p><strong>Global convergence</strong>를 보이기 위해서는 어떤 step에서도 다음과 같은 step size \(t\)에 대해 backtracking exit condition을 만족함을 보일 수 있다.</p>

<blockquote>
  <p>\begin{align}
t \le \min \left\{ 1, \frac{2m}{L} (1-\alpha) \right\} \<br />
\end{align}</p>
</blockquote>

<p>이 식으로 global minimum에 도달했을 때인  update direction이 0으로 수렴한다는 것을 보일 수 있다</p>

<p><strong>Local quadratic convergence</strong>를 보이기 위해 충분히 여러번 반복하게 되면 pure newton step \(t=1\)은 backtracking exit conditions을 만족하여 다음 식이 성립된다.</p>

<blockquote>
\[\begin{align}
\parallel x^{+} - x^{\star} \parallel_2 &amp; \le \frac{1}{\sqrt(m)} \parallel x^{+} - x^{\star} \parallel_H \\\\
&amp; =  \frac{1}{\sqrt(m)} \parallel \text{prox}_H(x - H^{-1} \nabla g(x) )  - \text{prox}_H(x^{\star} - H^{-1} \nabla g(x^{\star}) )  \parallel_H \\\\
&amp; \le \frac{M}{2m} \parallel x - x^{\star} \parallel_2^2 \\\\
\end{align}\]
</blockquote>

<p>이를 정리해 보면 다음과 같다.</p>

<blockquote>
  <p>\begin{align}
\parallel x^{+} - x^{\star} \parallel_2 \ \le \ \frac{1}{\sqrt(m)} \parallel x^{+} - x^{\star} \parallel_H \  \le \ \frac{M}{2m} \parallel x - x^{\star} \parallel_2^2
\end{align}</p>
</blockquote>

<ul>
  <li>
    <p>첫번째 부등식은 lowest eigenvalue bound에 대해 성립하며 등식은 \(x^+\) 정의와 global minimum \(x^{\star}\)에서 \(\text{prox}_H(\cdot)\)이 identity가 된다는 사실에 의해 성립한다.</p>
  </li>
  <li>
    <p>두번째 부등식은 proximal operator의 nonexpasiveness, Lipshitz assumption, largest eigenvalue bound에 의해 성립한다.</p>
  </li>
</ul>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_8"></a>19-05 Notable examples</h1>
            <h2 id="glmnet-and-quic">Glmnet and QUIC</h2>
<p>Proximal newton method의 매우 유명한 패키지가 두 가지가 있다.</p>

<ul>
  <li>
    <p><strong>glmnet</strong> (Friedman et al., 2009): \(l_1\) penalized generalized linear models에 대한 prox Newton를 구현한 패키지. Coordinate descent를 이용해서 inner problem을 푼다.</p>
  </li>
  <li>
    <p><strong>QUIC</strong>  (Hsiesh et al., 2011): graphical lasso problem에 대한 prox Newton을 구현한 패키지. Factorization trick을 사용하고 coordinate descent를 이용해서 inner problem을 푼다.</p>
  </li>
</ul>

<p>두 구현 패키지는 각자의 용도에 맞춰서 매우 광범위하게 사용되고 있으며 state-of-the-art라고 할 수 있다.</p>

<p>Proximal Newton method는  proximal gradient보다 \(g\)의 gradient을 덜 자주 계산한다. 따라서, 계산 비용이 커질수록 proximal newton이 유리하다. 또한, inner solver를 신중하게 선택할수록 좋은 성능을 얻을 수 있다.</p>

<h2 id="example-lasso-logistic-regression">Example: lasso logistic regression</h2>
<p>Lee et al. (2012)논문에서 제시된 예제를 살펴보자.</p>

<p>\(l_1\) regularized logistic regression에대해 다음 세가지 방법에 대해서 성능을 평가하였다.
1.FISTA : accelerated prox grad 2. spaRSA : spectral projected gradient method 3. PN  : proximal Newton</p>

<h4 id="dense-hessian-x-n5000-p6000-예시">Dense hessian X (n=5000, p=6000) 예시</h4>
<p>데이터 수 n = 5000, feature 개수 p = 6000인 dense feature matrix \(X\)를 갖는 문제에 대해 다음과 같은 성능을 보였다. Hessian이 dense하기 때문에 매우 challenging한 문제라고 할 수 있다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter19/09.05_Lasso_Example1.PNG" alt="[Fig 1] Dense hessian X (n=5000, p=6000) [2]" width="70%" />
  <figcaption style="text-align: center;">[Fig 1] Dense hessian X (n=5000, p=6000) [2]</figcaption>
</p>
</figure>

<p>오른쪽은 함수 호출 기준으로, 왼쪽은 시간 기준으로 평가한 것으로서, 함수 호출 기준으로 봤을 때가 PN의 성능이 매우 우세함을 알 수 있다.</p>

<p>여기서 비용은 \(g\)와 \(\nabla g\)를 계산하는 시간이 대부분이며 특히 \(\exp\)와 \(\log\)함수를 계산하는 시간이 많이 들었다.</p>

<h4 id="sparse-hessian-x-n542000-p47000-예시">Sparse hessian X (n=542,000, p=47,000) 예시</h4>

<p>다음의 경우는 \(X\)가 sparse하기 때문에 \(g\)와 \(\nabla g\)를 계산하는 시간이 덜 들었다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter19/09.05_Lasso_Example-sparse.PNG" alt="[Fig 2] Sparse hessian X (n=542,000, p=47,000) [2]" width="70%" />
  <figcaption style="text-align: center;">[Fig 2] Sparse hessian X (n=542,000, p=47,000) [2]</figcaption>
</p>
</figure>

<h2 id="inexact-prox-evaluations">Inexact prox evaluations</h2>
<p>Proximal Newton method에서 proximal operation을 계산할 때  prox operator가 closed form이 아니기 때문에 정확히 계산하지 못한다. 그럼에도 불구하고, 매우 높은 정확도를 갖는다면 매우 좋은 성질이 될 수 있다.</p>

<p>Lee (2014)에서는 global convergence와  local superlinear convergence를 보장하는 inner problem의 stopping rule을 제안했다.</p>

<h4 id="three-stopping-rules">Three stopping rules</h4>
<p>Graphical lasso estimation 문제에 inner optimizations을 위한 세 가지 stopping rules을 비교하였다. 이때, 데이터 개수는 n = 72이고 feature 개수는 p = 1255이다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter19/09.05_Inexact_prox.PNG" alt="[Fig 3] Three stopping rules [2]" width="70%" />
  <figcaption style="text-align: center;">[Fig 3] Three stopping rules [2]</figcaption>
</p>
</figure>

<p>세 가지 stopping rule은 adaptive, maxiter = 10, exact이다. Maxiter는 inner iteration을 최대 10번까지만 하는 방식이고 exact는 정확한 해를 구할 때까지 반복하는 방식이다.</p>

<p>Proximal newton method가 quadratic convergence를 만족하므로 exact는  quadratic convergence를 만족한다고 볼 수 있다. Maxiter=10의 경우 최대 10번의 inner iteration으로는 quadratic convergence를 만족하지 못하지만 adaptive의 경우 quadratic convergence를 만족하며 세 가지 방식 중 가장 빠르다.</p>

<h4 id="stopping-rule-of-usual-newton-method">Stopping rule of usual newton method</h4>
<p>일반적인 newton’s method에서는 inner problem은 \(x^{(k-1)}\)의 \(g\)에 대한 qudratic approximation인 \(\tilde{g}_{k-1}\)를 최소화한다. 그리고, \(\eta_k, k=1,2,3,...\)를 선택해서 다음 조건을 만족할 때 중지한다. (이를 forcing sequence라고 한다.)</p>

<blockquote>
  <p>\begin{align}
\parallel \nabla \tilde{g}_{k-1}(x^{(k)}) \parallel_2 &amp; \le \eta_k \parallel  \nabla g(x^{(k-1)})  \parallel_2 \<br />
\end{align}</p>
</blockquote>

<p>이 조건은 다음 위치에서의 gradient가 현재 위치에서의 gradient보다 \(\eta_k\)배 만큼 작다는 것을 의미한다. 이때, Quadratic approximation은 \(\tilde{g}_{k-1}(z) = \nabla g(x)^T (z-x) + \frac{1}{2t} \parallel  z - x \parallel_2^2\)이다.</p>

<h4 id="stopping-rule-of-proximal-gradient-method">Stopping rule of proximal gradient method</h4>
<p>Lee et al. (2012)에서는 proximal gradient에서는 gradient 대신에 generalized gradient를 사용하는 방식을 제안하였다.</p>

<blockquote>
\[\begin{align}
\parallel G_{\tilde{f}_{k-1}/M}(x^{(k)}) \parallel_2 &amp; \le \eta_k \parallel  G_{f/M}(x^{(k-1)})  \parallel_2
\end{align}\]
</blockquote>

<p>여기서 \(\tilde{f}_{k-1} = \tilde{g}_{k-1} + h\)이고 \(mI \preceq \nabla^2 g \preceq MI\)이다.</p>

<p>그리고, 다음과 같이 \(\eta_k\)를 설정하여 inexact proximal newton이 local superlinear rate를 갖는다는 것을 증명하였다.</p>

<blockquote>
\[\begin{align} 
\eta_k \le \min \left\{ \frac{m}{2},  \frac{\parallel  G_{\tilde{f}_{k-2}/M}(x^{(k-1)}) - G_{f/M}(x^{(k-1)})  \parallel_2}{\parallel  G_{f/M}(x^{(k-2)})  \parallel_2} \right\}
\end{align}\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_9"></a>19-06 Proximal quasi-Newton methods</h1>
            <p>문제가 커질수록 Hessian의 계산 비용이 매우 높아진다. <strong>Proximal quasi-Newton method</strong>는 각 step에서 Hessian \(H^{(k-1)} = \nabla^2 g(x^{(k-1)})\)를 계산하지 않는 방식으로 superlinear 혹은 linear convergence의 수렴 속도를 제공한다.</p>

<h2 id="proximal-quasi-newton-method">Proximal quasi-Newton method</h2>
<ul>
  <li>Lee (2014)는 Hessian을  BFGS-style로 update하는 방식을 제안했다. 이 방법은 매우 잘 실행되며 local superlinear convergence의 수렴 속도를 갖는다.</li>
  <li>Tseng and Yun (2009)은  Hessian을 blockwise로 근사하는 방식을 제안했다. 이 방법은 \(f = g + h\)에서 \(h\)가 일부 최적화 변수에 의존하는 부분으로 나뉠 수 있을 때만 작동한다. Hessian을 blockwise로 계산하면 계산이 매우 빨라진다. 이 방법은 linear convergence의 수렴 속도를 갖는다.</li>
</ul>

<p>Quasi-Newton은 Hessian 계산이 힘들때 뿐 아니라 Hessian이 singular이거나 near singular인 ill-condition에서도 유용하다.</p>

<h4 id="참고-논문">참고 논문</h4>
<ul>
  <li>J. Lee and Y. Sun and M. Saunders (2014), “Proximal Newton-type methods for minimizing composite functions”</li>
  <li>P. Tseng and S. Yun (2009), “A coordinate gradient descent method for nonsmooth separable minimization”</li>
</ul>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_10"></a>19-07 Projected Newton method</h1>
            <h2 id="whats-wrong-with-projected-newton">What’s wrong with projected Newton?</h2>
<p>\(h\)가 convex set \(C\)의 indicator function \(h = I_c(x)\)일 때 문제는 다음과 같이 정의될 수 있다.</p>

<blockquote>
\[\min_{x} \ g(x) \quad  \text{subject to}  \quad  x \in C\]
</blockquote>

<p>따라서,  \(h(x) = I_c(x)\)이면 proximal gradient descent는 <strong>projected gradient descent</strong>가 된다. 즉, projected gradient descent는 proximal gradient descent의 special case이다.</p>

<p>\(h(x) = I_c(x)\)일 때 proximal Newton의 경우엔 어떠한가? 이 경우 update 식은 다음과 같이 정의된다.</p>

<blockquote>
\[\begin{align}
z^{+} &amp; =\underset{z \in C}{\text{argmin}} \ \frac{1}{2} \parallel x - H^{-1} \nabla g(x) - z \parallel_H^2  \\\\
&amp;= \underset{z \in C}{\text{argmin}} \ \nabla g(x)^T (z - x) + \frac{1}{2} (z - x)^T H (z - x)  \\\\
\end{align}\]
</blockquote>

<p>\(H = I\)이면 \(x - \nabla g(x)\)를 set \(C\)에 projection한 결과가 되지만, 일반적인 \(H \neq I\)에 대해서는 projection이 아니다. (\(H = I\)이면 \(l_2\)-norm이 되기 때문에 H-norm이 아닌 \(l_2\)-norm 이었다면 projection이 되었을 것이다.) 
따라서, projected Newton method는 proximal Newton method의 special case가 아니다.</p>

<h2 id="projected-newton-for-box-constraints">Projected Newton for box constraints</h2>
<p>특별한 경우 box constraint를 갖는 문제에 대해 projected Newton를 적용할 수 있다. (Bertsekas, 1982; Kim et al., 2010; Schmidt et al., 2011).</p>

<p>문제가 다음과 같다고 하자.</p>

<blockquote>
\[\min_{x} \ g(x) \quad  \text{subject to}  \quad  l \le x \le u\]
</blockquote>

<p>Projected Newton method의 시작 점 \(x^{(0)}\)와 작은 상수 \(\epsilon \gt 0\)라고 할 때  다음 단계를 반복한다 (\(k = 1, 2, 3, ...\)).</p>

<ul>
  <li>단계1: Binding set을 정의한다.</li>
</ul>

<blockquote>
  <p>\begin{align}
B_{k-1} &amp; = \{ i : x_i^{(k-1)} \le l_i + \epsilon \quad \text{and} \quad  \nabla_i g(x^{(k-1)}) \gt 0 \} \quad  \cup \quad 
\{ i : x_i^{(k-1)} \ge u_i - \epsilon  \quad \text{and} \quad  \nabla_i g(x^{(k-1)}) \lt 0 \} 
\end{align}</p>
</blockquote>

<p>최적화 step에서 이 변수들을 box constraint의 경계로 밀어낸다. 이들을 점점 더 많이 밀어낼수록 목적 함수는 줄어든다.</p>

<ul>
  <li>단계2: Free set \(F_{k-1} = \left\{1,....,n \right\} \backslash B_{k-1}\)을 정의한다.</li>
  <li>단계3: Free variable을 따라서 Hessian의 주요 submatrix의 inverse를 정의한다.</li>
</ul>

<blockquote>
\[S^{(k-1)} = [(\nabla^2 g(x^{(k-1)}))_{F_{k-1}}]^{-1}\]
</blockquote>

<ul>
  <li>단계4: Fee variable을 따라 Newton step을 실행하고 projection을 한다.</li>
</ul>

<blockquote>
\[\begin{align}
x_{(k)} = P_{[l, u]} \left( x^{(k-1)} - t_k \begin{bmatrix} S^{(k-1)} &amp; 0 \\
0 &amp; I \end{bmatrix} 
\begin{bmatrix} \nabla F_{k-1} g(x^{(k-1)}) \\ \nabla B_{k-1} g(x^{(k-1)}) \end{bmatrix}
\right)
\end{align}\]
</blockquote>

<p>여기서 \(P_{[l,u]}\)는 \([l, u] = [l_1, u_1] \times \cdots [l_n, u_n]\)로의 projection이다.</p>

<p>행렬식을 보면 free varaible에 대해서는 Newton step을 실행하지만 binding variable의 경우 변하지 않는 것을 알 수 있다. 또한, projection은 box 범위 밖에 있는 점들에 대해서 각 좌표에 대해 적절한 \(l_i\) 또는 \(u_i\)를 지정해주는 간단한 작업이다.</p>

<p>이 방법은 문제가 매우 크고 (ex, 차원이 큰 경우) 대부분의 variable이 boundary 근처에 있어서 free set이 매우 작을 때 최적화를 하는 방법이다.</p>

<p>어떤 종류의 문제가 box constraint를 갖는가? 다음과 같이 이런 종류의 문제는 매우 많은 것으로 알려져 있다.</p>

<ul>
  <li>Nonnegative least squares</li>
  <li>Support vector machine dual</li>
  <li>Graphical lasso dual</li>
  <li>Fused lasso (total variation denoising) dual</li>
</ul>

<h2 id="convergence-properties">Convergence properties</h2>
<ul>
  <li>Bertsekas (1982)는 적절한 가정하에 projected Newton으로 유한번 반복을 하면 적절한 binding constraints를 찾을 수 있다는 것을 보였다. 그러면, free variable에 대해 Newton’s method와 같아진다.</li>
  <li>Bertsekas (1982)는 또한  superlinear convergence를 증명하였다.</li>
  <li>Kim et al. (2010), Schmidt et al. (2011)은 BFGS-style update를 사용한 projected quasi-Newton method를 제안했다.</li>
</ul>

        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/convex-optimization/public/js/script.js'></script>
  </body>
</html>
