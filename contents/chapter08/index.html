<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      Subgradient Method &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/convex-optimization/public/css/poole.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/syntax.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/lanyon.css">
  <link rel="stylesheet" href="/convex-optimization/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/logo.png">
  <link rel="shortcut icon" href="https://simonseo.github.io/convex-optimization/convex-optimization/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://simonseo.github.io/convex-optimization/convex-optimization/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://simonseo.github.io/convex-optimization/convex-optimization/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/convex-optimization/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>08. Subgradient Method</h1>






<!-- Get first post and show it -->

<p>본 장에서는 subgradient 개념을 이용하여 컨벡스 함수이지만 미분 가능하지 않은 함수에서도 적용할 수 있는 subgradient method를 살펴보고, subgradient method의 수렴성과 수렴도를 예시와 함께 살펴보도록 하겠다.</p>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">08-01 Subgradient Method</a>
    </li>
  
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_2"> 08-01-01 Step size choices</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_3"> 08-01-02 Basic Inequality</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_4"> 08-01-03 Convergence analysis</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_5"> 08-01-04 Convergence rate</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_6"> 08-01-05 Example: Regularized Logistic Regression</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_7"> 08-01-06 Polyak step sizes</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_8"> 08-01-07 Example: Intersection of sets</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_9"> 08-01-08 Projected Subgradient Method</a>
    </li>
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_10">08-02 Stochastic Subgradient Method</a>
    </li>
  
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_11"> 08-02-01 Stochastic Subgradient Method</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_12"> 08-02-02 Convergence of Stochastic Methods</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_13"> 08-02-03 Convergence Rate of Stochastic Method</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_14"> 08-02-04 Batch vs Stochastic Methods</a>
    </li>
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_15">08-03 Improving on the Subgradient Method</a>
    </li>
  
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>08-01 Subgradient Method</h1>
            <p>함수의 정의역은 \({R}^n\)이며 모든 구간에서 미분 가능하지 않은 컨벡스 함수 \(f\)가 있다고 가정하자.</p>

<p>Subgradient method는 gradient descent에서 gradient를 subgradient로 바꾼 형태로 정의된다. ( \(\nabla f(x^{(k-1)}) → g(x^{(k-1)})\))</p>

<blockquote>
\[x^{(k)} = x^{(k-1)} - t_k ⋅ g^{(k-1)}, \quad k = 1, 2, 3, . . .\]
</blockquote>

<p>여기서 \(g^{(k-1)} \in \partial f(x^{(k-1)})\), 즉 \(g^{(k-1)}\)는 \(x^{(k-1)}\) 지점에서 함수 \(f\)에 대한 임의의 subgradient이다.</p>

<h2 id="subgradient-method-not-subgradient-descent">Subgradient method (not subgradient “descent”)</h2>

<p>Subgradient method는 gradient descent와 다르게 항상 하강(descent)하지 않는다는 특징이 있다 (subgradient “descent”라 명명하지 않는 이유). 그러므로 subgradient method를 사용할 때는 모든 시행(iteration)에 대해 가장 좋은 결과를 파악하고 있어야 한다.</p>

<blockquote>
\[f(x_{best}^{(k)}) = \min_{i=0,...k} f(x^{(i)})\]
</blockquote>

<p>\(f(x^{(k)}_{best})\)는 subgradient method를 \(k\)번 시행하였을 때 함수 \(f\)가 반환하는 최솟값을 의미한다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>08-01-01 Step size choices</h1>
            <p>Subgradient method에서도 다양한 방법으로 <strong>step size</strong>를 선택할 수 있다.</p>

<p>그 중에서도 다음 2가지 방식을 자세히 살펴보도록 하자.</p>

<ul>
  <li><strong>Fixed step sizes</strong>: \(t_k = t\), where \(k = 1, 2, 3, ...\)</li>
  <li><strong>Diminishing step sizes</strong>: 아래의 조건을 충족하는 \(t_k\)</li>
</ul>

<blockquote>
  <p>\begin{align}
\sum_{k=1}^{\infty} t_k = \infty, \quad \sum_{k=1}^{\infty} t_k^{2} &lt; \infty
\end{align}</p>
</blockquote>

<h4 id="example-of-diminishing-step-sizes">Example of Diminishing step sizes</h4>

<blockquote>
\[\begin{align}
&amp; t_k = \frac{1}{k}, k = 1,2,3,... \\
&amp; \sum_{k=1}^{\infty}t_k = \infty \quad \text{(Harmonic  series)} \\
&amp; \sum_{k=1 }^{\infty}t^2_k ≈ 1.644934 &lt; \infty \quad \text{(Basel problem)} \\
\end{align}\]
</blockquote>

<p>Subgradient method에서 사용되는 step size는 gradient descent에서와는 달리 미리 설정 되어야 한다는 것이 특징이다. 다시 말하면 gradient descent의 backtracking line search처럼 subgradient method의 step size는 곡면의 특징에 맞게 바뀌지 않는다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>08-01-02 Basic Inequality</h1>
            <p>Subgradient method의 convergence theorem과 convergence rate는 다음과 같은 basic inequality를 활용해 증명할 수 있다.</p>

<h2 id="basic-inequality">Basic Inequality</h2>

<blockquote>
\[\begin{align}
f_{best}^{k} - f^* \quad \le \quad \frac{R^{2}+G^{2}\sum_{i=1}^{k}\alpha_{i}^{2}}{2\sum_{i=1}^{k}\alpha_{i}} 
\end{align}\]
</blockquote>

<h2 id="proof">Proof</h2>
<p>\(x^*\)가 함수 \(f\)의 optimal point라고 하면 다음과 같은 등식이 유도된다.</p>

<blockquote>
\[\begin{alignat}{1}
 \Vert x^{(k+1)}-x^* \Vert _2^{2} &amp; \quad = \quad  \Vert x^{(k)}-\alpha_k g^{(k)}-x^* \Vert _2^{2}  \\
                                   &amp; \quad = \quad  \Vert (x^{(k)}-x^*)-\alpha_k g^{(k)} \Vert _2^{2}  \\
                                   &amp; \quad = \quad  \Vert x^{(k)}-x^* \Vert _2^2 - 2 \alpha_k g^{(k)T}(x^{(k)}-x^*)+\alpha_k^2 \Vert g^{(k)} \Vert _2^2 \\
\end{alignat}\]
</blockquote>

<p>Subgradient의 정의에서 다음과 같은 부등식이 유도된다.</p>

<blockquote>
\[\begin{alignat}{1}
f(x^*) \ge f(x^{(k)}) + g^{(k)T}(x^*-x^{(k)}) &amp; \quad \Longleftrightarrow \quad f(x^*)-f(x^{(k)}) \ge  g^{(k)T}(x^*-x^{(k)}) \\
                     &amp; \quad  \Longleftrightarrow \quad f(x^{(k)} - f(x^*)) \le  g^{(k)T}(x^{(k)}-x^*) \\
                     &amp; \quad \Longleftrightarrow \quad -2\alpha_{k}(f(x^{(k)}) - f(x^*)) \ge  -2\alpha_{k}(g^{(k)T}(x^{(k)}-x^*)) \\
                     &amp; \quad \Longleftrightarrow \quad -2\alpha_{k}(g^{(k)T}(x^{(k)}-x^*)) \le -2\alpha_{k}(f(x^{(k)})-f(x^*)) \\
\end{alignat}\]
</blockquote>

<p>위의 등식과 부등식을 이용하여 아래의 부등식을 유도된다.</p>

<blockquote>
\[\begin{alignat}{1}
 \Vert x^{(k+1)}-x^* \Vert _2^{2}  &amp; \quad = \quad  \Vert x^{(k)}-x^* \Vert _2^{2}-2\alpha_k g^{(k)T}(x^{(k)}-x^*)+\alpha_k^{2} \Vert g^{(k)} \Vert _2^{2} \\
                    &amp; \quad \le \quad  \Vert x^{(k)}-x^* \Vert _2^{2}-2\alpha_k (f(x^{(k)})-f^*)+\alpha_k^{2} \Vert g^{(k)} \Vert _2^{2} \\
\end{alignat}\]
</blockquote>

<p>\(k=1,2,3...,n\)일때, 위 부등식에 의해 다음과 같은 관계가 성립한다.</p>

<blockquote>
\[\begin{alignat}{1}
 \Vert x^{(2)}-x^* \Vert _2^{2} &amp; \quad \le \quad  \Vert x^{(1)}-x^* \Vert _2^{2}-2\alpha_1(f(x^{(1)})-f^*)+\alpha_1^{2} \Vert g^{(1)} \Vert _2^{2} \\
 \Vert x^{(3)}-x^* \Vert _2^{2} &amp; \quad \le \quad  \Vert x^{(2)}-x^* \Vert _2^{2}-2\alpha_2(f(x^{(2)})-f^*)+\alpha_2^{2} \Vert g^{(2)} \Vert _2^{2} \\
 \Vert x^{(4)}-x^* \Vert _2^{2} &amp; \quad \le \quad  \Vert x^{(3)}-x^* \Vert _2^{2}-2\alpha_2(f(x^{(3)})-f^*)+\alpha_2^{2} \Vert g^{(3)} \Vert _2^{2} \\
&amp; \quad ... \quad \\
 \Vert x^{(n+1)}-x^* \Vert _2^{2} &amp; \quad \le \quad  \Vert x^{(n)}-x^* \Vert _2^{2}-2\alpha_2(f(x^{(n)})-f^*)+\alpha_2^{2} \Vert g^{(n)} \Vert _2^{2} \\
\end{alignat}\]
</blockquote>

<p>위의 관계를 이용하면 아래와 같은 재귀적인 전개가 가능하다.</p>

<blockquote>
\[\begin{alignat}{1}
 \Vert x^{(2)}-x^* \Vert_2^{2} &amp; \quad \le \quad  \Vert x^{(1)}-x^* \Vert_2^{2}-2\alpha_1(f^{(1)}-f^*)+\alpha_1^{2} \Vert g^{(1)} \Vert_2^{2} \\
 \Vert x^{(3)}-x^* \Vert_2^{2} &amp; \quad \le \quad  \Vert x^{(2)}-x^* \Vert_2^{2}-2\alpha_2(f^{(2)}-f^*)+\alpha_2^{2} \Vert g^{(2)} \Vert_2^{2} \\
&amp; \quad \le \quad ( \Vert x^{(1)}-x^* \Vert_2^{2}-2\alpha_1(f^{(1)}-f^*)+\alpha_1^{2} \Vert g^{(1)} \Vert_2^{2})-2\alpha_2(f^{(2)}-f^*)+\alpha_2^{2} \Vert g^{(2)} \Vert_2^{2} \\
&amp; \quad = \quad  \Vert x^{(1)}-x^* \Vert_2^{2}-2\alpha_1(f^{(1)}-f^*)-2\alpha_2(f^{(2)}-f^*)+\alpha_1^{2} \Vert g^{(1)} \Vert_2^{2}+\alpha_2^{2} \Vert g^{(2)} \Vert_2^{2} \\
&amp; \quad = \quad  \Vert x^{(1)}-x^* \Vert_2^{2} -2\sum_{i=1}^{2}\alpha_i(f(x^{(i)})-f^*) + \sum_{i=1}^{2}\alpha_i^{2} \Vert g^{(i)} \Vert_2^{2} \\
&amp; \quad ... \quad &amp; \\
 \Vert x^{(k)}-x^* \Vert_2^{2}, &amp; \quad k=4,...,n+1 \text{도 위와 같이 전개된다.}
\end{alignat}\]
</blockquote>

<p>위를 일반화 해보자.</p>

<blockquote>
\[\begin{alignat}{1}
 \Vert x^{(k+1)}-x^* \Vert_2^{2} \quad = \quad  \Vert x^{(1)}-x^* \Vert_2^{2} -2\sum_{i=1}^{k}\alpha_{i}(f(x^{(i)})-f^*)+\sum_{i=1}^{k}\alpha_{i}^{2} \Vert g^{(i)} \Vert_2^{2}
\end{alignat}\]
</blockquote>

<p>\(\Vert x^{(k+1)}-x^* \Vert _2^{2} \ge 0\)과 \(R \ge  \Vert x^{(1)}-x^* \Vert _2\)라 하면, 다음과 같이 부등식이 정리된다.</p>

<blockquote>
\[\begin{alignat}{2}
&amp;  \Vert x^{(k+1)}-x^* \Vert_2^{2} \quad \le \quad R^{2}-2\sum_{i=1}^{k}\alpha_i(f(x^{(i)})- f^{*})+\sum_{i=1}^{k}\alpha_i^{2} \Vert g^{(i)} \Vert_2^{2}\\
\Longleftrightarrow &amp; \quad 0 \quad \le \quad  \Vert x^{(k+1)}-x^* \Vert_2^{2} \quad \le \quad  R^{2}-2\sum_{i=1}^{k}\alpha_i(f(x^{(i)})-f^*)+\sum_{i=1}^{k}\alpha_i^{2} \Vert g^{(i)} \Vert_2^{2} \\
\Longleftrightarrow &amp; \quad 0 \quad \le \quad R^{2}-2\sum_{i=1}^{k}\alpha_i(f(x^{(i)})-f^*)+\sum_{i=1}^{k}\alpha_i^{2} \Vert g^{(i)} \Vert_2^{2} \\
 \Longleftrightarrow &amp; 2\sum_{i=1}^{k}\alpha_i(f(x^{(i)})-f^*) \quad \le \quad R^{2}+\sum_{i=1}^{k}\alpha_i^{2} \Vert g^{(i)} \Vert_2^{2}
\end{alignat}\]
</blockquote>

<p>이때 아래의 관계를 이용하여 부등식을 다시 한번 정리한다.</p>

<blockquote>
\[\begin{align}
\sum_{i=1}^{k}\alpha_{i}(f(x^{(i)})-f^*)  \quad \ge \quad (\sum_{i=1}^{k}\alpha_i)\min_{i=1,...,k}(f(x^{(i)})-f^*) = (\sum_{i=1}^{k}\alpha_i)(f(x_{best}^{(k)})-f^*)
\end{align}\]
</blockquote>

<p>정리된 부등식은 다음과 같다.</p>

<blockquote>
\[\begin{align}
\min_{i=1,..,k} f(x^{(i)})-f^* \quad = \quad f_{best}^{(k)} - f^* \le \frac{R^{2}+\sum_{i=1}^{(k)}\alpha_i^{2} \Vert g^{(i)} \Vert _2^2}{2\sum_{i=1}^{k}\alpha_i} 
\end{align}\]
</blockquote>

<p>\(f\)는 Lipschitz condition에 의해 \(\Vert g^{(k)} \Vert_2 \le G\)를 만족하므로 최종적으로 basic inequality가 유도된다.</p>

<blockquote>
\[\begin{align}
f_{best}^{k} - f^* \quad \le \quad \frac{R^{2}+G^{2}\sum_{i=1}^{k}\alpha_i^{2}}{2\sum_{i=1}^{k}\alpha_i} \\
\end{align}\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>08-01-03 Convergence analysis</h1>
            <p>Gradient descent에서는 \(\nabla f\)가 Lipschitz continous하다고 가정하였지만 subgradient method에서는 \(f\)가 Lipschitz continous하다고 가정한다. (Gradient descent의 convergence theorem <a href="/convex-optimization/contents/chapter06/2021/03/20/06_03_01_convergence_analysis_and_proof/">06-03-01</a> 절을 참조)</p>

<p>\(f\)는 convex이고 dom \(f = R^n\)이며 \(f\)가 Lipschitz condition을 만족한다고 하자.</p>

<blockquote>
  <p>\begin{align}
| f(x) - f(y) | \le G \lVert x - y \rVert_2 \text{ for all } x, y
\end{align}</p>
</blockquote>

<p>다음과 같은 가정이 주어지면 fixed step sizes와 diminishing step sizes의 convergence 공식은 각각 다음과 같다.</p>

<h2 id="convergence-theorem-for-fixed-step-sizes">Convergence theorem for fixed step sizes</h2>

<p>Fixed step sizes는 다음과 같은 수렴성을 가진다.</p>
<blockquote>
  <p>\begin{align}
\lim_{k→\infty} f(x^{(k)}_{best}) \le f^* + \frac{G^{2}t}{2}
\end{align}</p>
</blockquote>

<h2 id="convergence-theorem-for-diminishing-step-sizes">Convergence theorem for diminishing step sizes</h2>

<p>Diminishing step sizes method는 다음과 같은 수렴성을 가진다.</p>

<blockquote>
  <p>\begin{align}
\lim_{k→\infty}f(x^{(k)}_{best}) = f^*
\end{align}</p>
</blockquote>

<h2 id="proofs">Proofs</h2>

<p>Fixed step-sizes와 diminishing step-sizes의 증명은 각각 다음과 같다.</p>

<h2 id="proof-of-convergence-theorem-for-fixed-step-sizes">Proof of convergence theorem for fixed step sizes</h2>

<p>Fixed step size method는 \(\sum_{i=1}^{k}t_{i} = kt\)임을 이용하여 증명한다.</p>

<blockquote>
\[\begin{align}
&amp; f_{best}^{(k)} - f^* \le \frac{R^{2}+G^{2}\sum_{i=1}^{k}t_{i}^{2}}{2\sum_{i=1}^{k}t_{i}} = \frac{R^{2}+G^{2}k t^{2}}{2kt}  = \frac{R^{2}}{2tk} + \frac{G^{2}t}{2} \\
&amp; \lim_{k→\infty}(f^{(k)}_{best} - f^*) \le 0 + \frac{G^{2}t}{2} = \frac{G^{2}t}{2} \\
&amp; \lim_{k→\infty}(f^{(k)}_{best}) \le f^* + \frac{G^{2}t}{2}
\end{align}\]
</blockquote>

<h2 id="proof-of-convergence-theorem-for-diminishing-step-sizes">Proof of convergence theorem for diminishing step sizes</h2>

<p>Diminishing step sizes가 만족하는 아래의 성질 (1), (2)를 이용하여 basic inequality에서부터 증명한다.</p>

<blockquote>
\[\begin{align}
\text{(1)} \sum_{i=1}^{\infty} t_i = \infty, \quad \text{(2)}  \sum_{i=1}^{\infty} t_i^{2} = \beta &lt; \infty
\end{align}\]
</blockquote>

<blockquote>
\[\begin{align}
&amp; f_{best}^{(k)} - f^* \le \frac{R^{2}+G^{2}\sum_{i=1}^{k}t_{i}^{2}}{2\sum_{i=1}^{k}t_{i}} \\
&amp; \lim_{k→\infty}(f^{(k)}_{best} - f^* ) \le \frac{R^{2}+G^{2}\beta}{2\infty} = 0 \\
&amp; \lim_{k→\infty}(f^{(k)}_{best}) =  f^* \\
\end{align}\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_5"></a>08-01-04 Convergence rate</h1>
            <p>Convergence rate는 알고리즘이 \(\epsilon\) 오차 범위 내 suboptimal point에 도달하기까지 필요한 시행 수의 경향성을 <a href="https://en.wikipedia.org/wiki/Big_O_notation">Big-O 표기법</a>으로 나타낸 것이다. 예를들어 \(\epsilon\)이 \(10^{-2}\)이고 알고리즘의 convergence rate가 \(O(1/\epsilon)\)라면, \(\epsilon\)-suboptimal point까지 도달하는데 필요한 시행 횟수의 경향성은 대략 \(1/10^{-2}=10^2\)를 따른다.</p>

<p><a href="/convex-optimization/contents/chapter08/2020/03/29/08_01_02_basic_inequality/">08-01-02 Basic inequality</a>를 이용하여 fixed step sizes의 subgradient method에 대한 convergence rate를 구해보자.</p>

<blockquote>
  <p>\(Recall:\)
\begin{align}
f^{k}_{best} - f^{*} \quad \le \quad \frac{R^{2}}{2kt} + \frac{G^{2}t}{2}
\end{align}</p>
</blockquote>

<p>임의의 \(\epsilon\)이 \(\frac{R^{2}}{2kt} \le \frac{\epsilon}{2}\)와 \(\frac{G^{2}t}{2} \le \frac{\epsilon}{2}\)를 만족한다고 할 때 (\(\epsilon\)은 suboptimality gap, \(G\)는 Lipschitz constant, \(R\)은 알고리즘의 시작점과 optimal point간의 거리),  \(\frac{R^{2}}{2kt} + \frac{G^{2}t}{2} \le \epsilon\)이다. 만약 \(\frac{G^{2}t}{2} \le \frac{\epsilon}{2}\)이라면 \(t \le \frac{\epsilon}{G^{2}}\)이고 \(\frac{R^{2}}{2kt} \le \frac{\epsilon}{2}\)는 결국 \(\frac{R^2G^2}{\epsilon^2} \le k\)를 도출할 수 있다. 이는 시행 횟수가 최소 \(\frac{R^2G^2}{\epsilon^2}\) 이상이 되면 \(f^{k}_{best} - f^{*} \le \epsilon\)을 만족하게 된다는 의미이다.</p>

<p>이 알고리즘의 convergence rate는 \(O(1/\epsilon^2)\)이므로 이는 gradient descent method의 convergence rate인 \(O(1/\epsilon)\)보다 현저히 많은 시행 횟수가 필요로 한다는 것을 시사한다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_6"></a>08-01-05 Example: Regularized Logistic Regression</h1>
            <p>만약 \(i=1,...,n\)에 대해서 \((x_i,y_i) \in R^p ×\){\(0, 1\)}가 주어졌을때, logisitc regression loss는 다음과 같다.</p>
<blockquote>

  <p>\begin{align}
f(\beta) = \sum_{i=1}^n\big(-y_ix_i^T\beta + log(1+exp(x_i^T\beta))\big)
\end{align}</p>
</blockquote>

<p>이 함수는 linear 함수와 log-sum-exp 함수의 finite sum의 형태로서 미분 가능한 컨벡스 함수이다.</p>

<p>이때 우리가 \(\beta\)에 대한 regularization problem은 다음과 같이 정리된다.</p>

<blockquote>

  <p>\begin{align}
min_{\beta} \text{ } f(\beta) + \lambda ⋅ P(\beta)
\end{align}</p>
</blockquote>

<p>여기서 \(P(\beta)가  \Vert \beta \Vert _2^2\)(ridge penalty) 또는 \(\Vert \beta \Vert _1\)(lasso penalty)로 정의된다고 해보자.</p>

<p>Ridge penalty를 적용한 loss 함수는 여전히 미분 가능한 컨벡스 함수이지만 lasso penalty를 적용한 loss 함수는 미분 불가능한 컨벡스 함수가 된다.</p>

<p>이러한 두 loss 함수에 대해 gradient descent for ridge와 subgradient method for lasso를 적용하여 시행 횟수 \(k\)에 대한 objective function의 값을 출력해보면 두 방정식의 수렴 특징을 관찰할 수 있다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter08/08_01_grad_vs_subgrad.png" alt="grad_vs_subgrad" width="90%" height="90%" />
</p>
  <figcaption style="text-align: center;">[Fig 1] Gradient descent vs Subgradient method [3]</figcaption>
</figure>

<p>위 실험은 gradient descent가 subgradient method보다 수렴속도가 훨씬 빠르다는 것을 보여준다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_7"></a>08-01-06 Polyak step sizes</h1>
            <p><strong>Polyak step sizes</strong>는 optimal value가 알려져 있을때 step size를 설정하는 방법이다. 만약 \(f^*\)가 알려져 있을 때 다음과 같이 Polyak step sizes를 정의 할 수 있다.</p>

<h2 id="convergence-theorem-for-polyak-step-sizes">Convergence theorem for Polyak step-sizes</h2>
<blockquote>

\[\begin{align}
t_k = \{\frac{f^{(k-1)}-f^*}{ \Vert g^{(k-1)} \Vert_2^{2}}\}, \quad k = 1,2,3...
\end{align}\]
</blockquote>

<h2 id="proof-of-convergence-theorem-for-polyak-step-sizes">Proof of convergence theorem for Polyak step-sizes</h2>
<p>증명은 <a href="/convex-optimization/contents/chapter08/2020/03/29/08_01_02_basic_inequality/">basic inequality</a>의 유도과정에 이용된 부등식으로부터 증명할 수 있다.</p>

<blockquote>

\[\begin{align}
 \Vert x^{(k)}-x^* \Vert_2^{2}  \quad \le \quad  \Vert x^{(k-1)}-x^* \Vert_2^{2}-2t_k (f(x^{(k-1)})-f^*)+t_k^{2} \Vert g^{(k-1)} \Vert_2^{2} \\
\end{align}\]
</blockquote>

<p>위 부등식의 오른쪽 항을 \(t_k\)에 대해서 미분하여 0과 같게 하면 오른쪽 항을 최소화시키는 Polyak step size를 도출할 수 있다.</p>
<blockquote>

\[\begin{align}
&amp; \frac{\partial}{\partial t_k}  \Vert x^{(k-1)}-x^* \Vert_2^{2}-2t_k (f(x^{(k-1)})-f^*)+t_k^{2} \Vert g^{(k-1)} \Vert_2^{2} \quad = \quad 0 \\
 \Longleftrightarrow \quad &amp; -2(f(x^{(k-1)})-f^*)+2t_k \Vert g^{(k-1)} \Vert_2^{2}  \quad = \quad 0 \\
 \Longleftrightarrow \quad &amp; 2(f(x^{(k-1)})-f^*)  \quad = \quad 2t_k \Vert g^{(k-1)} \Vert_2^{2} \\
 \Longleftrightarrow \quad &amp; f(x^{(k-1)})-f^* \quad = \quad t_k \Vert g^{(k-1)} \Vert_2^{2} \\
 \Longleftrightarrow \quad &amp; t_k = \frac{f(x^{(k-1)})-f^*}{ \Vert g^{(k-1)} \Vert_2^{2}} \quad \text{(Polyak step size at k)}
\end{align}\]
</blockquote>

<p>Polyak step size의 convergence rate도 <a href="/convex-optimization/contents/chapter08/2020/03/29/08_01_02_basic_inequality/">basic inequality</a>에서 유도된 부등식으로부터 유도할 수 있다.</p>

<h2 id="congervence-rate-for-polyak-step-sizes">Congervence rate for Polyak step-sizes</h2>

<p><a href="/convex-optimization/contents/chapter08/2020/03/29/08_01_02_basic_inequality/">basic inequality</a>에서 유도된 부등식에 Polyak step size \(t_i\)를 대입해보자.</p>
<blockquote>

\[\begin{align}
&amp; 2\sum_{i=1}^{k}t_i(f(x^{(i)})-f^*) \le R^2 + \sum_{i=1}^kt_i^2 \Vert g^{(i)} \Vert_2^2 \\
 \Longleftrightarrow \quad &amp; 2\sum_{i=1}^{k}\frac{(f(x^{(i)})-f^*)^2}{ \Vert g^{(i)} \Vert_2^2} \le R^2 + \sum_{i=1}^k\frac{(f(x^{(i)})-f^*)^2}{ \Vert g^{(i)} \Vert_2^2} \\
 \Longleftrightarrow \quad &amp; \sum_{i=1}^{k}\frac{(f(x^{(i)})-f^*)^2}{ \Vert g^{(i)} \Vert_2^2} \le R^2 \\
\end{align}\]
</blockquote>

<p>Lipschitz condition \(\Vert g^{(i)} \Vert_2 \le G\)를 항상 만족한다고 가정하면, 위의 부등식은 아래와 같이 정리된다.</p>
<blockquote>

\[\begin{align}
&amp; \sum_{i=1}^{k}(f(x^{(i)})-f^*)^2 \le R^2G^2 \\
 \Longleftrightarrow \quad &amp; k ⋅ (f(x^{(i)})-f^*)^2 \le R^2G^2 \\
 \Longleftrightarrow \quad &amp; \sqrt{k} ⋅ (f(x^{(i)})-f^*) \le RG \\
 \Longleftrightarrow \quad &amp; (f(x^{(i)})-f^*) \le \frac{RG}{\sqrt{k}} \\
\end{align}\]
</blockquote>

<p>\(\frac{RG}{\sqrt{k}}=\epsilon\)이라 하면, \(k=\big(\frac{RG}{\epsilon}\big)^2\)이므로 \(\epsilon\)에 대한 suboptimal point에 도달하는 것이 보장되기 위해서는 \(\big(\frac{RG}{\epsilon}\big)^2\)만큼의 시행 횟수가 필요하다. 즉, convergence rate는 \(O(1/\epsilon^{2})\)으로 다른 subgradient method와 동일하다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_8"></a>08-01-07 Example: Intersection of sets</h1>
            <p>닫힌 컨벡스 집합(closed convex set)들의 교차점을 찾고 싶다고 하자.</p>

<p>우선, 임의의 점 \(x\)로 부터 집합 \(C_i\)까지의 거리를 나타내는 \(f_i(x)\)와 점 \(x\)에서 모든 집합 \(C_i, i=1,...,m\)에 대해 가장 먼 거리를 나타내는 \(f\)를 정의해보자.</p>
<blockquote>

\[\begin{align}
f_i(x) &amp; = \mathbb{dist}(x, C_i), i=1,...,m \\
f(x) &amp; = \max_{1,...,m}\text{ }f_i(x)
\end{align}\]
</blockquote>

<p>위의 두 함수를 이용하면 다음과 같이 컨벡스 집합들의 교차점을 찾는 최적화 문제로 정의할 수 있다.</p>

<blockquote>

\[\begin{align}
min_{x}\text{ }f(x)
\end{align}\]
</blockquote>

<p>컨벡스 집합의 교차점을 구하는 문제는 임의의 점 \(x\)와 가장 먼 컨벡스 집합 \(C_i\)의 거리 \(f_i(x)\)를 최소화하는 \(x\)를 구하는 문제로 바뀐다. 이때, 위 문제의 목적 함수인 \(f(x)\)는 컨벡스이다. 만약 모든 집합의 동시적인 교차점이 존재한다면 \(f^* = 0\)이 될 것이고 optimal point는 \(x^* \in C_1 \cap C_2 \cap ... \cap C_m\)로 표현할 수 있다.</p>

<h2 id="gradient-of-distance-function">Gradient of distance function</h2>

<p><a href="/convex-optimization/contents/chapter07/2021/03/25/07_03_05_example_distance_to_convex_set/">이전 장</a>에서 컨벡스 집합과의 거리를 \(dist(x, C_i) = \min_{y \in C} \lVert y-x \lVert _2\)로 정의했고 이 함수의 gradient는 다음과 같음을 보였다.</p>

<blockquote>

\[\begin{align}
\partial dist(x,C) = \frac{x-P_C(x)}{ \Vert x-P_C(x) \Vert_2}
\end{align}\]
</blockquote>

<p>여기서 \(P_C(x)\)는 점 \(x\)에서 집합 \(C\)으로의 projection이다.</p>

<h2 id="subdifferential-of-finite-pointwise-maximum">Subdifferential of finite pointwise maximum</h2>

<p>Finite pointwise maximum 함수 \(f(x)=max_{i=1,...,m}\text{ }f_i(x)\)에 대한 subdifferential은 다음과 같이 정의 된다.</p>

<blockquote>

\[\begin{align}
\partial f(x) = \text{conv}\left(\bigcup_{i:f_i(x)=f(x)} \partial f_i(x)\right)
\end{align}\]
</blockquote>

<p>즉, \(x\)의 subdifferential은 그 지점의 모든 subdifferential \(\partial f_i(x), i=1,...,m\)의 합집합에 대한 convex hull로 정의된다.</p>

<p>만약 \(f_i(x) = f(x)\) 이고 \(g_i \in \partial f_i(x)\)이라면 \(g_i \in \partial f(x)\)이다.</p>

<h2 id="deriving-subgradient-updating-steps">Deriving subgradient updating steps</h2>

<p><a href="/convex-optimization/contents/chapter07/2021/03/25/07_03_05_example_distance_to_convex_set/">이전 장</a>에서 보았던 \(dist(x, C_i)\)는 다음과 같은 subgradient를 가진다.</p>

<blockquote>
  <p>\(Recall:\)
\(\begin{align}
g_i = \nabla f_i(x) = \frac{x-P_{C_i}(x)}{ \Vert x-P_{C_i}(x) \Vert_2}
\end{align}\)</p>
</blockquote>

<p>만약 컨벡스 집합의 교차점이 있다면 우리는 \(f^*=0\)임을 바로 알 수 있기에 Polyak step sizes를 사용할 수 있다. 위 subgradient 수식을 보면 \(x-P_{c_i}(x)\)가 정규화된 형태이므로 \(\Vert g \Vert_2^{2}=1\)이다. 결국 Polyak step size \(t_k = \{\frac{f^{(k-1)}-f^*}{ \Vert g^{(k-1)} \Vert_2^{2}}\}\)에 우리가 알고 있는 값을 대입하면 다음과 같은 subgradient method 공식을 도출할 수 있다.</p>

<blockquote>

\[\begin{align}
x^{(k)} &amp; = x^{(k-1)} - t_{k}⋅g_{k-1} \\
&amp; = x^{(k-1)} - \frac{f^{(k-1)}-f^*}{ \Vert g^{(k-1)} \Vert_2^{2}} \frac{x^{(k-1)}-P_{C_i}(x)}{ \Vert x^{(k-1)}-P_{C_i}(x) \Vert_2}  \\
&amp; = x^{(k-1)} - f(x^{k-1}) \frac{x^{(k-1)}-P_{C_i}(x)}{ \Vert x^{(k-1)}-P_{C_i}(x) \Vert_2}
\end{align}\]
</blockquote>

<p>여기서 Polyak size인 \(f(x^{(k-1)})\)는 \(dist(x_i^{(k-1)}, C_i) =  \Vert x^{(k-1)}-P_{C_i}(x) \Vert_2\) 이므로 subgradient method는 아래와 같이 정리된다.</p>

<blockquote>

\[\begin{align}
x^{(k)} = P_{C_i}(x^{(k-1)})
\end{align}\]
</blockquote>

<p>이 문제는 그림으로 표현하면 각 스텝에서 가장 가까운 컨벡스 함수에 projection을 반복하는 형태이다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter08/08_01_projection.png" alt="projection" width="60%" height="60%" />
</p>
  <figcaption style="text-align: center;">[Fig 2] Alternating Projection Algorithm [10]</figcaption>
</figure>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_9"></a>08-01-08 Projected Subgradient Method</h1>
            <p><a href="/convex-optimization/contents/chapter08/2020/03/29/08_01_07_example_intersection_of_sets/">앞서 본 예제</a>의 방법을 projected subgradient method라고 한다. 이 알고리즘은 제약조건이 있는 convex problem에서 이용할 수 있다.</p>

<p>제약 조건을 만족하는 domain을 집합 \(C\)라고 할 때, 제약조건이 있는 컨벡스 문제는 다음과 같이 정의된다.</p>

<blockquote>

\[\begin{align}
\min_x \text{ }f(x) \quad \text{subject to } x \in C
\end{align}\]
</blockquote>

<p>Projected subgradient method를 사용하면 위와 같은 문제를 비교적 쉽게 풀 수 있다. Projected subgradient method는 일반적인 subgradient method과 동일하지만 각 시행 마다 집합 \(C\)로 결과 값을 projection 해주는 형태이다.</p>

<blockquote>

\[\begin{align}
x^{(k)} = P_c(x^{(k-1)} - t_k ⋅ g^{(k-1)}), \quad k = 1,2,3,...
\end{align}\]
</blockquote>

<p>만약 projection이 가능하다면 이 방법은 subgradient method와 동일한 수렴성과 수렴도를 가진다.</p>

<p>Projected subgradient method에서 주의할 점은 \(C\)가 단순한 형태의 컨벡스 집합 일지라도 \(P_c\) 연산이 어려우면 전체 문제 또한 풀기 어려워진다는 것이다. 일반적으로 다음과 같은 집합 \(C\)은 비교적 쉽게 projection할 수 있다:</p>

<ul>
  <li>
    <p>Affine images: {\(Ax=b : x \in R^{n}\)}</p>
  </li>
  <li>
    <p>Solution set of linear system: {\(x: Ax=b\)}</p>
  </li>
  <li>
    <p>Nonnegative orthat: \(R_+^{n} =\){\(x: x\ge 0\)}</p>
  </li>
  <li>
    <p>Some norm balls: {\(x: \lVert x \lVert _p \le 1\)} for \(p=1,2,\infty\)</p>
  </li>
  <li>
    <p>Some simple polyhedra and simple cones</p>
  </li>
</ul>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_10"></a>08-02 Stochastic Subgradient Method</h1>
            <p>Stochastic subgradient method는 앞서 보았던 stochastic gradient descent에서 gradient를 subgradient로 바꾼 것과 동일한 형태이다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_11"></a>08-02-01 Stochastic Subgradient Method</h1>
            <p>다음과 같이 함수의 합을 최소화하는 문제를 고려해보자.</p>

<blockquote>
\[\begin{equation}
\min_x \sum_{i=1}^m f_i(x)
\end{equation}\]
</blockquote>

<p>이 문제에 subgradient method를 적용하면 각 함수 \(f_i\)에 대해 subgradient를 구해서 합산을 해야 한다. (<a href="/convex-optimization/contents/chapter06/2021/03/20/06_05_stochastic_gradient_descent/">stochastic gradient descent</a>에서 도출한 방법과 동일)</p>

<p>정리하면 stochastic subgradient method는 다음과 같은 형태를 가진다.</p>

<blockquote>
\[\begin{align}
x^{(k)} = x^{(k-1)} - t_k ⋅ g_{i_k}^{(k-1)}, \quad k = 1, 2, 3, . . . 
\end{align}\]
</blockquote>

<p>여기서 \(i_k \in {1,...,m}\)는 \(k\)번째 시행에서 선택된 하나의 인덱스 값이며, 이는 뒷장에서 stochastic subgradient method의 convergence rate에서 살펴볼  cyclic method 또는 random method에 따라 다르게 결정된다. \(g_{i}^{(k-1)} \in \partial f_{i}(x^{k-1})\)이며 이 업데이트 방향은 모든 데이터를 사용하는 일반적인 <a href="/convex-optimization/contents/chapter08/2020/03/29/08_01_subgradient_method/">subgradient method</a> (batch subgradient method 또는 full batch subgradient method라고 부름)에서의 \(\sum_{i=1}^{m} g_i^{(k-1)}\)를 대체한다.</p>

<p>만약 각 \(f_i, i = 1,...,m\)이 모두 미분 가능하다면 이 알고리즘은 stochastic gradient descent이 된다. (stochastic subgradient method가 좀 더 일반적인 형태)</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_12"></a>08-02-02 Convergence of Stochastic Methods</h1>
            <p>각 함수 \(f_i, i = 1,...,m\)는 컨벡스이고 Lipschitz continuous with constant G 하다고 가정하자.</p>

<p>Stochastic subgradient method에서 fixed step sizes와 diminishing step sizes에 대해 각각 다음의 성질을 가진다.</p>

<ul>
  <li><strong>Fixed step sizes</strong> for \(t_k = t\), \(k = 1, 2, 3, ...\)</li>
</ul>

<blockquote>
\[\text{Cyclic과 randomized method는 fixed step sizes일 경우 아래를 만족한다:} \\
\begin{align}
\lim_{k→\infty} f(x_{best}^{(k)}) \le f^{*} + 5m^{2}G^{2}t/2
\end{align}\]
</blockquote>

<p>여기서 \(mG\)는 \(\sum_{i=1}^{m} f_i\)의 Lipschitz constant이다.</p>

<ul>
  <li><strong>Diminishing step sizes</strong></li>
</ul>

<blockquote>
\[\text{Cyclic과 randomized method는 diminishing step sizes일 경우 모두 아래를 만족한다:} \\
\begin{align}
\lim_{k→\infty} f(x_{best}^{(k)}) = f^{*}
\end{align}\]
</blockquote>


        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_13"></a>08-02-03 Convergence Rate of Stochastic Method</h1>
            <p>Cyclic 방법과 randomized 방법은 convergence rate의 차이를 보인다.</p>

<p>Batch subgradient method의 <a href="/convex-optimization/contents/chapter08/2020/03/29/08_01_04_convergence_rate/">convergence rate</a>는 \(O(G_{batch}^{2}/\epsilon^{2})\)이다. (\(G_{batch}\)는 \(\sum\text{ }f_i\)에 대한 Lipschitz constant)</p>

<ul>
  <li>
    <p><strong>Cyclic method</strong>: Cyclic method의 iteration complexity는 \(O(m^{3}G^{2}/\epsilon^{2})\)이다. 만약 \(m\)번의 cyclic stochastic subgradient method를 한 번의 batch subgradient method로 가정한다면 각 cycle에서 \(O(m^{2}G^{2}/\epsilon^{2})\) 만큼의 시행이 필요하다. (\(G\)는 하나의 함수 \(f_i\)의 Lipschitz constant)</p>
  </li>
  <li>
    <p><strong>Randomized method</strong>: Randomized method의 iteration complexity는 \(O(m^{2}G^{2}/\epsilon^{2})\)이다. 즉, randomized method는 \(O(mG^{2}/\epsilon^2)\)번의 시행이 필요하므로 batch method와 cyclic method의 \(O(m^2G^2/\epsilon^2)\)보다 \(m\)배 빠르게 수렴하는 것을 알 수 있다. 결과적으로 Big-O 표기법으로는 \(m\)의 값이 크면 randomized method이 convergence rate가 더 빠르다고 할 수 있다.</p>
  </li>
</ul>

<p>Randomized method와 cyclic method의 convergence rate는 Big-O 표기법으로는  \(m\) 배 만큼의 차이가 있지만 사실 cyclic method의 Big-O표현은 worst-case bounded이고 randomized method은 average-case bounded이다. 즉, 어떠한 경우엔 두 방식의 convergence rate의 차이가 Big-O 표기법에서 보이는 것과 같이 그리 크게 차이나지 않을 수 도 있다는 점을 기억하자.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_14"></a>08-02-04 Batch vs Stochastic Methods</h1>
            <p>Batch method와 stochastic method의 수렴은 다음과 같은 성질을 띈다.</p>

<p>일반적으로 stochastic method는 초반에 빠르게 optimal point 근처로 접근하지만, optimal point 근처에서 어느 순간 더 이상의 수렴을 잘하지 못한다. 반면 batch method는 느리지만 비교적 정확하게 optimal point로 점차 수렴해가는 것을 관찰 할 수 있다.</p>

<p>아래는 <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>을 batch 방식과 stochastic 방식을 사용했을때, 각각의 수렴성을 비교한 그림이다. (regularizaton은 사용하지 않음)</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/convex-optimization/img/chapter_img/chapter08/08_02_stochastic_vs_batch.png" alt="stochastic_vs_batch" width="80%" height="80%" />
</p>
  <figcaption style="text-align: center;">[Fig 3] Batch vs Stochastic Gradient Descent [2]</figcaption>
</figure>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_15"></a>08-03 Improving on the Subgradient Method</h1>
            <p>Subgradient method는 미분할 수 없는 컨벡스 함수에도 사용할 수 있다는 점에서 범용성이 큰 것이 장점이다 (more general). 하지만 convergence rate가 \(O(1/\epsilon^{2})\)이므로 gradient descent의 convergence rate인 \(O(1/\epsilon)\)보다 훨씬 느리다.</p>

<p>Gradient descent와 subgradient method 각각의 장점을 잘 조합하는 방법은 없을까? 다음 장에서는 이 두 알고리즘의 장점을 결합한 proximal gradient descent란 방법을 알아보도록 하겠다.</p>

        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/convex-optimization/public/js/script.js'></script>
  </body>
</html>
