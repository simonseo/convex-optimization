<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      Canonical Problems &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://convex-optimization-for-all.github.io/public/logo.png">
  <link rel="shortcut icon" href="https://convex-optimization-for-all.github.io/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://convex-optimization-for-all.github.io/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="https://convex-optimization-for-all.github.io/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>05. Canonical Problems</h1>






<!-- Get first post and show it -->

<h1 id="canonical-problems">Canonical Problems</h1>

<p><a href="/chapter01/2021/01/07/optimization_problems/">첫 번째 장</a>에서 convex optimization problem이 다음과 같이 정의됨을 알아보았다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter05/05_00_optimization_problem.png" alt="[Fig1] Convex Optimization Problem in standard form [3]" width="70%" />
  <figcaption style="text-align: center;">[Fig1] Convex Optimization Problem in standard form [3]</figcaption>
</p>
</figure>

<ul>
  <li>The domain set is convex</li>
  <li>The objective function \(f\) and the inequality constraint function \(g_i\) are convex</li>
  <li>The equality constraint function \(h_j\) is affine</li>
</ul>

<p>이때 objective function과 constraint function의 유형에 따라 optimization problem은 다양한 범주로 나뉘어지게 된다. 이 장에서는 그 중 다음 6가지 세부항목에 대해 알아보도록 할 것이다.</p>

<ul>
  <li>Linear Programming (LP)</li>
  <li>Quadratic Programming (QP)</li>
  <li>Quadratically Constrained Quadratic Programming (QCQP)</li>
  <li>Second-Order Cone Programming (SOCP)</li>
  <li>Semidefinite Programming (SDP)</li>
  <li>Conic Programming (CP)</li>
</ul>

<p>위의 문제들은 다음과 같은 포함관계를 가지고 있으며, 우측으로 갈수록 좀 더 일반화된 형식이라고 볼 수 있다.</p>

<p>\(LP \subseteq QP \subseteq QCQP \subseteq SOCP \subseteq SDP \subseteq CP\)</p>
<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter05/05_00_canonical_problems.jpeg" alt="[Fig2] Canonical Problems" width="90%" />
  <figcaption style="text-align: center;">[Fig2] Canonical Problems</figcaption>
</p>
</figure>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">05-01 Linear Programming (LP)</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_2">05-02 Quadratic Programming (QP)</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_3">05-03 Quadratically Constrained Quadratic Programming (QCQP)</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_4">05-04 Second-Order Cone Programming (SOCP)</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_5">05-05 Semidefinite Programming (SDP)</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_6">05-06 Conic Programming (CP)</a>
    </li>
  
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>05-01 Linear Programming (LP)</h1>
            <p>목적함수(objective function)와 제약함수(constraint function)가 모두 affine이면 그 최적화 문제는 <em>linear program</em> (LP)이라고 불린다. General linear program은 다음과 같은 형태를 띈다.</p>

<h3 id="general-lp">General LP</h3>

<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{c^T x + d} \\\\
   &amp;\text{subject to } &amp;&amp;{Gx \preceq h}\\\\
   &amp; &amp;&amp;{Ax = b},\\\\
   &amp;\text{where } &amp;&amp;G \in \mathbb{R}^{\text{m x n}} \text{ and } A \in \mathbb{R}^{\text{p x n}}.
\end{align} \\\]
</blockquote>

<ul>
  <li>위 목적함수의 \(+d\)는 최적화의 과정 및 결과에 영향을 주지 않으므로 생략되어도 무방하다.</li>
  <li>만약 동일한 형태의 제약 아래 \(c^T x + d\)를 최대화하는 문제가 주어졌을 경우, 이를 \(-c^T x - d\)를 최소화하는 문제로 바꾸어 풀 수 있다.</li>
  <li>위 문제는 기하학적으로 polyhedron 형태의 feasible set에 대해 affine function \(c^T x + d\)를 최소화시키는 \(x^{*}\)를 찾는 것으로 해석된다.</li>
</ul>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter05/05_01_geometric_interpretation_of_LP.png" alt="[Fig1] Geometric interpretation of LP [1]" width="70%" />
  <figcaption style="text-align: center;">[Fig1] Geometric interpretation of LP [1]</figcaption>
</p>
</figure>

<h2 id="lp-in-standard-form">LP in Standard form</h2>
<p>General LP가 아닌 standard form LP의 형태로 문제정의에 이용할 수 있다.</p>

<h3 id="standard-form-lp">Standard form LP</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{c^T x + d} \\\\
   &amp;\text{subject to } &amp;&amp;{A x = b} \\\\
   &amp; &amp;&amp;{x \succeq 0}.
\end{align}\]
</blockquote>

<p>모든 general LP는 아래의 과정에 의해 standard form LP로 변형될 수 있다.</p>

<h3 id="converting-lps-to-standard-form">Converting LPs to standard form</h3>

<p><strong>Step1.</strong> Slack variable s를 이용하여 inequality constraint를 equality constraint로 바꿔준다.</p>
<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{x, s} &amp;&amp;{c^T x + d} \\\\
    &amp;\text{subject to } &amp;&amp;{Gx + s = h} \\\\
    &amp; &amp;&amp;{Ax = b},\\\\
    &amp; &amp;&amp;{s \succeq 0}.
\end{align}\]
</blockquote>

<p><strong>Step2.</strong> x를 두 개의 nonnegative variables로 치환한다.
\(x = x^{+}  - x^{-}\) 이고, \(x^{+} \text{, } x^{-} \succeq 0.\)</p>

<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{x^{+}, x^{-}, s} &amp;&amp;{c^Tx^{+} - c^Tx^{-} + d} \\\\
    &amp;\text{subject to } &amp;&amp;{Gx^{+} - Gx^{-} + s = h} \\\\
    &amp; &amp;&amp;{Ax^{+} - Ax^{-} = b},\\\\
    &amp; &amp;&amp;{s \succeq 0}\\\\
    &amp; &amp;&amp;{x^{+} \succeq 0}, {x^{-} \succeq 0}.
\end{align}\]
</blockquote>

<p><strong>Step3.</strong> \(\tilde{x}\), \(\tilde{c}\), \(\tilde{b}\), \(\tilde{A}\)를 정의.</p>

<blockquote>
  <p>\(\tilde{x} =
\begin{bmatrix}
x^{+} \\\\
x^{-} \\\\
s
\end{bmatrix}, 
\tilde{c} =
\begin{bmatrix}
c \\\\
-c \\\\
0
\end{bmatrix},
\tilde{b} =
\begin{bmatrix}
h \\\\
b
\end{bmatrix}\), 
\(\tilde{A} =
\begin{bmatrix}
G &amp; -G &amp; I\\\\
A &amp; -A &amp; O
\end{bmatrix}\)</p>
</blockquote>

<p><strong>Step4.</strong> <em>Step2</em>의 문제를 \(\tilde{x}\), \(\tilde{c}\), \(\tilde{b}\), \(\tilde{A}\)로 치환.</p>

<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{\tilde{x}} &amp;&amp;{\tilde{c}^T \tilde{x} + d} \\\\
    &amp;\text{subject to} &amp;&amp;{\tilde{A} \tilde{x} = \tilde{b}} \\\\
    &amp; &amp;&amp;{\tilde{x} \succeq 0}.
\end{align}\]
</blockquote>

<h3 id="example-1-diet-program">Example 1) Diet program</h3>

<p>영양분에 대한 요구사항을 만족하는 가장 싼 음식의 조합을 찾는 문제다.</p>

<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{x} &amp;&amp;{c^T x} \\\\
    &amp;\text{subject to } &amp;&amp;{Dx \succeq d} \\\\
    &amp; &amp;&amp;{x \succeq 0}.
\end{align}\]
</blockquote>

<ul>
  <li>\(c_j\): 음식 j에 대한 단위당 가격</li>
  <li>\(d_i\): 영양소 i에 대한 최소 권장 섭취량</li>
  <li>\(D_{ij}\): 영양소 i가 음식 j에 들어있는 정도</li>
  <li>\(x_j\): 식단에 포함된 음식 j의 양</li>
</ul>

<h3 id="example-2--basis-pursuit">Example 2)  Basis pursuit</h3>

<p><a href="https://en.wikipedia.org/wiki/Underdetermined_system">Undetermined linear system</a>은 변수의 갯수가 등식의 갯수보다 많은 선형시스템이다. \(X\beta = y\)에 대한 the sparsest solution을 찾는 문제는 아래와 같은 non-convex problem으로 정의된다.</p>

<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{\beta} &amp;&amp;{\|\beta\|_0} \\\\
    &amp;\text{subject to } &amp;&amp;{X\beta = y},\\\\
    &amp; \text{given } &amp;&amp;y \in \mathbb{R}^n \text{ and } X \in \mathbb{R}^\text{n x p} \text{, where } p &gt; n.\\\\
\end{align} \\\]
</blockquote>

<ul>
  <li>
\[{\| \beta \|_0} = \sum_{j=1}^p 1, \left\{ \beta_j \neq 0 \right\}\]
  </li>
</ul>

<p>위의 문제가 non-convex가 되는 이유는 바로 목적함수로 사용되는 \(L_0\) norm 때문이다. \(L_1\) norm이 <a href="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/#four">sparsity를 높이는 성질에 착안</a>하여 이를 \(L_0\) norm 대신 목적함수로 사용하면 문제를 convex로 만들어 솔루션을 근사할 수 있다. 우리는 이러한 방식을 <em>basis pursuit</em>라고 부른다.</p>

<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{\beta} &amp;&amp;{\|\beta\|_1} \\\\
    &amp;\text{subject to } &amp;&amp;{X\beta = y},\\\\
    &amp; \text{given } &amp;&amp;y \in \mathbb{R}^n \text{ and } X \in \mathbb{R}^\text{n x p} \text{, where } p &gt; n.
\end{align} \\\]
</blockquote>

<p>또한 basis pursuit는 다음과 같이 linear program으로 변형된다.</p>

<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{\beta, z} &amp;&amp;{1^Tz} \\\\
    &amp;\text{subject to } &amp;&amp;{z \succeq \beta}\\\\
    &amp; &amp;&amp;{z \succeq -\beta}\\\\
    &amp; &amp;&amp;{X\beta = y}
\end{align}\]
</blockquote>

<ul>
  <li>\(\beta\)의 각 component의 절댓값보다 \(z\)의 각 component가 크거나 같아야한다.</li>
  <li>최적화를 통해 \(z\)의 sparsity를 높여가며, \(\beta\)의 sparsity 또한 높아지도록 한다.</li>
</ul>

<h3 id="example-3--dantzig-selector">Example 3)  Dantzig selector</h3>

<p>Basis pursuit에서 다룬 문제와 목적이 동일하지만, y에 noise가 있는 경우를 전제해보자 ( \(X\beta \approx y\)). 이러한 문제를 <a href="https://statweb.stanford.edu/~candes/software/l1magic/downloads/papers/DantzigSelector.pdf">Dantzig selector</a>라고 한다.</p>

<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{\beta} &amp;&amp;{\|\beta\|_1} \\\\
    &amp;\text{subject to } &amp;&amp;{\| X^T (y - X \beta) \|_{\infty} \leq \lambda},\\\\
    &amp;\text{given } &amp;&amp;y \in \mathbb{R}^n \text{ and } X \in \mathbb{R}^\text{n x p} \text{, where } p &gt; n. \ \text{Here } \lambda \geq 0 \text{ is a hyper-parameter. }\\\\
\end{align} \\\]
</blockquote>

<ul>
  <li>\(y - X \beta \in \mathbb{R}^n\)은 residual이다.</li>
  <li>\(\|y - X \beta\|_{\infty} \leq \lambda\) 는 왜 inequality constraint로 사용되지 않을까?
    <ul>
      <li>Residual을 최소의 값으로 만들어주고 싶다고 하자.</li>
      <li>이는 min \(\| y - X\beta\|_2^2\)과 같이 표현될 수 있으며, 이 목적함수의 미분값이 0이 되는 지점을 찾는 것과 같다.</li>
      <li>즉, \(\frac{d(\| y - X\beta\|_2^2)}{d\beta} = -\frac{1}{2}X^T(y - X \beta) = 0\)이다.</li>
      <li>문제에 정의된 제약함수 \(X^T(y - X \beta)\)는 이러한 아이디어에서 도출된다.</li>
      <li>다르게 말하면 이는 residual이 variable X와 상관관계(correlation)가 없길 바라는 것과 같다. (\(X^T(y - X \beta) = 0\)는 residual vector와 X의 column space가 orthogonal함을 의미한다.)</li>
    </ul>
  </li>
</ul>

<p>Dantzig selector는 마찬가지로 다음과 같이 linear program으로 변형된다.</p>

<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{\beta, z} &amp;&amp;{\|\beta\|_1} \\\\
    &amp;\text{subject to } &amp;&amp;{x_j^T (y - X \beta) \preceq \lambda}, \text{for all } j = 1 \dotsc p\\\\
    &amp; &amp;&amp;{-x_j^T (y - X \beta) \preceq \lambda}, \text{for all } j = 1 \dotsc p\\\\
    &amp; &amp;&amp; z \succeq -\beta\\\\
    &amp; &amp;&amp; z \succeq \beta,\\\\
    &amp; \text{given } &amp;&amp;y \in \mathbb{R}^n \text{ and } X \in \mathbb{R}^\text{n x p} \text{, where } p &gt; n. \ \text{Here } x_j \text{ is a jth column of } X.\\\\
\end{align}\\\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>05-02 Quadratic Programming (QP)</h1>
            <p><em>Quadratic Program</em>(QP)는 목적함수(objective function)가 이차식(convex quadratic)이고, 제약함수(constraint functions)가 모두 affine인 convex optimization problem이다. General quadratic program은 다음과 같은 형태로 표현될 수 있다.</p>

<h3 id="quadratic-program">Quadratic Program</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{(1/2)x^T P x + q^T x + r} \\\\
   &amp;\text{subject to } &amp;&amp;{Gx \preceq h} \\\\
   &amp; &amp;&amp;{Ax = b},\\\\
   &amp; \text{where } &amp;&amp;P \in \mathbb{S}_{+}^n, G \in \mathbb{R}^{\text{m x n}} \text{, and } A \in \mathbb{R}^{\text{p x n}}.
\end{align}\\\]
</blockquote>

<ul>
  <li>위 목적함수의 \(+ r\)는 최적화의 과정 및 결과에 영향을 주지 않으므로 생략되어도 무방하다.</li>
  <li>\(P \in \mathbb{S}_{+}^n\)를 만족하지 않을 경우 위 문제는 더 이상 convex가 아니게 된다.</li>
  <li>Quadratic program에서 직접 명시되어있지 않더라도 \(P \in \mathbb{S}_{+}^n\)임을 가정한다.</li>
  <li>위 문제는 기하학적으로 polyhedron 형태의 feasible set에서 convex quadratic function(ellipsoid) \((1/2)x^T P x + q^T x + r\)를 최소화시키는 \(x^{*}\)를 찾는 것으로 해석된다.</li>
</ul>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter05/05_02_geometric_interpretation_of_QP.png" alt="[Fig 1] Geometric interpretation of QP [1]" width="70%" />
  <figcaption style="text-align: center;">[Fig 1] Geometric interpretation of QP [1]</figcaption>
</p>
</figure>

<h2 id="qp-in-standard-form">QP in Standard form</h2>
<p>Quadratic program의 standard form은 다음과 같이 표현된다.</p>

<h3 id="standard-form-qp">Standard form QP</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{(1/2)x^T P x + q^T x + r} \\\\
   &amp;\text{subject to } &amp;&amp;{A x = b} \\\\
   &amp; &amp;&amp;{x \succeq 0}.
\end{align}\]
</blockquote>

<p>General form의 quadratic program은 아래의 과정으로 standard form QP로 변형될 수 있다.</p>

<h3 id="converting-qps-to-standard-form">Converting QPs to standard form</h3>
<p><strong>Step1.</strong> Slack variable s를 이용하여 inequality constraint를 equality constraint로 바꿔준다.</p>
<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{x, s} &amp;&amp;{(1/2)x^T P x + q^T x + r} \\\\
    &amp;\text{subject to } &amp;&amp;{Gx + s = h} \\\\
    &amp; &amp;&amp;{Ax = b},\\\\
    &amp; &amp;&amp;{s \succeq 0}.
\end{align}\]
</blockquote>

<p><strong>Step2.</strong> x를 두 개의 nonnegative variables로 치환한다.
\(x = x^{+}  - x^{-}\)이고, \(x^{+} \text{, } x^{-} \succeq 0.\)</p>

<blockquote>
\[\begin{align}
    &amp;\text{minimize}_{x^{+}, x^{-}, s} &amp;&amp;{(1/2)(x^{+} - x^{-})^T P (x^{+} - x^{-}) + q^T x^{+} - q^T x^{-} + r}\\\\
    &amp;\text{subject to } &amp;&amp;{Gx^{+} - Gx^{-} + s = h} \\\\
    &amp; &amp;&amp;{Ax^{+} - Ax^{-} = b},\\\\
    &amp; &amp;&amp;{s \succeq 0}\\\\
    &amp; &amp;&amp;{x^{+} \succeq 0}, {x^{-} \succeq 0}.
\end{align}\]
</blockquote>

<p><strong>Step3.</strong> 
\(\tilde{x}\),
\(\tilde{q}\), 
\(\tilde{b}\),
\(\tilde{A}\), 
\(\tilde{P}\)를 정의.</p>

<blockquote>
\[\tilde{x} =
\begin{bmatrix}
x^{+} \\\\
x^{-} \\\\
s
\end{bmatrix},
\tilde{q} =
\begin{bmatrix}
q \\\\
-q \\\\
0
\end{bmatrix},
\tilde{b} =
\begin{bmatrix}
h \\\\
b
\end{bmatrix},
\tilde{A} =
\begin{bmatrix}
G &amp; -G &amp; I \\\\
A &amp; -A &amp; O
\end{bmatrix},
\tilde{P} =
\begin{bmatrix}
 P &amp; -P &amp; O \\\\
-P &amp;  P &amp; O \\\\
 O &amp;  O &amp; O \\\\
\end{bmatrix}\]
</blockquote>

<p><strong>Step4.</strong> <em>Step2</em>의 문제를
\(\tilde{x}, \tilde{q}, \tilde{b}, \tilde{A}, \tilde{P}\)
로 치환.</p>

<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{\tilde{x}} &amp;&amp;{(1/2)\tilde{x}^T \tilde{P} \tilde{x} + \tilde{q}^T \tilde{x} + r} \\\\
   &amp;\text{subject to } &amp;&amp;{\tilde{A} \tilde{x} = \tilde{b}} \\\\
   &amp; &amp;&amp;{\tilde{x} \succeq 0}.
\end{align}\]
</blockquote>

<h2 id="lp-and-equivalent-qp">LP and equivalent QP</h2>
<p>Quadratic program의 목적함수에서 이차항을 제거하게 되면 linear program의 형태와 동일해짐을 알 수 있다. 즉, LP는 QP의 한가지 특수한 경우에 해당하며, LP \(\subseteq\) QP의 관계가 성립한다.</p>

<h3 id="recall-general-lp">Recall: General LP</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{c^T x + d} \\\\
   &amp;\text{subject to } &amp;&amp;{Gx \preceq h} \\\\
   &amp; &amp;&amp;{Ax = b},\\\\
   &amp; \text{where } &amp;&amp;G \in \mathbb{R}^{\text{m x n}} \text{ and } A \in \mathbb{R}^{\text{p x n}}.
\end{align}\\\]
</blockquote>

<h3 id="example-1-portfolio-optimization">Example 1) Portfolio optimization</h3>
<p>Financial portfolio를 만듦에 있어 performance와 risk를 적절히 조율(trade-off)하는 문제다.</p>

<blockquote>
\[\begin{align}
   &amp;\text{maximize}_{x} &amp;&amp;{\mu^T x - \frac{\gamma}{2}x^T P x} \\\\
   &amp;\text{subject to } &amp;&amp;{1^Tx = 1} \\\\
   &amp; &amp;&amp;{x \succeq 0}.
\end{align}\]
</blockquote>

<ul>
  <li>\(\mu\): expected assets’ returns.</li>
  <li>\(P\): covariance matrix of assets’ returns.</li>
  <li>\(gamma\): risk aversion (hyper-parameter).</li>
  <li>\(x\): portfolio holdings (percentages).</li>
</ul>

<p>\(\mu\)와 \(P\)는 과거의 데이터를 통해서 얻을 수 있으며, 각 종목에 \(x\)만큼 투자했을 때 그 평균을 \(\mu^T x\), 분산을 \(x^T P x\)로 표현할 수 있다.</p>

<h3 id="example-2--support-vector-machines">Example 2)  Support vector machines</h3>
<p><a href="https://ratsgo.github.io/machine%20learning/2017/05/23/SVM/">Support vector machines(이하 SVM)</a>은 quadratic program의 한 예에 해당한다. 아래는 SVM의 변형인 <a href="https://ratsgo.github.io/machine%20learning/2017/05/29/SVM2/">C-SVM</a>이다. SVM에 대한 자세한 설명은 본 장의 주제에서 벗어나므로 여기서는 생략하도록 한다.</p>

<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{\beta, \beta_0, \xi} &amp;&amp;{\frac{1}{2} \| \beta \|_2^2 + C \sum_{i=1}^{n} \xi_i} \\
   &amp;\text{subject to } &amp;&amp;{\xi_i \geq 0, i = 1, \dotsc, n} \\
   &amp; &amp;&amp;{y_i (x_i^T \beta + \beta_0) \geq 1 - \xi_i, i = 1, \dotsc, n},\\
   &amp; \text{given} &amp;&amp; \text{y} \in \left\{-1, 1\right\}^n, X \in \mathbb{R}^{\text{n x p}} \text{ having rows } x_1, \dotsc, x_n.
\end{align}\\\]
</blockquote>

<h3 id="example-3--least-squares-in-regression">Example 3)  Least-squares in regression</h3>
<p>다음과 같은 convex quadratic function을 최소화하는 문제는 (unconstrained) QP에 해당한다.</p>
<blockquote>
\[\| Ax - b \|_2^2 = x^T A^TA x - 2b^TAx + b^Tb\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>05-03 Quadratically Constrained Quadratic Programming (QCQP)</h1>
            <p>Quadratic program에서 inequality contraint function이 이차식(convex quadratic)으로 교체되면, 이는 <em>Quadratically constrained quadratic program</em>(QCQP)이라고 불린다.</p>

<h3 id="quadratically-constrained-quadratic-program">Quadratically Constrained Quadratic Program</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{(1/2)x^T P_0 x + q_0^T x + r_0} \\\\
   &amp;\text{subject to } &amp;&amp;{(1/2)x^T P_i x + q_i^T x + r_i \leq 0}, i = 1, \dotsc, m\\\\
   &amp; &amp;&amp;{Ax = b},\\\\
   &amp; \text{where } &amp;&amp;P_i \in \mathbb{S}_{+}^n \text{ for } i = 0, \dotsc, m \text{, and } A \in \mathbb{R}^{\text{p x n}}.
\end{align}\\\]
</blockquote>

<h2 id="qp-and-equivalent-qcqp">QP and equivalent QCQP</h2>
<p>QCQP의 inequality constraint에서 \(P_i = 0, \text{ for } i = 1, \dotsc, m\)이면 QP의 형태와 동일해짐을 알 수 있다. 즉, QP는 QCQP의 한가지 특수한 경우에 해당하며, QP \(\subseteq\) QCQP의 관계가 성립한다.</p>

<h3 id="recall-quadratic-program">Recall: Quadratic Program</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{(1/2)x^T P x + q^T x + r} \\\\
   &amp;\text{subject to } &amp;&amp;{Gx \preceq h} \\\\
   &amp; &amp;&amp;{Ax = b},\\\\
   &amp; \text{where } &amp;&amp;P \in \mathbb{S}_{+}^n, G \in \mathbb{R}^{\text{m x n}} \text{, and } A \in \mathbb{R}^{\text{p x n}}.
\end{align}\\\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>05-04 Second-Order Cone Programming (SOCP)</h1>
            <p>General LP에서 inequality constraint가 우항이 affine function인 second-order cone costraint로 교체되면, 이는 <em>Second-Order Cone Program</em>(SOCP)이다.</p>

<h3 id="second-order-cone-program">Second-Order Cone Program</h3>

<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{f^T x} \\\\
   &amp;\text{subject to } &amp;&amp;{\| A_i x + b_i \|_2 \leq c_i^T x + d_i, i = 1, \dotsc, m}\\\\
   &amp; &amp;&amp;{Fx = g},\\\\
   &amp; \text{where } &amp;&amp;x \in \mathbb{R}^n \text{ is the optimization variable, } A_i  \in \mathbb{R}^{n_i \text{ x n}}, \text{ and } F \in \mathbb{R}^{\text{p x n}}.
\end{align}\\\]
</blockquote>

<h3 id="recall-norm-cone">Recall: Norm cone</h3>
<p><em>Norm cone</em>은 반경 \(t\) 이내인 점들로 이뤄진 cone으로 \((x,t)\)로 정의되는 \(R^{n+1}\)차원의 convex cone이다. 이때, 반경은  임의의 norm으로 정의된다.</p>

<blockquote>
\[\left\{(x, t) : \left \Vert x \right \| \le t \right\} \text{, for a norm } \left \Vert  · \right \|\]
</blockquote>

<p>아래 그림에는 \(l_2\) norm \(\left \Vert  · \right \|_2\)에 대한 norm cone이 그려져 있다. 이를 <em>second-order cone</em> 또는 ice cream cone이라고도 부른다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter05/05_04_Norm_Cone.PNG" alt="[Fig1] Norm Cone [1]" width="70%" />
  <figcaption style="text-align: center;">[Fig1] Norm Cone [1]</figcaption>
</p>
</figure>

<h2 id="qcqp-and-equivalent-socp">QCQP and equivalent SOCP</h2>
<p>QCQP는 다음의 유도과정을 거쳐 SOCP의 한가지 특수한 경우로 변형된다. (즉, QCQP \(\subseteq\) SOCP)</p>

<h3 id="recall-quadratically-constrained-quadratic-program">Recall: Quadratically Constrained Quadratic Program</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{(1/2)x^T P_0 x + q_0^T x + r_0} \\\\
   &amp;\text{subject to } &amp;&amp;{(1/2)x^T P_i x + q_i^T x + r_i \leq 0}, i = 1, \dotsc, m\\\\
   &amp; &amp;&amp;{Ax = b},\\\\
   &amp; \text{where } &amp;&amp;P_i \in \mathbb{S}_{+}^n \text{ for } i = 0, \dotsc, m \text{, and } A \in \mathbb{R}^{\text{p x n}}
\end{align}\\\]

</blockquote>

<p><strong>Step1.</strong> 유도과정의 편의를 위해 약간 다른 형태로 QCQP를 재정의한다.</p>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{x^T P_0 x + 2q_0^T x + r_0} \\\\
   &amp;\text{subject to } &amp;&amp;{x^T P_i x + 2q_i^T x + r_i \leq 0}, i = 1, \dotsc, m\\\\
   &amp; &amp;&amp;{Ax = b},\\\\
   &amp; \text{where } &amp;&amp; P_i \in \mathbb{S}_{+}^n \text{ for } i = 0, \dotsc, m \text{, and } A \in \mathbb{R}^{\text{p x n}}.
\end{align}\\\]

</blockquote>

<p><strong>Step2.</strong> \(P_0\)는 positive semidefinite matrix이므로 \(P_0 = \tilde{P_0}\tilde{P_0}\)를 만족하는 \(\tilde{P_0}\) 또한 positive semidefinite matrix이다[<a href="https://en.wikipedia.org/wiki/Positive-definite_matrix#Further_properties">5</a>]. \(\tilde{P_0}\)는 eigendecomposition[<a href="https://en.wikipedia.org/wiki/Matrix_decomposition#Eigendecomposition">6</a>]에 의해 \(Q_0 \Lambda_0 Q_0^{-1}\)(\(= Q_0 \Lambda_0 Q_0^T\) [<a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices">7</a>])로 분해될 수 있는데 이를 이용하여 QCQP의 objective function을 변형하면 다음과 같다.</p>

<ul>
  <li>
\[P_0 = Q_0 \Lambda_0 \Lambda_0 Q_0^T\]
  </li>
  <li>
\[I = Q_0 \Lambda_0 \Lambda_0^{-1} Q_0^{-1} = Q_0 \Lambda_0^{-1} \Lambda_0 Q_0^{-1}\]
  </li>
</ul>

<blockquote>
\[\begin{align}
{x^T P_0 x + 2q_0^T x + r_0} &amp;= {x^T P_0 x + q_0^T x + x^T q_0 + q_0^T P_0^{-1} q_0 - q_0^T P_0^{-1} q_0 + r_0}\\\\
&amp;= {x^T Q_0 \Lambda_0 \Lambda_0 Q_0^T x} + 
     {q_0^T Q_0 \Lambda_0^{-1} \Lambda_0 Q_0^{-1} x} + {x^T Q_0 \Lambda_0 \Lambda_0^{-1} Q_0^{-1} q_0} + 
     {q_0^T Q_0 \Lambda_0^{-1} \Lambda_0^{-1} Q_0^T q_0} - {q_0^T P_0^{-1} q_0 + r_0}\\\\
&amp;=(\Lambda_0 Q_0^T x + \Lambda_0^{-1} Q_0^T q_0)^T(\Lambda_0 Q_0^T x + \Lambda_0^{-1} Q_0^T q_0) - {q_0^T P_0^{-1} q_0 + r_0}\\\\
&amp;=\| \Lambda_0 Q_0^T x + \Lambda_0^{-1} Q_0^T q_0 \|_2^2 - {q_0^T P_0^{-1} q_0 + r_0}\\\\
\end{align}\]
</blockquote>

<p><strong>Step3.</strong> Inequality constraint function에도 Step2와 같은 방법을 적용한뒤, Step1의 QCQP에 대입한다.</p>

<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{\| \Lambda_0 Q_0^T x + \Lambda_0^{-1} Q_0^T q_0 \|_2^2 - {q_0^T P_0^{-1} q_0 + r_0}} \\\\
   &amp;\text{subject to } &amp;&amp;{\| \Lambda_i Q_i^T x + \Lambda_i^{-1} Q_i^T q_i \|_2^2 \leq {q_i^T P_i^{-1} q_i + r_i}}, i = 1, \dotsc, m\\\\
   &amp; &amp;&amp;{Ax = b}.\\\\
\end{align}\]

</blockquote>

<p><strong>Step4.</strong> 목적함수의 \({q_0^T P_0^{-1} q_0 + r_0}\)는 상수이므로 생략되어도 무방하다.</p>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{\| \Lambda_0 Q_0^T x + \Lambda_0^{-1} Q_0^T q_0 \|_2^2} \\\\
   &amp;\text{subject to } &amp;&amp;{\| \Lambda_i Q_i^T x + \Lambda_i^{-1} Q_i^T q_i \|_2^2 \leq {q_i^T P_i^{-1} q_i + r_i} }, i = 1, \dotsc, m\\\\
   &amp; &amp;&amp;{Ax = b}.\\\\
\end{align}\]
</blockquote>

<p><strong>Step5.</strong> Scalar variable t를 도입하여 Step4에서와 동일한 문제를 정의할 수 있다.</p>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x, t} &amp;&amp;{t} \\\\
   &amp;\text{subject to } &amp;&amp;\lVert{\Lambda_{0} Q_0^T x + \Lambda_0^{-1} Q_0^T q_0} \rVert_2^2 \leq t\\\\
   &amp; &amp;&amp;{\| \Lambda_i Q_i^T x + \Lambda_i^{-1} Q_i^T q_i \|_2^2 \leq {q_i^T P_i^{-1} q_i + r_i} }, i = 1, \dotsc, m\\\\
   &amp; &amp;&amp;{Ax = b}.\\\\
\end{align}\]
</blockquote>

<p>위는 SOCP의 한가지 특수한 경우에 해당한다. 즉, QCQP \(\subseteq\) SOCP의 관계가 성립한다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_5"></a>05-05 Semidefinite Programming (SDP)</h1>
            <p>General LP에서 inequality constraint가 linear matrix inequality(LMI)로 교체되면, 이는 <em>Semidefnite Program</em>(SDP)이다.</p>

<h3 id="semidefinite-program">Semidefinite Program</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{c^T x + d} \\\\
   &amp;\text{subject to } &amp;&amp;{xF_1 + \dotsb + x_nF_n + G \preceq 0} \\\\
   &amp; &amp;&amp;{Ax = b},\\\\
   &amp; \text{where } &amp;&amp;G, F_1, \dotsb, F_n \in \mathbb{S}^{k} \text{ and } A \in \mathbb{R}^{\text{p x n}}.
\end{align}\\\]
</blockquote>

<ul>
  <li>\(G, F_1, \dotsb, F_n\)가 모두 diagonal matrices면, 위의 inequality constraint는 n개의 linear inequalities와 동일해진다. 이 경우 SDP는 LP와 같다.</li>
  <li>복수의 LMI는 다음과 같이 단일의 LMI로 표현된다.
    <blockquote>
\[x_1\hat{F_1} + \dotsb + x_n\hat{F_n} + \hat{G} \preceq 0, \phantom{5} x_1\tilde{F_1} + \dotsb + x_n\tilde{F_n} + \tilde{G} \preceq 0\]

      <center>is equivalent to single LMI: </center>

\[x_1
\begin{bmatrix}
    \hat{F_1} &amp; 0 \\\\
    0 &amp; \tilde{F_1} \\\\
\end{bmatrix} + 
x_2
\begin{bmatrix}
    \hat{F_2} &amp; 0 \\\\
    0 &amp; \tilde{F_2} \\\\
\end{bmatrix} + 
\dotsb
+
x_n
\begin{bmatrix}
    \hat{F_n} &amp; 0 \\\\
    0 &amp; \tilde{F_n} \\\\
\end{bmatrix} + 
\begin{bmatrix}
    \hat{G_1} &amp; 0 \\\\
    0 &amp; \tilde{G_1} \\\\
\end{bmatrix}
\preceq 0\]
    </blockquote>
  </li>
</ul>

<h2 id="sdp-in-standard-form">SDP in Standard form</h2>
<p>다음과 같이 표현될 때, semidefinite program의 standard form이라고 한다.</p>

<h3 id="standard-form-sdp">Standard form SDP</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{X} &amp;&amp;{tr(CX)} \\\\
   &amp;\text{subject to } &amp;&amp;{tr(A_iX) = b_i, \phantom{5} i=1,\dotsc,p} \\\\
   &amp; &amp;&amp;{X \succeq 0},\\\\
   &amp; \text{where } C, A_1, \dotsc, A_p \in \mathbb{S}^n.
\end{align}\]
</blockquote>

<ul>
  <li>Recall: \(tr(CX) = \sum_{i,j=1}^n C_{ij}X_{ij}\)</li>
</ul>

<p>모든 SDP는 아래의 과정에 의해 standard form SDP로 변형될 수 있다.</p>

<h3 id="converting-sdps-to-standard-form">Converting SDPs to standard form</h3>
<p><strong>Step1.</strong>  Slack variable S를 이용하여 inequality constraint를 equality constraint로 바꿔준다.</p>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{c^T x + d} \\\\
   &amp;\text{subject to } &amp;&amp;{\sum_{l=1}^n F_l x_l+ S = -G} \\\\
   &amp; &amp;&amp;{Ax = b}\\\\
   &amp; &amp;&amp;{S \succeq 0}
\end{align}\]
</blockquote>

<p><strong>Step2.</strong> step1에서 유도된 equality constraint를 각 component에 대한 식으로 변형한다.</p>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{c^T x + d} \\\\
   &amp;\text{subject to } &amp;&amp;{\sum_{l=1}^n (F_l x_l)_{ij} + S_{ij} = -G_{ij}, i,j = 1, \dotsc, k} \\\\
   &amp; &amp;&amp;{Ax = b}\\\\
   &amp; &amp;&amp;{S \succeq 0}
\end{align}\]
</blockquote>

<p><strong>Step3.</strong> x를 두 개의 nonnegative variables로 치환한다.
\(x = x^{+}  - x^{-}\)이고, \(x^{+} \text{, } x^{-} \succeq 0.\)</p>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{c^T (x^{+}  - x^{-}) + d} \\\\
   &amp;\text{subject to } &amp;&amp;{\sum_{l=1}^n (F_l x^{+} _l)_{ij} - \sum_{l=1}^n (F_l x^{-} _l)_{ij} + S_{ij} = -G_{ij}, i,j = 1, \dotsc, k} \\\\
   &amp; &amp;&amp;{Ax^{+}  - Ax^{+} = b}\\\\
   &amp; &amp;&amp;{S \succeq 0}\\\\
   &amp; &amp;&amp;{x^{+} \text{, } x^{-} \succeq 0}.
\end{align}\]
</blockquote>

<p><strong>Step4.</strong> \(X, C, \tilde{A}, \tilde{b}\)를 정의.</p>

<ul>
  <li>All the blanks are zero.</li>
</ul>

<blockquote>
  <p>\(X = 
\begin{bmatrix}
diag(x^{+})\\\\
 &amp; diag(x^{-})\\\\
&amp;&amp; s_{11}\\\\
&amp;&amp;&amp; s_{12}\\\\
&amp;&amp;&amp;&amp;\dotsc\\\
&amp;&amp;&amp;&amp;&amp;s_{ij}\\\\
&amp;&amp;&amp;&amp;&amp;&amp;\dotsc \\\
&amp;&amp;&amp;&amp;&amp;&amp;&amp;s_{kk}\\\\
\end{bmatrix}
,\)
\(C = 
\begin{bmatrix}
diag(c)\\\\
&amp; -diag(c) &amp;\\\\
&amp; &amp; O_{k^2 \text{ x } k^2}\\\\
\end{bmatrix}
,\)
\(P_{ij} = 
\begin{bmatrix}
(F_1)_{ij}\\\\
&amp;(F_2)_{ij}\\\\
&amp;&amp;\dotsc\\\\
&amp;&amp;&amp;(F_n)_{ij}\\\\
&amp;&amp;&amp;&amp;-(F_1)_{ij}\\\\
&amp;&amp;&amp;&amp;&amp;-(F_2)_{ij}\\\\
&amp;&amp;&amp;&amp;&amp;&amp;\dotsc\\\\
&amp;&amp;&amp;&amp;&amp;&amp;&amp;-(F_n)_{ij}\\\\
&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;0&amp;\\\\
&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;\dotsc\\\\
&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;1 \phantom{1} (\text{ij th position})\\\\
&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;\dotsc\\\\
&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;0\\\\
\end{bmatrix}
,\)</p>

  <p>\(Q_{i}= 
\begin{bmatrix}
diag(A_i)\\\\
&amp;-diag(A_i)\\\\
&amp;&amp;O_{k^2 \text{ x } k^2}\\\\
\end{bmatrix}\)
(\(A_i\) is ith row of A),
\(\tilde{A} = 
\begin{bmatrix}
P_{11}\\\\
\dotsc\\\\
P_{kk}\\\\
Q_{1}\\\\
\dotsc\\\\
Q_{p}\\\\
\end{bmatrix}
-G_{ij} = tr(P_{ij}X)
,\)</p>

  <p>\(b_i = tr(Q_iX)\),</p>

  <p>\(\tilde{b} = 
\begin{bmatrix}
-G_{11}\\\\
\dotsc\\\\
-G_{kk}\\\\
b_{1}\\\\
\dotsc\\\\
b_{p}\\\\
\end{bmatrix}\).</p>
</blockquote>

<p><strong>Step5.</strong> <em>Step3</em>의 문제를 \(X, C, \tilde{A}, \tilde{b}\)로 치환.</p>

<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{X} &amp;&amp;{tr(CX)} \\\\
   &amp;\text{subject to } &amp;&amp;{tr(\tilde{A}_iX) = \tilde{b}_i, \phantom{5} i=1,\dotsc,k^2+p} \\\\
   &amp; &amp;&amp;{X \succeq 0}.
\end{align}\]
</blockquote>

<h2 id="socp-and-equivalent-sdp">SOCP and equivalent SDP</h2>
<p>Schur complement[<a href="https://en.wikipedia.org/wiki/Schur_complement">8</a>]를 이용하여 SOCP의 inequality constraint를 표현하면 SOCP는 SDP의 어떤 특수한 경우로 변형된다. 즉, SOCP \(\subseteq\) SDP의 관계가 성립한다.</p>

<h3 id="recall-second-order-cone-program">Recall: Second-Order Cone Program</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{f^T x} \\\\
   &amp;\text{subject to } &amp;&amp;{\| A_i x + b_i \|_2 \leq c_i^T x + d_i, i = 1, \dotsc, m}\\\\
   &amp; &amp;&amp;{Fx = g}.
\end{align}\]
</blockquote>

<h3 id="socp-to-sdp-by-schur-complement">SOCP to SDP by Schur complement</h3>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{f^T x} \\\\
   &amp;\text{subject to } 
   &amp;&amp;
   \begin{bmatrix}
   (c_i^T x + d)I    &amp; A_i x + b_i \\\\
   (A_i x + b_i)^T &amp; c_i^T x + d \\\\
   \end{bmatrix} \succeq 0, i = 1, \dotsc, m\\\\
   &amp; &amp;&amp;{Fx = g}.
\end{align}\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_6"></a>05-06 Conic Programming (CP)</h1>
            <p>General LP에서 inequality constraint가 generalized inequality constraint로 교체되면, 이는 <em>Conic Program</em>(CP)이다.</p>

<h4 id="conic-program">Conic Program</h4>
<blockquote>
\[\begin{align}
   &amp;\text{minimize}_{x} &amp;&amp;{c^T x + d} \\\\
   &amp;\text{subject to } &amp;&amp;{Fx + g \preceq_{K} 0} \\\\
   &amp; &amp;&amp;{Ax = b},\\\\
   &amp; \text{where } &amp;&amp;c, x \in \mathbb{R}^{n}, A \in \mathbb{R}^{p \text{ x } n}, \text{and } b \in \mathbb{R}^{p}.
\end{align}\\\]
</blockquote>

<ul>
  <li>\(F: \mathbb{R}^n \rightarrow Y\) is a linear map, \(g \in Y\), for Eucliean space Y.</li>
  <li>LP는 \(K = \mathbb{R}_{+}^n\)일때이며, 이는 CP의 special case에 해당한다.</li>
  <li>SDP는 \(K = \mathbb{S}_{+}^n\)일때이며, 이 또한 CP의 special case에 해당한다. 즉, SDP \(\subseteq\) CP의 관계가 성립한다.</li>
</ul>

        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
