<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      Dual Methods &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://convex-optimization-for-all.github.io/public/logo.png">
  <link rel="shortcut icon" href="https://convex-optimization-for-all.github.io/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://convex-optimization-for-all.github.io/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="https://convex-optimization-for-all.github.io/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>20. Dual Methods</h1>






<!-- Get first post and show it -->

<p>본 장에서는 dual 을 이용하여 문제를 해결하는 방법으로서,  dual subgradient method, dual decompostion method, augmented Lagrangian method에 대해 알아보고, Alternating Direction Method of Multipliers (ADMM)의 개념을 간단히 알아본다.</p>

<p>우선 앞에서 배운 내용 중 Proximal Newton method 와 Conjugate function 내용을 간단히 복습한다.</p>

<h2 id="review-proximal-newton-method">Review: proximal Newton method</h2>
<p>다음의 문제가 있다.</p>
<blockquote>
  <p>\begin{equation}
\min_x g(x) + h(x)
\end{equation}</p>
</blockquote>

<p>여기서, 함수 \(g\)와 \(h\)는 convex 함수이며, \(g\)는 두번 미분 가능하고, \(h\)는 simple 하다고 가정한다.</p>

<p>Proximal Newton method는 최초 \(x^{(0)} \in \mathbb{R}^n\)에서 시작되며, 먼저 함수 \(g\)와 \(h\)에게 모두 좋은 최적의 벡터 방향을 아래와 같이 찾는다</p>
<blockquote>
  <p>\begin{alignat}{1}
v^{(k)} &amp; = \arg \min_v g({x^{(k-1)}})^T v +  \frac{1}{2} v^T \nabla^2 g(x^{(k-1)}) v + h(x^{(k-1)} + v) 
\end{alignat}</p>
</blockquote>

<p>위에서 찾아진 방향으로 아래와 같이 다음 \(x^{(k)}\)를 업데이트한다.</p>
<blockquote>
  <p>\begin{equation}
x^{(k)} = x^{(k-1)} + t_k v^{(k)}, k=1,2,3,\dots 
\end{equation}</p>
</blockquote>

<p>여기서, \(t_k\)는 step size로서 backtracking으로 결정된다.</p>

<p>위 두 과정을 반복적으로 실행한다.</p>

<blockquote>
  <ul>
    <li>위 반복(iteration)은 매우 비용이 많이 든다 (\(v^{(k)}\)를 계산하는 것이 일반적으로 매우 어렵다)</li>
    <li>그러나, 적당한 조건하에서는 converge하기까지 매우 적은 iteration이 요구되고, local quadratic convergence의 수렴 속도를 갖는다</li>
  </ul>
</blockquote>

<h2 id="review-conjugate-function">Review: conjugate function</h2>
<p>\(f: \mathbb{R}^n \to \mathbb{R}\)에 대해, conjugate 함수는 아래와 같이 정의된다.</p>
<blockquote>
  <p>\begin{equation}
f^*(y) = \max_x y^Tx - f(x)
\end{equation}</p>
</blockquote>

<p>(1) Conjugate 함수는 아래와 같이 쓸 수 있으며, 이는 dual 문제에서 자주 나타나는 형태이다.</p>
<blockquote>
  <p>\begin{equation}
-f^{\ast}(y) = \min_x f(x) - y^Tx
\end{equation}</p>
</blockquote>

<p>(2) 만약 \(f\)가 closed하고 convex이면, \(f^{**} = f\) 이다. 또한,</p>
<blockquote>
  <p>\begin{equation}
x \in \partial f^{\ast}(y) \Longleftrightarrow y \in \partial f(x) \Longleftrightarrow x \in \arg\min_z f(z) - y^Tz
\end{equation}</p>
  <h4 id="proof">Proof</h4>
  <p>먼저, \(x \in \partial f^{\ast}(y) \Longleftrightarrow y \in \partial f(x)\)을 증명한다.</p>
</blockquote>

<h4 id="1단계--x-in-partial-fasty-longleftarrow-y-in-partial-fx">1단계 : \(x \in \partial f^{\ast}(y) \Longleftarrow y \in \partial f(x)\)</h4>
<blockquote>

  <p>\(y \in \partial f(x)\)를 가정하자. 그러면, \(x\)는 \(y^Tz - f(z)\)를 최대로 하게 하는 \(z\)들의 집합 \(M_y\) 에 속하게 된다, 즉 \(x \in M_y\). <br /> 그러나, \(f^{\ast}(y)=   \max_z y^Tz - f(z)\) 이고, \(\partial f^{\ast}(y)=\text{cl} \left( \text{conv} \left( \bigcup_{z \in M_y} \left\{ z \right\} \right) \right)\). 따라서, \(x \in \partial f^{\ast}(y)\)</p>
</blockquote>

<h4 id="2단계--x-in-partial-fasty-longrightarrow-y-in-partial-fx">2단계 : \(x \in \partial f^{\ast}(y) \Longrightarrow y \in \partial f(x)\)</h4>
<blockquote>

  <p>위에서 보인것과 같이, 만약, \(x \in  \partial f^{\ast}(y)\) 이면, \(y \in \partial f^{\ast\ast}(x)\). 여기서, \(f^{\ast\ast}(x)=f\) 이므로 \(y \in \partial f(x)\).</p>
</blockquote>

<p>위 1, 2 단계를 통해, \(x \in \partial f^{\ast}(y) \Longleftrightarrow y \in \partial f(x)\)이 증명되었다.</p>
<h4 id="3단계--x-in-partial-fasty-longleftrightarrow-y-in-partial-fx-longleftrightarrow-x-in-argmin_z-fz---ytz">3단계 : \(x \in \partial f^{\ast}(y) \Longleftrightarrow y \in \partial f(x) \Longleftrightarrow x \in \arg\min_z f(z) - y^Tz\)</h4>
<blockquote>

  <p>한편, \(y \in \partial f(x) \Longleftrightarrow x \in \arg\min_z f(z) - y^Tz\)은 subgradient의 정의로부터 자명한 사실이다.  <br />
따라서, 위 두 증명을 통해, \(x \in \partial f^{\ast}(y) \Longleftrightarrow y \in \partial f(x) \Longleftrightarrow x \in \arg\min_z f(z) - y^Tz\)임이 증명되었다.</p>
</blockquote>

<p>(3) 만약 \(f\)가 strictly convex이면,</p>
<blockquote>
\[\begin{equation}
\nabla f^{\ast}(y) = \arg\min_z f(z) - y^T z
\end{equation}\]
</blockquote>

<h4 id="proof-1">Proof</h4>

<blockquote>
  <p>\(f\)가 strictly convex이면, \(f(z)-y^Tz\)는 최소값을 갖는 유일한 \(z\)가 존재하며, 
이것은 위 (2)에 대한 증명으로부터 \(\nabla f^{\ast}(y)\)이어야 한다.</p>
</blockquote>

<p>다시 말하면 \(f\)가 strictly convex이면  \(f^{\ast}\)의 subgradient는 1개이며 gradient가 된다. 따라서,  \(f^{\ast}\)는 differentiable한 함수이다.</p>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">20-01 Dual (sub)gradient methods</a>
    </li>
  
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_2"> 20-01-01 Convergence Analysis</a>
    </li>
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_3">20-02 Dual Decomposition</a>
    </li>
  
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_4"> 20-02-01 Dual Decomposition with Equality Constraint</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_5"> 20-02-02 Dual Decomposition with Inequality Constraint</a>
    </li>
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_6">20-03 Augmented Lagrangians</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_7">20-04 A peak at ADMM</a>
    </li>
  
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_8"> 20-04-01 ADMM</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_9"> 20-04-02 Converegence Guarantee</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_10"> 20-04-03 ADMM in Scaled Form</a>
    </li>
  

  
  
  
  
  
    <li style="text-align:left; vertical-align: middle;  margin-left: 0em;" >
      <a href="#_page_11"> 20-04-04 Example - Alternating Projection</a>
    </li>
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_12">20-05 References</a>
    </li>
  
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>20-01 Dual (sub)gradient methods</h1>
            <p>Close-form 형태의 dual (conjugate)을 찾을 수 없는 경우에도 dual 기반의 subgradient 또는 gradient method를 사용할 수 있다.</p>

<p>예를 들어, 다음의 문제를 보자.</p>
<blockquote>
  <p>\begin{equation}
\min_x f(x) \text{ subject to } Ax = b
\end{equation}</p>
</blockquote>

<p>위 문제의 dual 문제는 아래와 같다. 여기서 \(f^{\ast}\)는 \(f\)의 conjugate이다.</p>
<blockquote>
  <p>\begin{equation}
\max_u -f^{\ast}(-A^T u) - b^T u
\end{equation}</p>
</blockquote>

<p>이때, \(g(u)\)를 \(-f^{\ast}(-A^Tu)-b^Tu\)로 정의하면 \(g(u)\)의 subgradient는 다음과 같다.</p>
<blockquote>
  <p>\begin{equation}
\partial g(u) = A \partial f^{\ast}(-A^Tu) - b
\end{equation}</p>
</blockquote>

<p>위 식에서 \(\partial f^{\ast}(-A^Tu)\)를 \(x\)로 정리하면 아래와 같이 표현될 수 있다.</p>

<blockquote>
  <p>\begin{equation}
\partial g(u) = Ax-b \quad \text{where} \quad x \in \arg\min_z f(z) + u^T A z
\end{equation}</p>
</blockquote>

<h2 id="dual-subgradient-method">Dual subgradient method</h2>
<p><strong>Dual subgradient method</strong>는 dual 문제의 목적식을 최대화하기 위해 시작점 \(u^{(0)}\)에서 시작해서 \(k=1,2,3,\dots\)에 대해 다음 단계를 반복한다.</p>
<blockquote>
\[\begin{alignat}{1}
x^{(k)} &amp; \in \arg \min_x f(x) + ({u^{(k-1)}})^T A x  \\
u^{(k)} &amp; = u^{(k-1)} + t_k (A x^{(k)} - b) 
\end{alignat}\]
</blockquote>

<p>여기서 step size \(t_k(k=1,2,3,\dots\))는 표준적인 방식으로 선택된다.</p>

<h4 id="strictly-convex인-경우">Strictly Convex인 경우</h4>
<p>만약 \(f\)가 strictly convex라면 \(f^{\ast}\)는 미분가능해진다.</p>

<p>따라서, 알고리즘은 \(k=1,2,3,\dots\)에 대해 다음 단계를 반복하는 <strong>dual gradient ascent</strong>가 된다.</p>
<blockquote>
\[\begin{alignat}{1}
x^{(k)} &amp; = \arg \min_x f(x) + ({u^{(k-1)}})^T A x  \\
u^{(k)} &amp; = u^{(k-1)} + t_k (A x^{(k)}-b) 
\end{alignat}\]
</blockquote>

<p>이전 방식과 다른 점은 \(x^{(k)}\)가 유일하다는 것이다. (\(\text{argmin}\)과의 관계가 \(=\) 관계임을 확인해보라.)</p>

<p>여기서 step size \(t_k(k=1,2,3,\dots\))도 표준적인 방식으로 선택되며 \(\text{argmin}\)을 수행할 때 proximal graidient나 acceleration도 평소처럼 적용할 수 있다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>20-01-01 Convergence Analysis</h1>
            <h2 id="lipschitz-gradients-and-strong-convexity">Lipschitz gradients and strong convexity</h2>
<p>\(f\)가 closed convex 함수라고 가정하자. 그러면 다음 동치 관계가 성립된다.</p>
<blockquote>
  <p>\begin{equation}
\text{\(f\) is strongly convex with parameter \(d\) \(\Longleftrightarrow \nabla f^{\ast}\) Lipschitz with parameter \(1/d\).} 
\end{equation}</p>
</blockquote>

<h3 id="proof">Proof</h3>
<p>만약 \(g\)가 strongly convex하고 \(x\)에서 miminize된다고 하면 다음 관계가 성립한다.</p>
<blockquote>
  <p>\begin{equation}
g(y) \geq g(x) + \frac{d}{2}\lVert y-x \rVert_2^2, \text{ for all } y
\end{equation}</p>
</blockquote>

<p>우선, \(g(x) = f(x) − u^T x\)를 최소화하는 \(x_u = \nabla f^{\ast}(u)\)와 \(g(x) = f(x) − v^T x\)를 최소화하는 \(x_v = \nabla f^{\ast}(v)\)가 있다고 하자.</p>

<p>그러면, 위 식으로부터 다음 두 부등식을 얻을 수 있다.</p>
<blockquote>
\[\begin{alignat}{1}
f(x_v) - u^Tx_v \geq f(x_u) - u^T x_u + \frac{d}{2} \lVert x_u - x_v \rVert_2^2 \\
f(x_u) - v^Tx_u \geq f(x_v) - v^T x_v + \frac{d}{2} \lVert x_u - x_v \rVert_2^2 
\end{alignat}\]
</blockquote>

<p>위 두 식을 더하면 다음과 같은 식을 얻을 수 있다.</p>
<blockquote>
  <p>\begin{equation}
f(x_v) - u^Tx_v + f(x_u) - v^Tx_u \geq f(x_u) - u^T x_u +  f(x_v) - v^T x_v + d \lVert x_u - x_v \rVert_2^2.<br />
\end{equation}</p>
</blockquote>

<p>이 식을 재정렬 후 Cauchy-Schwartz를 적용하면 다음과 같이 정리된다.</p>
<blockquote>
\[\begin{align}
d \lVert x_u - x_v \rVert_2^2 &amp; \leq - u^Tx_v - v^Tx_u + u^T x_u + v^T x_v \\\\
&amp; = (u-v)^T(x_u - x_v) \\\\
&amp; \leq \lVert u-v \rVert_2 \lVert x_u - x_v \rVert_2
\end{align}\]
</blockquote>

<p>따라서, 다음과 같은 관계를 확인할 수 있다.</p>

<blockquote>
\[\lVert x_u - x_v \rVert_2 \leq \frac{1}{d} \lVert u-v \rVert_2\]
</blockquote>

<p>이로써 \(\nabla f^{\ast}\) Lipschitz with parameter \(1/d\)이 증명되었다.</p>

<h2 id="convergence-guarantees">Convergence guarantees</h2>
<p>위 결과와 gradient descent를 결합하여, dual objective의 최적해로의 수렴성을 다음과 같이 설명할 수 있다.</p>

<ul>
  <li>만약 \(f\)가 파라미터 \(d\)로 strongly convex 하면, step size \(t_k=d (k=1,2,3, \dots\))에 대해서, dual gradient ascent는 \(O(1/\epsilon)\)으로 converge한다.</li>
  <li>만약 \(f\)가 파라미터 \(d\)로 strongly convex 하고, \(\nabla f\)는 파라미터 \(L\)로 Lipschitz하면, step size \(t_k=2/(1/d + 1/L)\) (\(k=1,2,3, \dots\))에 대해서, dual gradient ascent는 \(O(\log(1/\epsilon))\)으로 converge한다. (linear convergence)</li>
</ul>


        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>20-02 Dual Decomposition</h1>
            <p>본 절에서는 dual을 이용하여 문제를 decomposition하는 기법에 대해 알아본다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>20-02-01 Dual Decomposition with Equality Constraint</h1>
            <p>다음의 문제를 보자.</p>
<blockquote>
  <p>\begin{equation}
\min_x \sum_{i=1}^B f_i(x_i) \quad \text{ subject to } \quad Ax = b
\end{equation}</p>
</blockquote>

<p>만약, 변수 \(x\)를 \(B\)개의 블록으로 분할하고, \(x = (x_1,\dots,x_B) \in \mathbb{R}^n, \text{ where } x_i \in \mathbb{R}^{n_i}\), matrix \(A\) 역시 \(B\)개의 sub-matrix 블록으로 다음과 같이 분할하면, \(A = [A_1, \dots, A_B], \text{ where } A_i \in \mathbb{R}^{m \times n_i}\), 위 minimization 문제는 다음과 같이 \(B\)개의 분리된 문제로 분해될 수 있다.</p>
<blockquote>
\[\begin{alignat}{1}
&amp; \quad x^+ \in \arg\min_x \sum_{i=1}^B f_i(x_i) + u^T Ax  \\
\Longleftrightarrow &amp; \quad x_i^+ \in \arg\min_{x_i} f_i(x_i) + u^T A_ix_i, \quad i=1,\dots, B
\end{alignat}\]
</blockquote>

<h4 id="dual-decomposition-알고리즘">Dual decomposition 알고리즘:</h4>

<blockquote>
\[\begin{alignat}{1}
x_i^{(k)} &amp; \in \arg \min_{x_i} f_i(x_i) + (u^{(k-1)})^T A_i x_i, \quad i=1,\dots,B  \\
u^{(k)}   &amp; = u^{(k-1)} + t_k \left(\sum_{i=1}^B A_i x_i^{(k)} - b \right)
\end{alignat}\]
</blockquote>

<p>위 두 단계는 아래와 같이 해석할 수 있다.</p>
<blockquote>
  <ul>
    <li>첫번째 수식은 broadcast 단계로서, \(B\)개의 프로세서의 각각에게 \(u\)를 보낸다. 그리고, 프로세서 각각은 병렬로 자신의 최적 \(x_i\)를 찾는다.</li>
    <li>두번째 수식은 gather 단계로서, 각 프로세서로부터 \(A_i x_i\)를 모은다. 그리고 global dual 변수 \(u\)를 업데이트 한다.</li>
  </ul>
</blockquote>

<p>위 두 단계는 \(k=1,2,3,\dots\)에 대해 계속 반복한다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter20/decomposition.png" alt="[Fig 1] Broadcast and Gather" width="70%" />
  <figcaption style="text-align: center;">[Fig 1] Broadcast and Gather</figcaption>
</p>
</figure>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_5"></a>20-02-02 Dual Decomposition with Inequality Constraint</h1>
            <p>다음의 문제를 생각해 보자. 앞의 문제와 다른점은 제약식이 부등식의 관계를 갖는 것이다.</p>
<blockquote>
\[\begin{equation}
\min_x \sum_{i=1}^B f_i(x_i) \quad \text{subject to} \quad \sum_{i=1}^B A_i x_i \leq b
\end{equation}\]
</blockquote>

<h2 id="dual-decomposition-projected-subgradient-method">Dual decomposition (projected subgradient method)</h2>
<p>위 문제에서는 dual 변수가 항상 \(0\)보다 같거나 커야 한다, 즉 \(u \geq 0\). 따라서, 다음 스텝의 \(u\)값을 계산할 때, \(0\)보다 큰 범위안으로 projection을 시켜서 업데이트를 한다.</p>

<blockquote>
\[\begin{alignat}{1}
x_i^{(k)} &amp; \in \arg \min_{x_i} f_i(x_i) + (u^{(k-1)})^T A_i x_i, \quad i=1,\dots,B  \\
u^{(k)}   &amp; = u^{(k-1)} + t_k \left(\sum_{i=1}^B A_i x_i^{(k)} - b \right)_+
\end{alignat}\]
</blockquote>

<p>여기서, \(u_{+}\)는 0보다 큰 \(u\)를 의미한다, 즉, \((u_+)_i = \max \left\{0,u_i \right\}, i=1,\dots,m\). 
위  과정을 \(k=1,2,3,\dots\)에 대해서 반복한다.</p>

<h4 id="price-coordination-interpretation">Price coordination interpretation</h4>
<p>일반적으로 dual decomposition 문제들은 price coordination 관점에서 다음과 같이 해석될 수 있다. (Vandenberghe)</p>
<blockquote>
  <ul>
    <li>\(B\)개의 독립적인 유닛이 있고, 각 유닛은 자신의 결정 변수 \(x_i\)를 결정한다.</li>
    <li>각 제약조건은 \(B\)개의 유닛이 공유하고 있는 자원에 대한 제약을 의미하며, dual 변수 \(u_j\)는 자원 \(j\)의 가격을 의미한다.</li>
    <li>Dual 변수는 아래와 같이 업데이트되며
 \begin{equation}
 u_j^{+} = (u_j - t s_j)_{+}, \quad  j=1,\dots,m
 \end{equation}</li>
  </ul>

  <p>\(\quad\) 여기서, \(s=b-\sum_{i=1}^B A_ix_i\)는 슬랙 변수로써 <br />
\(\qquad\) - \(s_j &lt; 0\)이면, 자원 \(j\)가 over-utilized 되고 있다는 의미이고, 따라서, price \(u_j\)를 증가시킨다 <br />
\(\qquad\) - \(s_j &gt; 0\)이면, 자원 \(j\)가 under-utilized되고 있다는 의미이고,  따라서, price \(u_j\)를 감소시킨다 <br />
\(\qquad\) - price는 향상 음수가 되지 않도록 한다.</p>
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_6"></a>20-03 Augmented Lagrangians</h1>
            <p>Dual ascent의 단점은 수렴을 보장하기 위해 강한 조건이 필요하다는 것이다. (수렴을 보장하려면 \(f\)가 strongly convex해야 했다.) 이런 단점은 <strong>Augmented Lagrangian method</strong> (또는 <strong>Method of multipliers</strong>)에 의해 개선될 수 있다.</p>

<p>Primal 문제를 아래와 같이 변환한다.</p>
<blockquote>
  <p>\begin{equation}
\min_x f(x) + \frac{\rho}{2} \lVert Ax - b \rVert _2^2 \quad \text{ subject to } \quad Ax = b
\end{equation}</p>
</blockquote>

<p>여기서 \(\rho &gt; 0\)이다. \(A\)가 full column rank를 갖는다면 목적식은 strongly convex하다. 이는 원래의 문제와 정확히 동일한 문제가 된다. (Augmented term인 \(Ax - b\)는 0이 되기 때문이다.)</p>

<h2 id="augmented-lagrangian-method">Augmented Lagrangian Method</h2>
<p><strong>Dual gradient ascent</strong> : \(k=1,2,3,\dots\)에 대해 다음을 반복한다.</p>
<blockquote>
\[\begin{alignat}{1}
x^{(k)} &amp; \in \arg\min_x f(x) + (u^{(k-1)})^T A x + \frac{\rho}{2} \lVert Ax - b \rVert_2^2  \\
u^{(k)} &amp; = u^{(k-1)} + \rho (A x^{(k)} - b)
\end{alignat}\]
</blockquote>

<p>위 dual 알고리즘에서 \(\rho\)는 step size 역할을 한다, 즉 \(t_k=\rho\)이다. 이것은 다음에서 그 이유를 알 수 있다.</p>

<h4 id="rho가-step-size일-때-optimality-증명">\(\rho\)가 step size일 때 optimality 증명</h4>

<p>\(x^{(k)}\)는 \(f(x) + (u^{(k-1)})^T Ax + \frac{\rho}{2} \lVert Ax - b\rVert _2^2\) 를 최소화하므로, 
원래 primal 문제에 대한 stationary 조건에 따라, \(x^{(k)}\)에서 목적식의 subgradient가 아래와 같이 \(0\)을 포함해야 한다.</p>

<blockquote>
\[\begin{alignat}{1}
0 &amp; \in \partial f(x^{(k)}) + A^T (u^{(k-1)}) + \rho (A x^{(k)} -b))  \\
  &amp; = \partial f(x^{(k)}) + A^T u^{(k)}
\end{alignat}\]
</blockquote>

<p>위식에서, \(u^{(k)} = u^{(k-1)} + \rho (A x^{(k)} - b)\)로 동작하게 되면, 적당한 조건하에서 \(Ax^{(k)}-b\)가 \(0\)으로 가까워지면서 feasible한 해를 제공하기 시작하고, 궁극적으로 KKT 조건이 만족되고, \(x^{(k)}\)와 \(u^{(k)}\)가 optimality에 근접함을 보일 수 있다.</p>

<p><strong>Augmented Lagrangian method</strong>의 장점은 훨씬 좋은 수렴성을 갖는다는 것이고, 단점은 문제를 분해할 수 있는 decomposability를 잃는다는 것이다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_7"></a>20-04 A peak at ADMM</h1>
            <h2 id="lipschitz-gradients-and-strong-convexity">Lipschitz gradients and strong convexity</h2>
<p>본 절에서는 Alternating Direction Method of Multipliers (ADMM) 기법의 개요에 대해 알아본다. 앞에서 augmented Lagrangian 방법이 decomposability를 제공하지 못했지만, ADMM은 수렴성과 함께  decomposability를 제공하는 method이다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_8"></a>20-04-01 ADMM</h1>
            <p>다음 문제를 보자.</p>
<blockquote>
  <p>\begin{equation}
\min_x f(x) + g(z) \quad \text{subject to} \quad Ax + Bz = c
\end{equation}</p>
</blockquote>

<p>앞에서처럼, 목적식을 다음과 같이 확장할 수 있다.</p>
<blockquote>
  <p>\begin{equation}
\min_x f(x) + g(z) + \frac{\rho}{2} \lVert Ax + Bz - c \rVert_2^2 \quad \text{subject to} \quad Ax + Bz = c
\end{equation}</p>
</blockquote>

<p>여기서, \(\rho &gt; 0\) 이다.</p>

<p>그리고, augmented Lagrangian을 다음처럼 정의할 수 있다.</p>
<blockquote>
  <p>\begin{equation}
L_{\rho} (x,z,u) = f(x) + g(z) + u^T(Ax + Bz - c) + \frac{\rho}{2} \lVert Ax + Bz - c \rVert_2^2
\end{equation}</p>
</blockquote>

<p>ADMM은 \(k=1,2,3 \dots\)에 대해서 다음의 step을 수행한다.</p>
<blockquote>
\[\begin{alignat}{1}
x^{(k)} &amp; = \arg\min_x  L_{\rho} (x,z^{(k-1)},u^{(k-1)}) \\
z^{(k)} &amp; = \arg\min_z  L_{\rho} (x^{(k)},z,u^{(k-1)}) \\
u^{(k)} &amp; = u^{(k-1)} + \rho (Ax^{(k)} + Bz^{(k)} - c) 
\end{alignat}\]
</blockquote>

<p>첫번째 식에서 구한 \(x^{(k)}\)가 \(z^{(k)}\)에 이용된다는 점은 매우 중요하다. 만일, 이렇게 하지 않으면 수렴되지 않을 수 있다.</p>

<p>일반 <strong>Method of multiplier</strong>에서는 처음 두 스텝이 다음의 joint 최소화로 바뀌게 된다는 점을 주의하자.</p>
<blockquote>
  <p>\begin{equation}
(x^{(k)}, z^{(k)}) = \arg\min_{x,z} L_{\rho} (x,z,u^{(k-1)})   <br />
\end{equation}</p>
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_9"></a>20-04-02 Converegence Guarantee</h1>
            <p>\(f\)와 \(g\)에 대한 적당한 조건 아래에서 (A와 B가 full rank일 필요는 없다), ADMM은 모든 \(\rho &gt; 0\)에 대해서 다음을 만족한다.</p>

<ul>
  <li><strong>Residual convergence</strong>: \(k\)가 \(\infty\)로 갈 때, \(r^{(k)} = A x^{(k)} - B z^{(k)} - c \to 0\), 즉 primal iteration이 feasibility로 접근한다.</li>
  <li><strong>Objective convergence</strong>: \(f(x^{(k)} + g(x^{(k)} \to f^{\ast} + g^{\ast}\), 여기서 \(f^{\ast} + g^{\ast}\)는 최적의 primal objective 값이다.</li>
  <li><strong>Dual convergence</strong>: \(u^{(k)} \to u^{\ast}\), 여기서 \(u^{\ast}\)는 dual solution 이다.</li>
</ul>

<p>정확한 수렴속도는 아직 알려지지 않았으며, 현재 많은 연구가 진행중이다. 대략적으로는 first-order method 와 비슷하거나 약간 더 빠르게 동작한다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_10"></a>20-04-03 ADMM in Scaled Form</h1>
            <p>ADMM은 dual 변수 \(u\)를 \(w=u/\rho\)로 바꾸어서 scaled form으로 표현할 수 있다. 그러면, ADMM step은 다음과 같이 나타낼 수 있다.</p>
<blockquote>
\[\begin{alignat}{1}
x^{(k)} &amp; = \arg\min_x f(x) + \frac{\rho}{2} \lVert Ax + Bz^{(k-1)} - c + w^{(k-1)} \rVert_2^2  \\
z^{(k)} &amp; = \arg\min_z g(x) + \frac{\rho}{2} \lVert Ax^{(k)} + Bz - c + w^{(k-1)} \rVert_2^2  \\
w^{(k)} &amp; = w^{(k-1)} + Ax^{(k)} + Bz^{(k)} - c 
\end{alignat}\]
</blockquote>

<p>위의 식은 다음의 과정을 통해 원래 식과 같다는 것을 알 수 있다.</p>

<blockquote>
\[\begin{align}
x^{(k)} &amp; = \arg\min_x f(x) + \frac{\rho}{2} \lVert Ax + Bz^{(k-1)} - c + w^{(k-1)} \rVert_2^2  \\
&amp; = \arg\min_x f(x)  + \frac{\rho}{2} \lVert Ax + Bz^{(k-1)} - c \rVert_2^2  + 2 \frac{\rho}{2} w^{(k-1)} (Ax + Bz^{(k-1)} - c)  + \lVert w^{(k-1)} \rVert_2^2 \\
&amp; = \arg\min_x f(x)  + \frac{\rho}{2} \lVert Ax + Bz^{(k-1)} - c \rVert_2^2  + u^{(k-1) } (Ax + Bz^{(k-1)} - c) \\
\end{align}\]
</blockquote>

<p>여기서, \(w^{(k)}\)은  \(k\)번째 residual의 합으로 볼 수도 있다.</p>

<blockquote>
\[\begin{equation}
w^{(k)} = w^{(0)} + \sum_{i=1}^k (Ax^{(i)} + Bz^{(i)} - c) 
\end{equation}\]
</blockquote>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_11"></a>20-04-04 Example - Alternating Projection</h1>
            <p>Convex set \(C,D \in \mathbb{R}^n\)의 교집합의 한 점을 찾는 문제를 고려해 보자.</p>
<blockquote>
  <p>\begin{equation}
\min_x I_C(x) + I_D(x)  <br />
\end{equation}</p>
</blockquote>

<p>위 문제를 ADMM형태로 바꾸기 위해, 아래와 같이 표현한다.</p>
<blockquote>
\[\begin{equation}
\min_{x,z} I_C(x) + I_D(x) \quad \text{subject to} \quad x - z = 0   
\end{equation}\]
</blockquote>

<p>각 ADMM cycle은 두개의 projection을 포함한다.</p>
<blockquote>
\[\begin{alignat}{1}
x^{(k)} &amp; = \arg\min_x P_C \left( z^{(k-1)} - w^{(k-1)} \right) \\
z^{(k)} &amp; = \arg\min_z P_D \left( x^{(k)} + w^{(k-1)} \right) \\
w^{(k)} &amp; = w^{(k-1)} + x^{(k)} + z^{(k)}
\end{alignat}\]
</blockquote>

<p>위의 식에서 \(x^{(k)}\)는 다음과 같이 도출된 것이다.</p>

<blockquote>
\[\begin{alignat}{1}
x^{(k)} &amp; = \arg\min_x I_C(x) + \frac{\rho}{2} \lVert x - z^{(k-1)} + w^{(k-1)} \rVert_2^2 \\
&amp; = \arg\min_x P_C \left( z^{(k-1)} - w^{(k-1)} \right) \\
\end{alignat}\]
</blockquote>

<p>위의 식에서 \(z^{(k)}\)는 다음과 같이 도출된 것이다.</p>

<blockquote>
\[\begin{alignat}{1}
z^{(k)} &amp; = \arg\min_x I_D(z) + \frac{\rho}{2} \lVert x^{(k-1)} - z + w^{(k-1)} \rVert_2^2 \\
&amp; = \arg\min_z P_D \left( x^{(k)} + w^{(k-1)} \right) \\
\end{alignat}\]
</blockquote>

<p>위 방법은 기존 alternating projection method와 비슷하지만 더 효율적이다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_12"></a>20-05 References</h1>
            <h3 id="references">References</h3>

<ul>
  <li>S. Boyd, N. Parikh, E. Chu, B. Peleato and J. Eckstein (2010), “Distributed optimization and statistical learning via the alternating direction method of multipliers”</li>
  <li>W. Deng and W. Yin (202), “On the global and linear convergence of the generalized alternating direction method of multipliers”</li>
  <li>M. Hong and Z. Luo (2012), “On  the linear convergence of the alternating direction  method of multipliers”</li>
  <li>F. lutzeler, P. Bianchi, Ph. Ciblat, and W. Hachem (2014), “Linear convergence rate for distributed optimization with the alternating direction method of multipliers”</li>
  <li>R. Nishihara, L. Lessard, B.  Recht, A. Packard, and M. Jordan (2015), “A general analysis of the convergence of ADMM”</li>
  <li>L. Vandenberghe, Lecture Notes for EE 236C, UCLA, Spring 2011-2012</li>
</ul>

        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
