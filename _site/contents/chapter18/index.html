<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
@media print {

    .container {
        padding: 0;
        max-width: unset;
        break-after: page;
        break-before: page;
    }
    .content {
        padding: 0;
    }
    .masthead {
        display: none;
    }
    .sidebar-toggle {
        display: none;
    }


}
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      Quasi-Newton Methods &middot; 모두를 위한 컨벡스 최적화
    
  </title>

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="/public/css/github-markdown.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="https://convex-optimization-for-all.github.io/public/logo.png">
  <link rel="shortcut icon" href="https://convex-optimization-for-all.github.io/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://convex-optimization-for-all.github.io/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-189737072-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>모두를 위한 컨벡스 최적화</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/">Home</a>

    

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter01/">01. Introduction</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter02/">02. Convex Sets</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter03/">03. Convex Functions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter04/">04. Convex Optimization Basis</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter05/">05. Canonical Problems</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter06/">06. Gradient Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter07/">07. Subgradient</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter08/">08. Subgradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter09/">09. Proximal Gradient Descent and Acceleration</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter10/">10. Duality in Linear Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter11/">11. Duality in General Programs</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter12/">12. KKT Conditions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter13/">13. Duality uses and correspondences</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter14/">14. Newton's Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter15/">15. Barrier Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter16/">16. Duality Revisited</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter17/">17. Primal-Dual Interior-Point Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item active" href="https://convex-optimization-for-all.github.io/contents/chapter18/">18. Quasi-Newton Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter19/">19. Proximal Netwon Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter20/">20. Dual Methods</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter21/">21. Alternating Direction Method of Mulipliers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter22/">22. Conditional Gradient Method</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter23/">23. Coordinate Descent</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter24/">24.  Mixed Integer Programming 1</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/contents/chapter25/">25.  Mixed Integer Programming 2</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://convex-optimization-for-all.github.io/reference/">26. Reference</a>
        
      
    

    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">모두를 위한 컨벡스 최적화</a>
            <small></small>
          </h3>
          <a class="github-logo__wrapper" target="_blank" href="https://github.com/convex-optimization-for-all/convex-optimization-for-all.github.io" titltle="Github">
           <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
          </a>
        </div>
      </div>

      <div class="container content">
        <h1>18. Quasi-Newton Methods</h1>






<!-- Get first post and show it -->

<p>1950년대 중반, Argonne 국립 연구소에서 근무 중이었던 물리학자 W.C. Davidon은 coordinate descent method를 이용하여 계산량이 큰 최적화 문제를 풀고 있었다. 불행하게도 당시의 컴퓨터가 불안정했던 탓에 계산이 끝나기도 전에 시스템의 충돌이 빈번히 일어났고, 이에 좌절한 Davidon은 계산속도를 좀 더 향상시킬 수 있는 방법을 찾기로 결심하게 된다. 그렇게 탄생하게 된 것이 바로 최초의 Quasi-Newton 알고리즘이다. 이는 nonlinear optimization을 극적으로 진보시키는 계기가 되었으며, 뒤이어 20여 년 동안 이 방법에 대한 다양한 후속연구들이 등장하였다.</p>

<p>아이러니하게도 <a href="http://www.math.mcgill.ca/dstephens/680/Papers/Davidon91.pdf">Davidon의 연구</a>는 당시 출판되지 못하고 30년 이상을 technical report로 남아있었다. 그리고 마침내 1991년이 되어서야 <a href="https://epubs.siam.org/toc/sjope8/1/1">SIAM Jounal on Optimization의 첫 번째 판</a>에 실리게 되었다.</p>

<p>Quasi-Newton methods는 각 반복(iterateration)에서 objective function에 대한 gradient만을 필요로 한다. 이는 이차 미분을 필요로하는 newton methods보다 계산적인 부담이 훨씬 적으며 더불어 superlinear convergence를 보인다는 점에서 충분히 매력적인 방법이라고 볼 수 있다 [14].</p>

<h2 id="motivation-for-quasi-newton-methods">Motivation for Quasi-Newton methods</h2>

<p>다음과 같은 unconstrained, smooth optimization problem이 있다고 해보자.</p>
<blockquote>
\[\min_x \: f(x), \\\\
\text{where } f \text{ is twice differentiable, and } dom \; f = \mathbb{R}^n.\]
</blockquote>

<p>위 문제에 대한 Gradient descent method와 Newton’s method에서의 x에 대한 업데이트 방법은 각각 아래와 같다.</p>
<blockquote>
  <p><strong>Gradient descent method:</strong>
\(x^+ = x - t \nabla f(x)\)</p>
</blockquote>

<blockquote>
  <p><strong>Newton’s method:</strong>
\(x^+ = x - t \nabla^2 f(x)^{-1} \nabla f(x)\)</p>
</blockquote>

<p>Newton’s method는 quadratic convergence rate (\(O(\log \log 1/ \epsilon)\))의 수렴속도를 보이는 장점이 있는 반면에 아래 두 과정에 의해 상당히 큰 계산비용이 발생하는 문제가 있다.</p>

<ul>
  <li>Hessian \(\nabla^2 f(x)\)의 계산: Hessian matrix를 계산하고 저장하기 위해서는 \(O(n^2)\)의 메모리를 필요로 한다. 이는 고차원의 함수를 다루기에 적절하지 않은 성능이다. (참고: <a href="https://en.wikipedia.org/wiki/Hessian_matrix#Use_in_optimization">Hessian matrix</a> in Wikipedia)</li>
  <li>방정식 \(\nabla^2 f(x) p = -\nabla f(x)\)의 풀이: 이 방정식을 풀기 위해서는 Hessian \(\nabla^2 f(x)\)에 대한 역행렬을 계산해야 한다. (참고: <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra">Computational complexity of Matrix algebra</a> in Wikipedia)</li>
</ul>

<p>Quasi-Newton method에서는 대신 \(\nabla^2 f(x)\)를 근사(approximation)한 \(B\)를 이용한다.</p>
<blockquote>
  <p><strong>Quasi-Newton method:</strong>
\(x^+ = x + tp \\\\
\text{where } Bp = -\nabla f(x).\)</p>
</blockquote>

<p>이때 B는 계산하기 쉬워야 하며, 또한 방정식 \(Bp = g\)를 풀기에도 용이해야 한다.</p>

<h2 id="quasi-newton-algorithm">Quasi-Newton Algorithm</h2>
<p>Quasi-Newton algorithm은 다음과 같다.</p>

<ul>
  <li>Pick initial \(x^0\) and \(B^0\)</li>
  <li>For \(k = 0, 1, \dots\)
    <ul>
      <li>Solve \(B^k p^k = - \nabla f(x^k)\)</li>
      <li>Pick \(t_k\) and let \(x^{k+1} = x^{k} + t_k p^k\)</li>
      <li>Update \(B^k\) to \(B^{k+1}\)</li>
    </ul>
  </li>
  <li>End for</li>
</ul>

<p>Optimal point에 점진적으로 다가갈 수 있도록 \(B\)를 업데이트 해가는 것이 이 방법의 큰 특징이다. 즉, \(B\)를 통해 next step인 \(B^+\)를 구하는 방법에 대해 이번 장에서 주로 논의하게 될 것이다. (<strong>Note:</strong> 편의상 \(B^{k+1}, B^k\)와 \(B^+, B\) 두 가지 표기를 혼용하여 사용하겠다.)</p>


<!-- Remove first element from post_list which is already shown above. -->
  

<!-- List up the posts in the chapter -->
<ul style="list-style: none;">

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_1">18-01 Secant Equation and Curvature Condition</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_2">18-02 Symmetric Rank-One Update (SR1)</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_3">18-03 Davidon-Fletcher-Powell (DFP) Update</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_4">18-04 Broyden-Fletcher-Goldfarb-Shanno (BFGS) update</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_5">18-05 The Broyden Class</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_6">18-06 Superlinear convergence</a>
    </li>
  
  

  
  
  
  
    <li style="text-align:left; vertical-align: middle; margin-left: -2em; margin-top: 5px;" >
      <a href="#_page_7">18-07 Limited Memory BFGS (LBFGS)</a>
    </li>
  
  

</ul>


<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_1"></a>18-01 Secant Equation and Curvature Condition</h1>
            <h2 id="secant-equation">Secant Equation</h2>
<p>앞서 \(B\)는 \(\nabla^2 f(x)\)를 근사하는 행렬이라고 했다. 행렬 \(B\)가 Hessian \(\nabla^2 f(x)\)와 비슷한 성질을 갖기 위해서는 secant equation이라는 조건을 만족해야 한다. \(x^{k+1} = x^k + s^k\)이고 \(f\)가 두 번 이상 미분 가능할 때, \(\nabla f(x^k + s^k)\)에 대한 first-order Taylor expansion은 true Hessian이 다음의 성질을 가짐을 보인다.</p>

<blockquote>
\[\nabla f(x^k + s^k)  \approx \nabla f(x^k) + \nabla^2 f(x^k) s^k\]
</blockquote>

<p>이때 \(\nabla^2 f(x^k)\)에 대한 근사 행렬을 \(B^{k+1}\)이라 한다. 이 행렬은 다음의 등식을 만족시킨다.</p>

<blockquote>
\[\nabla f(x^k + s^k)  = \nabla f(x^k) + B^{k+1} s^k\]
</blockquote>

<p>\(x^{k+1} = x^k + s^k, y^k = \nabla f(x^{k + 1})  - \nabla f(x^k)\)이면 위 등식은 아래와 같이 정리되고, 이를 secant equation이라 부른다.</p>

<blockquote>
\[B^{k+1} s^k = y^k\]
</blockquote>

<h2 id="the-intuition-of-secant-equation">The Intuition of Secant Equation</h2>

<p>\(x\)축은 \(x^k\)를, \(y\)축은 \(\nabla f(x^k)\)를 나타낸다고 할때 \(B^{k+1}\)은 \((x^k, \nabla f(x^k))\)와 \((x^{k+1}, \nabla f(x^{k+1}))\)를 통과하는 직선의 기울기와 같다.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter18/intuition_of_secant_eq.png" alt="[Fig1] The intuition of secant equation" width="70%" />
  <figcaption style="text-align: center;">[Fig1] The intuition of secant equation</figcaption>
</p>
</figure>

<h2 id="conditions-to-determine-b">Conditions to Determine \(B^+\)</h2>
<p>행렬 \(B\)를 기반으로 계산된 \(B^+\)는 다음의 3가지 조건을 만족해야한다.</p>

<ol>
  <li>\(B^+\) is symmetric: Hessian에 대한 추정이기 때문이다.</li>
  <li>\(B^+\)  close to \(B\): 유일한 \(B^+\)를 결정하기 위한 조건. \(B\)가 이미 유용한 정보를 가지고 있으므로 secant equation을 만족하는 \(B^+\) 중에서 \(B\)와 최대한 가까운 행렬을 고른다.</li>
  <li>\(B\) is positive definite \(\Rightarrow B^+\) is positive definite: Global optimum을 보장하기 위해서 문제의 convexity를 유지한다. (참고: <a href="https://web.stanford.edu/group/sisl/k12/optimization/MO-unit4-pdfs/4.10applicationsofhessians.pdf">Analyzing the hessian</a>)</li>
</ol>

<h2 id="curvature-condition">Curvature Condition</h2>
<p>\(B^+\)가 positive definite이면서 \(B^+ s = y\)라는 것은 다음의 사실을 암시한다.</p>
<blockquote>
\[s^T y = s^T B^+ s &gt; 0.\]
</blockquote>

<p>(참고: <a href="https://en.wikipedia.org/wiki/Positive-definite_matrix">positive definite in WikiPedia</a>)</p>

<p>여기서 \(s^T y &gt; 0\)을 curvature condition이라 부른다. Curvature condition을 만족하면, secant equation \(B^+ s = y\)은 항상 solution(\(B^+\))을 갖는다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_2"></a>18-02 Symmetric Rank-One Update (SR1)</h1>
            <p>SR1 update는 rank-1의 symmetric matrix로 \(B\)를 업데이트 함으로써 \(B^+\)가 symmetry를 유지하고 secant equation을 계속해서 만족하게끔 업데이트하는 방법이다. Rank-1의 symmetric matrix가 \(a \in \left\{-1, 1\right\}\)와 \(u \in \mathbb{R^n}\)의 곱으로 분해된다고 하면 update form은 다음과 같을 것이다.</p>

<blockquote>
\[B^+ = B + auu^T.\]
</blockquote>

<h2 id="key-observation">Key Observation</h2>
<p>\(a\)와 \(u\)는 \(B^+\)가 secant equation을 만족하게끔 선택되어야 한다. 그러므로 secant equation \(y = B^+s\)에 위에서 소개한 update form을 대입해보자.</p>

<blockquote>
\[y = B^+s \Rightarrow y = Bs + (au^Ts)u. \quad \text{--- (1)}\]
</blockquote>

<p>\((au^Ts)\)는 scalar이므로 \(u\)는 \(y-Bs\)와 임의의 scalar \(\delta\)와의 곱으로도 표현할 수 있을 것이다 \(\big( u = \delta(y - Bs) \big)\). (1)의 \(u\)를 \(\delta(y - Bs)\)으로 치환해보자.</p>

<blockquote>
\[y-Bs = a\delta^2 \big[ s^T(y - Bs) \big] (y -Bs),\]
</blockquote>

<p>위 등식을 만족하는 파라미터 \(\delta\)와 \(a\)는 다음과 같다.</p>

<blockquote>
\[a = \text{sign} \big[ s^T (y - Bs) \big], \quad \delta = \pm | s^T(y-Bs) |^{-1/2}. \quad \text{--- (2)}\]
</blockquote>

<h2 id="the-only-sr1-updating-formula">The Only SR1 Updating Formula</h2>
<p>Key observation에서 얻은 정보를 활용해 유일한 형태의  SR1 update를 유도할 수 있다 ([14]의 6.2). <br />
\(\big( u = \delta (y - Bs)\) 와 (2)를 \(B^+ = B + auu^T\)에 대입. \(\big)\)</p>

<blockquote>
\[B^+ = B + \frac{(y-Bs)(y-Bs)^T}{(y-Bs)^Ts}.\]

</blockquote>

<h2 id="the-update-formula-for-the-inverse-hessian-approximation">The Update Formula for the Inverse Hessian Approximation</h2>

<p>\(x^+\)를 구하기 위해서는 \(B^{-1}\)의 계산이 필요해진다.</p>

<blockquote>
\[x^+ = x + tp = x - tB^{-1}\nabla f(x)\]
</blockquote>

<p>\(B\) 대신 \(B^{-1}\)를 업데이트 할 수 있다면 매번 \(B^{-1}\)을 계산하기 위한 비용을 줄일 수 있지 않을까?</p>

<p><a href="https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula">Sherman–Morrison formula</a>를 이용하면 유도과정을 통해 \(B^{-1}\) 또한 동일한 형태로 업데이트 할 수 있다는 것을 알 수 있다. (\(H = B^{-1}\))</p>

<blockquote>
\[H^+ = H + \frac{(s-Hy)(s-Hy)^T}{(s-Hy)^Ty}.\]
</blockquote>

<h2 id="shortcomings-of-sr1">Shortcomings of SR1</h2>

<p>SR1 은 아주 간단하다는 장점이 있지만 두 가지 치명적인 단점을 가지고 있다.</p>

<ol>
  <li>\((y-Bs)^Ts\)가 0에 가까워지면 업데이트에 실패할 수 있다.</li>
  <li>\(B\)와 \(H\)의 positive definiteness를 유지하지 못한다.</li>
</ol>

<p>이 뒤의 절에서는 SR1의 단점을 보완한 방법들을 소개한다.</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_3"></a>18-03 Davidon-Fletcher-Powell (DFP) Update</h1>
            <p>DFP update는 rank-2의 symmetric matrix로 \(H (=B^{-1})\)를 업데이트 하는 방법이다.</p>

<blockquote>
\[H^+ = H + auu^T + bvv^T.\]
</blockquote>

<p>DFP update를 통해 계산된 \(H^+\)가 secant equation을 만족한다면, \(s-Hy\)은 \(u\)와 \(v\)의 linear combination으로 표현할 수 있다. (참고: secant equation에 의해, \(B^+ s =y \Leftrightarrow H^+ y = s\))</p>

<blockquote>
\[H^+y = Hy + auu^Ty + bvv^Ty = Hy + (au^Ty)u + (bv^Ty)v = s\]

\[\Rightarrow s - Hy = (au^Ty)u + (bv^Ty)v\]
</blockquote>

<p>\(u=s, v=Hy\)로 두고 a와 b에 대해 풀면 \(H\)에 대한 updating formula가 유도된다.</p>
<blockquote>
\[H^+ = H - \frac{Hyy^TH}{y^THy} + \frac{ss^T}{y^Ts}\]
</blockquote>

<p>SR1 update에서와 마찬가지로 <a href="https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula">Sherman–Morrison formula</a>를 이용하여 \(B\)에 대한 updating formula를 유도할 수 있다.</p>

<blockquote>
\[\begin{align}
B^+ &amp;= B + \frac{(y-Bs)y^T}{y^Ts} + \frac{y(y-Bs)^T}{y^Ts} - \frac{(y-Bs)^Ts}{(y^Ts)^2} yy^T\\\\
&amp;= \big( I - \frac{ys^T}{y^Ts} \big) B \big( I - \frac{sy^T}{y^Ts} \big) + \frac{yy^T}{y^Ts} 
\end{align}\]
</blockquote>

<p>만약 \(B\)가 positive definite이면 \(\big( I - \frac{ys^T}{y^Ts} \big) B \big( I - \frac{sy^T}{y^Ts} \big)\)는 positive semidefinite이 된다. 이때 \(\frac{yy^T}{y^Ts}\)가 positive definite이면 \(B^+ = \big( I - \frac{ys^T}{y^Ts} \big) B \big( I - \frac{sy^T}{y^Ts} \big) + \frac{yy^T}{y^Ts}\)는 positive definite임이 보장된다. 이로써 SR1에서 제기 되었던 positive definiteness의 지속성 문제가 해결된다.</p>

<h2 id="dfp-update---alternate-derivation">DFP Update - Alternate Derivation</h2>

<p>Recall: curvature condition(\(y^Ts &gt; 0, y,s \in \mathbb{R}^n\))을 만족하면 secant equation을 만족하는 symmetric positive definite matrix가 존재한다.</p>

<p>DFP update는 1. symmetry를 만족하고, 2. secant equation을 만족하는 행렬 \(B^+\)와 \(B\)의 weighted Frobenius norm을 최소화 시키는 문제를 푸는 것으로도 유도된다. (각각의 다른 matrix norm은 각각의 다른 Quasi-Newton method와 연결된다. 그 중에서 이 문제의 solution을 구하기 쉽게 하면서도 scale-invariant optimization method로 작동하게끔 하는 norm이 바로 weighted Frobenius norm이다.)</p>

<blockquote>
  <p>Solve
\(\begin{align}
&amp; \min_{B^+} \: \: &amp;&amp; {\|W^{1/2} (B^+ - B) W^{1/2} \|_F} \\\\
&amp; \text{subject to } &amp;&amp; {B^+ = (B^+)^T} \\\\
   &amp;&amp;&amp; {B^+s = y}  \\\\
&amp; \text{where } &amp;&amp; W \in \mathbb{R}^{n \; \times \;n} \text{ is nonsingular and such that } Wy_k = s_k.
\end{align}\\\\\)</p>
</blockquote>

<p><strong>*참고</strong>:</p>

<ul>
  <li>
    <p>Frobenius norm: 행렬 \(A\)에 대한 Frobenius norm은 다음과 같이 정의된다.
\(\| A \|_{F}  \doteq ( \sum_{i,j} A_{i,j}^2 )^{1/2}\)</p>
  </li>
  <li>
    <p>Weighted Frobenius norm: 가중치 행렬 \(W(W \succ 0)\)에 대한 행렬 \(A\)의 weighted Frobenius norm은 다음과 같이 정의된다. 
\(\|A\|_W \doteq \| W^{1/2} A W^{1/2} \|_F\)</p>
  </li>
</ul>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_4"></a>18-04 Broyden-Fletcher-Goldfarb-Shanno (BFGS) update</h1>
            <p>BFGS의 아이디어는 DFP와 동일하다. 다만, B와 H의 역할이 뒤바뀌는 것만이 차이점이다.</p>

<p>BFGS는 다음 문제를 푸는 것으로 유도된다.</p>

<blockquote>
  <p>Solve
\(\begin{align}
&amp; \min_{H^+} \: \: &amp;&amp; {\|W^{1/2} (H^+ - H) W^{1/2} \|_F} \\\\
&amp; \text{subject to } &amp;&amp; {H^+ = (H^+)^T} \\\\
&amp;&amp;&amp; {H^+s = y}  \\\\
&amp; \text{where } &amp;&amp; W \in \mathbb{R}^{n \; \times \;n} \text{ is nonsingular and such that } Ws_k = y_k.
\end{align}\\\\\)</p>
</blockquote>

<p>유도되는 \(H\)와 \(B\) 에 대한 updating formula는 다음과 같다.</p>

<blockquote>
\[B^+ = B - \frac{Bss^TB}{s^TBs} + \frac{yy^T}{y^Ts}\]
</blockquote>

<p>and</p>

<blockquote>
\[\begin{align}
H^+ &amp;= H + \frac{(s-Hy)s^T}{y^Ts} + \frac{s(s-Hy)^T}{y^Ts} - \frac{(s-Hy)^Ty}{(y^Ts)^2} ss^T\\\\
&amp;= \big( I - \frac{sy^T}{y^Ts} \big) H \big( I - \frac{ys^T}{y^Ts} \big) + \frac{ss^T}{y^Ts} 
\end{align}\]
</blockquote>

<p>BFGS 또한 DFP처럼 positive definiteness를 유지한다. (만약 \(B\)가 positive definite이고 \(s^Ty &gt; 0\)이면 \(B^+\)는 positive definite이다.)</p>

<p>BFGS의 특장점은 self-correcting property를 지니고 있다는 것이다. 만약 행렬 \(H\)가 부정확하게 추정되어 iteration의 속도가 느려지게 되면 Hessian approximation이 단 몇 step 만에 이를 바로잡는 경향성을 보인다. 반면 DFP는 잘못된 Hessian approximation의 추정에 대해 효과적으로 바로잡지 못하므로 실전에서는 보통 BFGS의 성능이 더 좋은 편이다 [14].</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_5"></a>18-05 The Broyden Class</h1>
            <p>The Broyden class는 BFGS, DFP, SR1을 다음의 공식으로 일반화시킨다.</p>

<ul>
  <li><strong>Note:</strong> \(B^+_{\text{BFGS}}\)와 \(B^+_{\text{DFP}}\)는 각각 BFGS와 DFP에 의해 유도되는 \(B^+\)다.</li>
</ul>

<blockquote>
\[B^+ = (1 - \phi)B^+_{\text{BFGS}} + \phi B^+_{\text{DFP}}, \text{ for } \phi \in \mathbb{R}.\]
</blockquote>

<p>\(v\)를 \(\frac{y}{y^Ts} - \frac{Bs}{s^TBs}\)로 정의하면 위 공식은 아래와 같이 정리된다.</p>

<blockquote>
\[B^+ = B - \frac{Bss^TB}{s^TBs} + \frac{yy^T}{y^Ts} + \phi(s^TBs)vv^T.\]
</blockquote>

<p>Observe:</p>

<ul>
  <li>\(\phi =0\)일때, 위 update는 BFGS와 동일해진다.</li>
  <li>\(\phi =1\)일때, 위 update는 DFP와 동일해진다.</li>
  <li>\(\phi = \frac{y^Ts}{y^Ts - s^TBs}\)일때, 위 update는 SR1과 동일해진다.</li>
</ul>

<p><strong>*참고</strong>: \(\phi\)의 범위를 \([0,1]\)로 제한한 특수한 경우를 restricted Broyden class라 부른다 [14].</p>

        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_6"></a>18-06 Superlinear convergence</h1>
            <h4 id="assumption1">Assumption1:</h4>
<blockquote>
  <p>The Hessian matrix \(G\) is Lipschitz continuous at \(x^∗\), that is, 
\(\| G(x) − G(x^∗)  \le L \| x − x^∗ \|,\)
for all \(x\) near \(x^∗\), where \(L\) is a positive constant.</p>
</blockquote>

<h4 id="assumption2-wolfe-conditions">Assumption2: Wolfe conditions</h4>
<blockquote>
  <p>Assume \(t\) is chosen (via backtracking) so that
\(f(x + tp) \le f(x) + \alpha_1 t \nabla f(x)^T p\)
and
\(\nabla f(x + tp)^T p \ge \alpha_2 \nabla f(x)^T p\)
for \(0 &lt; \alpha_1 &lt; \alpha_2 &lt; 1.\)</p>
</blockquote>

<ul>
  <li>Wolfe conditions의 첫 번째 조건은 너무 큰 \(t\)가 선택되지 않게끔 한다.</li>
  <li>Wolfe conditions의 두 번째 조건은 너무 작은 \(t\)가 선택되지 않게끔 한다.</li>
</ul>

<p>DFP와 BFGS는 위 두 가정하에 superlinear convergence를 보인다. (참고: <a href="https://en.wikipedia.org/wiki/Rate_of_convergence">Rate of convergence in Wikipedia</a>)</p>
<blockquote>
\[\lim_{k \rightarrow \infty} \frac{ \| x^{k+1} - x^\ast \| }{ \| x^k - x^\ast \| } = 0.\]
</blockquote>

<h2 id="theorem-dennis-moré">Theorem (Dennis-Moré)</h2>

<p>다음은 Quasi-Newton method의 search direction이 Newton direction을 충분히 잘 근사하고 있을때, solution으로 수렴하는 과정에서 step length가 Wolfe conditions를 만족함을 보인다. Superlinear convergence를 보이기 위해 search direction이 만족해야하는 조건이라고도 할 수 있다 [14].</p>

<blockquote>
  <p>\(f\)가 두 번 미분 가능하고 \(x^k \rightarrow x^\ast\) s.t. \(\nabla f(x^\ast) = 0\)이며 \(\nabla^2 f(x^\ast)\)가 positive definite이라고 가정하자.</p>

\[\lim_{k \rightarrow \infty} \frac{\| \nabla f(x^k) + \nabla^2 f(x^k) p^k \| }{\| p^k \|} = 0.\]

  <p>만약 search direction \(p^k\)가 위 조건을 만족하면, 다음 두 가지 항목을 만족하는 \(k_0\)가 존재한다.</p>

  <ol>
    <li>\(k \ge k_0\)에 대해 step length \(t_k=1\)은 Wolfe conditions를 만족한다.</li>
    <li>만약 \(k \ge k_0\)에 대해 \(t_k = 1\)이면 \(x^k \rightarrow x^\ast\)는 superlinear convergence를 보인다.</li>
  </ol>
</blockquote>


        </article>
    </div>
</main>

<main class="container">
    <div class="content">
        <article class="post-body">
            <h1><a name="_page_7"></a>18-07 Limited Memory BFGS (LBFGS)</h1>
            <h2 id="introduction">Introduction</h2>

<p>LBFGS는 Limited-memory quasi-Newton methods의 한 예시로써, Hessian 행렬을 계산하거나 저장하기 위한 비용이 합리적이지 않을 경우 유용하게 사용된다. 이 방법은 밀도가 높은 \(n \times n\)의 Hessian 행렬을 저장하는 대신 \(n\)차원의 벡터 몇 개만을 유지하여 Hessian 행렬을 추정(approximation)하는 방식이다.</p>

<p>LBFGS의 알고리즘은 그 이름이 시사하는 것처럼 BGFS를 기반으로 한다. 주요한 아이디어는 Hessian에 대한 추정을 하기위해 가장 최근의 iteration들에서의 curvature information을 이용하자는 것이다. 반면 오래된 iteration들의 curvature information은 현재 iteration의 Hessian이 보이는 동향(behavior)과 다소 거리가 있을 것이므로 저장공간을 아끼는 측면에서 사용하지 않도록 한다.</p>

<p>여담으로 동일한 기법을 통해 다른 quasi-Newton 알고리즘(가령, SR1)의 limited-memory version도 유도 가능하다 [14].</p>

<h2 id="lbfgs">LBFGS</h2>

<p>LBFGS를 본격적으로 설명하기에 앞서 BFGS method에 대해 다시 살펴보자. 각 step에서 BFGS는 다음과 같이 \(x\)를 업데이트 한다.</p>
<blockquote>
\[x^+ = x - t H \nabla f, \\\\
\text{where } t \text{ is the step length and } H \text{ is updated at every iteration by means of the formula, }\\\\
\text{     }\\\\
H^+ =  \big( I - \frac{sy^T}{y^Ts} \big) H \big( I - \frac{ys^T}{y^Ts} \big) + \frac{ss^T}{y^Ts}.\\\\\]
</blockquote>

<p>\(H\)에 대한 업데이트 식을 이용하면 \(H^+q, q \in \mathbb{R}^n\)를 임의의 스칼라 \(\alpha, \beta \in \mathbb{R}\)와 임의의 벡터 \(p, s \in \mathbb{R}^n\)를 사용해 표현할 수 있다.</p>

<blockquote>
\[\begin{align}
H^+q &amp;=  \big( I - \frac{sy^T}{y^Ts} \big) H \big( I - \frac{ys^T}{y^Ts} \big)q + \frac{ss^Tq}{y^Ts}\\\\
&amp;= \big( I - \frac{sy^T}{y^Ts} \big) \underbrace{H \\big( q - \frac{s^T q}{y^Ts} y \big)}_{p} + \underbrace{\frac{s^Tq}{y^Ts}}_{\alpha} s\\\\
&amp;= \big( I - \frac{sy^T}{y^Ts} \big) p + \alpha s\\\\
&amp;= p - \underbrace{\frac{y^Tp}{y^Ts}}_{\beta}s + \alpha s \\\\
&amp;= p + (\alpha - \beta) s,\\\\
&amp; \text{where } \alpha = \frac{s^Tq}{y^Ts}, q^+ = q - \alpha y, p = Hq, \beta = \frac{y^Tp}{y^Ts}.
\end{align}\\\\\]
</blockquote>

<p>\(H\)가 k번의 BFGS update를 통해 얻이진다고 할때, \(Hq= -H\nabla f(x)\)는 length k의 반복문 2개로 계산할 수 있다 (아래 알고리즘 참고). 단, 메모리의 효율적인 사용을 위해 가장 최근 $m$개 iterations에서의 curvature information만을 이용한다. (\(k \ge m\))</p>

<h2 id="algorithm">Algorithm</h2>
<figure class="image" style="align: center;">
<p align="center">
  <img src="/img/chapter_img/chapter18/algorithm_quasi-newton.png" alt="[Fig1] The algorithm of LBFGS [3]" width="90%" />
  <figcaption style="text-align: center;">[Fig1] The algorithm of LBFGS [3]</figcaption>
</p>
</figure>

<p>보통 inverse Hessian approximation \(H_k\)는 dense하며, 변수의 개수가 많은 경우 저장 및 연산 비용이 매우 높아지게 된다. 반면 LBFGS는 \(H_k \nabla f_k\)을 연속한 벡터합과 벡터곱으로 얻어냄으로써 \(H_k\)의 계산 및 유지를 위한 비용문제를 완화시킬 수 있다. 뿐만 아니라 이 계산에 사용되는 initial Hessian approximation \(H^{0,k}\)는 보통 (실전에서 매우 효과적으로 작동한다고 검증된) identity matrix에 어떤 상수를 곱한 형태(\(H^{0,k} = \gamma_k I\))를 띄기 때문에 유지 및 계산에 그다지 큰 비용이 발생하지 않는다 ([14]의 7.2).</p>
<blockquote>
\[H^{0,k} = \gamma_k I, \\\\
\text{where } \: \gamma_k = \frac{s^T_{k-1}y_{k-1}}{y^T_{k-1}y_{k-1}}.\]
</blockquote>

<ul>
  <li><strong>Note:</strong> \(H^{0,k}\)는 매 iteration마다 다르게 선택될 수 있다.</li>
</ul>

        </article>
    </div>
</main>




      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
